{"componentChunkName":"component---node-modules-ocular-gatsby-src-templates-search-jsx","path":"/search","webpackCompilationHash":"a69192411ba5a955a9b1","result":{"pageContext":{"isCreatedByStatefulCreatePages":false,"data":[{"excerpt":"Contributing Contributions are welcome, assuming that they align with the general design goals and philosophy of the repo. Unless you just…","rawMarkdownBody":"# Contributing\n\nContributions are welcome, assuming that they align with the general design goals and philosophy of the repo.\n\nUnless you just want to contribute a small bug fix, it is a good idea to start by opening an issue and discuss your idea with the maintainers. This maximizes the chances that your contribution will be accepted once you open a pull request.\n\n## Configuring Your Development Environment\n\nTo contribute, you will likely want to clone the loaders.gl repository and make sure you can install, build and run tests.\n\nOur primary development environment is MacOS, but it is possible to build loaders.gl on Linux and Windows (using a Linux environment).\n\n### Setting up Linux Environment on Windows 10\n\nIt is possible to build under Windows, but not directly in the Windows command prompt. You will need to install a Linux command line environment.\n\nInstall [WSL (Windows Subsystem for Linux)](https://docs.microsoft.com/en-us/windows/wsl/install-win10) on Windows 10.\n\n### Install Node and NPM\n\n```bash\nsudo apt update\nsudo apt install nodejs\n```\n\n### Option: Install NVM\n\n- `https://www.liquidweb.com/kb/how-to-install-nvm-node-version-manager-for-node-js-on-ubuntu-12-04-lts/`\n- `https://github.com/nvm-sh/nvm/releases`\n\n### Install yarn\n\nhttps://www.hostinger.com/tutorials/how-to-install-yarn-on-ubuntu/\n\n```bash\nsudo apt update\nsudo apt install yarn nodejs\nyarn –version\n```\n\n### Install jq\n\n```bash\nsudo apt-get install jq\n```\n\n### Configuring your System\n\nOn Linux Systems Install packages\n\n- mesa-utils\n- xvfb\n- libgl1-mesa-dri\n- libglapi-mesa\n- libosmesa6\n- libxi-dev\n\nTo get the headless tests working: export DISPLAY=:99.0; sh -e /etc/init.d/xvfb start\n\n## Running Tests\n\n- `yarn lint`: Check coding standards and formatting\n- `yarn lint fix`: Fix errors with formatting\n- `yarn test node`: Quick test run under Node.js\n- `yarn test browser`: Test run under browser, good for interactive debugging\n- `yarn test`: Run lint, node test, browser tests (in headless mode)\n","slug":"docs/contributing","title":"Contributing"},{"excerpt":"Roadmap We are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the…","rawMarkdownBody":"# Roadmap\n\nWe are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the following ways:\n\n- **[RFCs](https://github.com/uber-web/loaders.gl/tree/master/dev-docs/RFCs)** - RFCs are technical writeups that describe proposed features in upcoming releases.\n- **[Roadmap Document](https://github.com/uber-web/loaders.gl/tree/master/docs/overview/roadmap)** - (this document) A high-level summary of our current direction for future releases.\n- **[Blog](https://medium.com/@vis.gl)** - We use the vis.gl blog to share information about what we are doing.\n- **[Github Issues](https://github.com/uber-web/loaders.gl/issues)** - The traditional way to start or join a discussion.\n\n\n## Feature Roadmap\n\nMany ideas are in tracker tasks in github, but here are some ideas:\n\n**Worker Thread Pool Priming** - Worker Pools should have an option to pre-warm so that loader thread pool is primed and ready to start off-thread parsing as soon as data arrives on the wire.\n\n**Progress Tracking** - loaders can provide progress callbacks and a `ProgressTracker` class to track the progress of a set of parallel loads.\n\n**Automatic Timing** - objects returned from loaders could contain a `stats` object with timing stats.\n\n**Stats and Default Settings** - Set `setDefaultOptions({stats: true})` to enable stats collection, etc.\n\n**MIME types** - Allow MIME types (e.g. from response headers) to be provided to assist in loader auto-selection. Enable Writers to return recommended MIMEtypes.\n\n\n## Format Roadmap\n\n### Data loaders\n\n- Streaming JSON loader\n\n### Geospatial loaders\n\nFocus on loading of complex geospatial data.\n- KML and Shapefile\n- Streaming GeoJSON loader\n\n### Scenegraph Formats\n\n- Focus on glTF/GLB - loaders.gl should to have a very solid implementation.\n- The glTF loaders should handle (e.g. preprocess) any glTF extensions that can be handled during the load phase (such as Draco, Basis - but many can only be handled during rendering).\n- Limited alternatives: Given the emergence of glTF as a major Khronos standard, and availability of good glTF conversion tools and exporters, loaders will most likely not implement any other scene/mesh description formats such as COLLADA.\n\n### Meshes\n\n- Given glTF, do not envision support beyond OBJ.\n- For OBJ, should we support MTL?\n\n### Point Clouds\n\nFocus on loading formats for large point clouds.\n\n### Massive Point Clouds/Data Sets\n\n- 3D Tiles\n- potree\n- i3s\n\nTile parsers are not enough, the 3d tiles category will need to include advanced traversal and tile loading caches\n\n### Other loaders\n\nFinally, some \"unusual\" loaders may be included just for fun, e.g. SVG tesselation.\n","slug":"docs/roadmap","title":"Roadmap"},{"excerpt":"Introduction loaders.gl is a growing suite of portable, framework-independent loaders and writers for a range of file formats related to…","rawMarkdownBody":"# Introduction\n\nloaders.gl is a growing suite of portable, framework-independent loaders and writers for a range of file formats related to geospatial and big data visualization, including point clouds, 3D geometries, images, geospatial formats as well as tabular data.\n\nloaders.gl is a major component of the [vis.gl](https://vis.gl) framework ecosystem. While all the provided loaders and writers are independently usable, frameworks like [deck.gl](https://deck.gl) and [luma.gl](https://luma.gl) are pre-integrated with loaders.gl.\n\n## Quick Code Example\n\nloaders.gl provides a small core API module with common functions to load and save data, and a number of additional modules that provide loaders and writers for specific file formats.\n\nA minimal example using the `load` function and the `CSVLoader` to load a CSV formatted table into a JavaScript array:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nconst data = await load('data.csv', CSVLoader);\n\nfor (const row of data) {\n  console.log(JSON.stringify(row)); // => '{header1: value1, header2: value2}'\n}\n```\n\nTo quickly get up to speed on how the loaders.gl API works, please see [Get Started](docs/developer-guide/get-started).\n\n## Major Components\n\n- **Loaders** - A suite of *loaders* for parsing various file formats into well-defined JavaScript data strucutures.\n- **Writers** - *writers* allowing data to be encoded and saved to certain formats.\n- **Core Functions** - A set of functions that make it easier to work with loaders and writers.\n\n## Why loaders.gl?\n\nMany open source projects already contain excellent loaders for the key 3D and geospatial formats under permissive licenses. However, due to design limitations (e.g. dependencies on a certain WebGL framework, not packaged for external use, lack of Node.js support etc) these can be hard to use in other applications.\n\nloaders.gl is an effort to collect some of the best existing open source loaders (and a handful of newly written loaders) and package them all in a unified, portable, framework-independent way.\n\n## Main Design Goals\n\n**Framework Agnostic** - Files are parsed into clearly documented data structures, that can be used with any JavaScript framework.\n\n**Loader Categories** - loaders.gl groups similar data formats into \"categories\". loaders in the same category return parsed data in \"standardized\" form, simplifying handling of multiple related formats.\n\n**Format Autodection** - Applications can specify multiple loaders when parsing a file, and loaders.gl will automatically pick the right loader for a given file.\n\n**Worker Pools** - Many loaders can run in web workers, keeping the main thread free for other tasks while parsing completes.\n\n**Node Support** - All loaders are work under Node.js, useful for running your unit tests under Node.\n\n**Bundle Size Optimized** - Each format is published as an independent npm module to allow cherry-picking, and additionally, modules are setup to let tree-shaking remove any symbols not imported by user.\n\nNote that while loaders.gl is optimized for use with WebGL frameworks (e.g. by returning typed arrays whenever possible), it does not have any actual WebGL dependencies and can be used without restrictions in non-WebGL applications.\n\n## Supported Platforms\n\nOur intention is for loaders.gl to work well on recent versions of the major evergreen browsers (Chrome, Firefox, Safari, both desktop and mobile). We also support as Node.js v10+ (assuming `@loaders.gl/polyfills` is installed).\n\nWe also have an ambition that loaders.gl should run on Edge, IE11 and Node.js v8, however this assumes that both `@loaders.gl/polyfills` and additional appropriate polyfills (e.g. babel polyfills) are installed. Testing on these older platforms is limited.\n\n## Licenses, Credits and Attributions\n\nLicense-wise, loaders.gl currently contains a collection of MIT and Apache licensed loaders. Each loader comes with its own license, so if the distinction matters to you, please check and decide accordingly. Additional licenses might be included in the future, however loaders.gl will never include code with non-permissive, commercial or copy-left licenses.\n\nRegading attributions, loaders.gl is partly a repackaging of the superb work done by many others in the open source community. We try to be as explicit as we can about the origins and attributions of each loader, both in the documentation page for each loader and in the preservation of comments relating to authorship and contributions inside forked source code.\n\nEven so, we can make mistakes, and we may note have the full history of the code we are reusing. If you think that we have missed something, or that we could do better in regard to attribution, please let us know.\n","slug":"docs","title":"Introduction"},{"excerpt":"Upgrade Guide v1.1 (In Development) A couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings…","rawMarkdownBody":"# Upgrade Guide\n\n## v1.1 (In Development)\n\nA couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings. Start replacing your use of these functions now to remove the console warnings and ensure a smooth future upgrade to v2.0.\n\nAlso, Node support now requires installing `@loaders.gl/polyfills` before use.\n\n### @loaders.gl/core\n\n- Removal: Node support for `fetchFile` now requires importing `@loaders.gl/polyfills` before use.\n- Removal: Node support for `TextEncoder`, and `TextDecoder` now requires importing `@loaders.gl/polyfills` before use.\n- Deprecation: `TextEncoder` and `TextDecoder` will not be exported from `loaders.gl/core` in v2.0.\n\n### @loaders.gl/images\n\n- Removal: Node support for images now requires importing `@loaders.gl/polyfills` before use.\n\n### @loaders.gl/gltf\n\n- Deprecation: `GLBParser`/`GLBBuilder` - These will be merged into GLTF classes..\n- Deprecation: `GLTFParser`/`GLTFBuilder` - The new `GLTF` class can hold GLTF data and lets application access/modify it.\n- Deprecation: `GLTFLoader` will no longer return a `GLTFParser` object in v2.0. Instead it will return a pure javascript object containing the parse json and any binary chunks. This object can be accessed through the `GLTF` class. Set `options.GLTFParser` to `false` to opt in to the new behavior now.\n\n## v1.0\n\nFirst official release of loaders.gl.\n","slug":"docs/upgrade-guide","title":"Upgrade Guide"},{"excerpt":"Category: GIS The GIS category is highly experimental and may be removed in a future release Several geospatial formats return data in the…","rawMarkdownBody":"# Category: GIS\n\n> The GIS category is highly experimental and may be removed in a future release\n\nSeveral geospatial formats return data in the form of lists of lng/lat encoded geometric objects.\n\n## GeoJSON Conversion\n\nGIS category data can be converted to GeoJSON (sometimes with a loss of information). Most geospatial applications can consume geojson.\n\n## Data Structure\n\nA JavaScript object with a number of top-level array-valued fields:\n\n| Field           | Description                        |\n| --------------- | ---------------------------------- |\n| `points`        | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `lines`         | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `polygons`      | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `imageoverlays` | Urls and bounds of bitmap overlays |\n| `documents`     |                                    |\n| `folders`       |                                    |\n| `links`         |                                    |\n\n## Loaders\n\n- [KMLLoader](/docs/api-reference/kml/kml-loader)\n\n","slug":"docs/specifications/category-gis","title":"Category: GIS"},{"excerpt":"Category: Table This category provides a set of conventions for working with tables in row-based, columnar or chunked/batched columnar…","rawMarkdownBody":"# Category: Table\n\nThis category provides a set of conventions for working with tables in row-based, columnar or chunked/batched columnar formats.\n\n## Data Structure\n\n| Field        | Type                | Contents     |\n| ------------ | ------------------- | ------------ |\n| `schema`     | `Object`            | Metadata of the table, maps name of each column to its type. |\n| `data`       | `Object` or `Array` | Data of the table, see [table types](#table-types) |\n| `length`     | `Number`            | Number of rows |\n\n\n## Table Types\n\nloaders.gl deals with (and offers utilities to convert between) three different types of tables:\n\n### Classic Tables (Row-Major)\n\nThis is the classic JavaScript table. `data` consists of an `Array` of `Object` instances, each representing a row.\n\n### Columnar Tables (Column-Major)\n\nColumnar tables are stored as one array per column. Columns that are numeric can be loaded as typed arrays which are stored in contigous memory. `data` is an `Object` that maps column names to an array or typed array.\n\nContiguous memory has tremendous benefits:\n\n- Values are adjacent in memory, the resulting cache locality can result in big performance gains\n- Typed arrays can of course be efficiently transferred from worker threads to main thread\n- Can be directly uploaded to the GPU for further processing.\n\n### Chunked Columnar Tables (DataFrames)\n\nA problem with columnar tables is that column arrays they can get very long, causing issues with streaming, memory allication etc. A powerful solution is to worked with chunked columnar tables, where columns is are broken into matching sequences of typed arrays.\n\nThe down-side is that complexity can increase quickly. Data Frames are optimized to minimize the amount of copying/moving/reallocation of data during common operations such e.g. loading and transformations, and support zero-cost filtering through smart iterators etc.\n\nUsing the Arrow API it is possible to work extremely efficiently with very large (multi-gigabyte) datasets.\n\n## Loaders\n\n- [ArrowLoader](/docs/api-reference/arrow/arrow-loader)\n- [CSVLoader](/docs/api-reference/csv/csv-loader)\n\n","slug":"docs/specifications/category-table","title":"Category: Table"},{"excerpt":"Category: 3D Tiles The 3D tiles category is still under development. The 3D Tiles category generalizes hierarchical geospatial data…","rawMarkdownBody":"# Category: 3D Tiles\n\n> The 3D tiles category is still under development.\n\nThe 3D Tiles category generalizes hierarchical geospatial data structures. It is being defined for the [OGC 3D Tiles](https://www.opengeospatial.org/standards/3DTiles) standard but may be generalized to handle the [OGC i3s](https://www.opengeospatial.org/standards/i3s) standard and the `potree` format as well.\n\n\n## Concepts\n\n* **Tile Header Hierarchy** - An initial, \"minimal\" set of data listing the *hierarchy of available tiles*, with  minimal information to allow an application to determine which tiles need to be loaded based on a certain viewing position in 3d space.\n* **Tile Header** - A minimal header describing a tiles bounding volume and a screen space error tolerance (allowing the tile to be culled if it is distant), as well as the URL to load the tile's actual content from.\n* **Tile Cache** - Since the number of tiles in big tilesets often exceed what can be loaded into available memory, it is important to have a system that releases no-longer visible tiles from memory.\n* **Tileset Traversal** - Dynamically loading and rendering 3D tiles based on current viewing position, possibly triggering loads of new tiles and unloading of older, no-longer visible tiles.\n\n\n## Tileset Traversal Support\n\nTo start loading tiles once a top-level tileset file is loaded, the application can instantiate the `Tileset3D` class and start calling `tileset3D.traverse(camera_parameters)`.\n\nSince 3D tiled data sets tend to be very big, the key idea is to only load the tiles actually needed to show a view from the current camera position.\n\nThe `Tileset3D` allows callbacks (`onTileLoad`, `onTileUnload`) to be registered that notify the app when the set of tiles available for rendering has changed. This is important because tile loads complete asynchronously, after the `tileset3D.traverse(...)` call has returned.\n\n## Coordinate Systems\n\nTo help applications process the `position` data in the tiles, 3D Tiles category loaders are expected to provide matrices are provided to enable tiles to be used in both fixed frame or cartographic (long/lat-relative, east-north-up / ENU) coordinate systems:\n\n* *cartesian*  WGS84 fixed frame coordinates\n* *cartographic*  tile geometry positions to ENU meter offsets from `cartographicOrigin`.\n\nPosition units in both cases are in meters.\n\nFor cartographic coordinates, tiles come with a prechosen cartographic origin and precalculated model matrix. This cartographic origin is \"arbitrary\" (chosen based on the tiles bounding volume center). A different origin can be chosen and a transform can be calculated, e.g. using the math.gl `Ellipsoid` class.\n\n\n## Tileset Fields\n\n| Field        | Type                | Contents     |\n| ------------ | ------------------- | ------------ |\n| `asset`              | `Object` (Optional) |  |\n| `root`               | `Object`    | The root tile header |\n| `cartesianCenter`    | `Number[3]` | Center of tileset in fixed frame coordinates |\n| `cartographicCenter` | `Number[3]` | Center of tileset in cartographic coordinates `[long, lat, elevation]` |\n| `webMercatorZoom`    | `Number[3]` | A web mercator zoom level that displays the entire tile set bounding volume |\n\n## TileHeader Fields\n\n| Field            | Type                | Contents     |\n| ---------------- | ------------------- | ------------ |\n| `boundingVolume` | `Object`            |  |\n\n\n## Tile Fields\n\n### Common Fields\n\n| Field        | Type                | Contents     |\n| ------------ | ------------------- | ------------ |\n| `loaderData` | `Object` (Optional) | Format specific data |\n| `version`    | `Number`            | See [Header](#header)    |\n| `type`       | `String`            | See [Mode](#mode)    |\n| `cartesianOrigin`         | `Number[3]`  | \"Center\" of tile geometry in WGS84 fixed frame coordinates    |\n| `cartographicOrigin`      | `Number[3]`  | \"Origin\" in lng/lat (center of tile's bounding volume)    |\n| `cartesianModelMatrix`    | `Number[16]` | Transforms tile geometry positions to fixed frame coordinates   |\n| `cartographicModelMatrix` | `Number[16]` | Transforms tile geometry positions to ENU meter offsets from `cartographicOrigin`. |\n\n\n### PointCloudTile Fields\n\n| Field                        | Type                | Contents     |\n| --------------               | -------- | -------- |\n| `attributes`                 | `Object` | Values are [accessor](#accessor) objects.  |\n| `attributes.positions.value` | `Float32Array`  |   |\n| `attributes.normals.value`   | `Float32Array`  |   |\n| `attributes.colors.value`    | `Uint8Array`    |   |\n\nTBA - batch ids?\n\n\n### Instanced3DModelTile Fields\n\n| Field          | Type     | Contents     |\n| -------------- | -------- | -------- |\n| `modelMatrix`  | | |\n\n\n### PointCloudTile Fields\n\n| Field          | Type     | Contents     |\n| -------------- | -------- | -------- |\n\n\n### CompositeTile Fields\n\n| Field          | Type       | Contents     |\n| -------------- | --------   | -------- |\n| `tiles`        | `Object[]` | Array of parsed tile objects |\n\n\n### Accessors\n\nFollowing vis.gl conventions, `attributes` are represented by \"glTF-style\" accessor objects with the `value` field containing the binary data for that attribute stored in a typed array of the proper type.\n\n| Accessors Fields | glTF? | Type         | Contents       |\n| ---------------- | ----- | ------------ | -------------- |\n| `value`          | No    | `TypedArray` | Contains the typed array (corresponds to `bufferView`). The type of the array will match the GL constant in `componentType`. |\n| `size`           | No    | `Number`      | Number of components, `1`-`4`. |\n| `byteOffset`     | Yes   | `Number`     | Starting offset into the bufferView. |\n| `count`          | Yes   | `Number`     | The number of elements/vertices in the attribute data. |\n","slug":"docs/specifications/category-3d-tiles","title":"Category: 3D Tiles"},{"excerpt":"What's New v1.3 (In Development, alpha/beta releases will soon become available) Target Release Date: Sep 13 : Loader Selection Improvements…","rawMarkdownBody":"# What's New\n\n## v1.3 (In Development, alpha/beta releases will soon become available)\n\nTarget Release Date: Sep 13\n\n- `@loaders.gl/core`: **Loader Selection Improvements**\n    - The loader selection mechanism is now exposed to apps through the new `selectLoader` API.\n    - Loaders can now examine the first bytes of a file\n    - This complements the existing URL extension based auto detection mechanisms.\n\n- `@loaders.gl/core`: **Worker Thread Pool**\n    - Now reuses worker threads. Performance gains by avoiding worker startup overhead.\n    - Worker threads are named, easy to track in debugger\n    - Worker based loaders can now call `parse` recursively to delegate parsing of embedded data (e.g. glTF, Draco) to other loaders\n\n- `@loaders.gl/3d-tiles`: **Tile3DLayer moved to deck.gl**\n    - Tile3DLayer is now exported from `@deck.gl/geo-layers`\n\n- `@loaders.gl/3d-tiles`: **Batched 3D Model Tile Support**\n    - `b3dm` tilesets can now be loaded and displayed by the `Tile3DLayer`\n\n- `@loaders.gl/3d-tiles`: **RequestScheduler**\n    - Cancels loads for not-yet loaded tiles that are no longer in view)\n    - Dramatically improves loading performance when panning/zooming through a tileset\n\n- `@loaders.gl/3d-tiles`: **Performance Tracking**\n    - `Tileset3D` now contain a `stats` object with stats on the loading process to help profile big tilesets.\n\n- `@loaders.gl/gltf`: **Version 2 Improvements**\n    - Select the new glTF parser by passing `options.gltf.parserVersion: 2` to the `GLTFLoader`.\n    - Many improvements to the v2 glTF parser.\n\n## v1.2\n\nRelease Date: Aug 8, 2019\n\n- `@loaders.gl/core`: File Type Auto Detection now supports binary files\n- `@loaders.gl/polyfills`: Fixed `TextEncoder` warnings\n- `@loaders.gl/arrow`: Improved Node 8 support\n- `@loaders.gl/images`: Image file extensions now added to loader object\n- `@loaders.gl/gltf`: Generate default sampler parameters if none provided in gltf file\n\n### @loaders.gl/3d-tiles (EXPERIMENTAL)\n\n- Support for dynamic traversal of 3D tilesets (automatically loads and unloads tiles based on viewer position and view frustum).\n- Support for loading tilesets from Cesium ION servers.\n- Asynchronous tileset loading\n- Auto centering of view based on tileset bounding volumes\n- deck.gl `Tile3DLayer` class provided in examples.\n\n## v1.1\n\nRelease Date: May 30, 2019\n\n### @loaders.gl/core\n\n- `fetchFile` function - Can now read browser `File` objects (from drag and drop or file selection dialogs).\n- `isImage(arrayBuffer [, mimeType])` function <sup>ENHANCED</sup> - can now accept a MIME type as second argument.\n\n### @loaders.gl/images\n\n- `getImageMIMEType(arrayBuffer)` function <sup>NEW</sup> - returns the MIME type of the image in the supplied `ArrayBuffer`.\n- `isImage(arrayBuffer [, mimeType])` function <sup>ENHANCED</sup> - can now accept a MIME type as second argument.\n\n### @loaders.gl/gltf\n\n- The glTF module has been refactored with the aim of simplifying the loaded data and orthogonalizing the API.\n- \"Embedded' GLB data (GLBs inside other binary formats) can now be parsed (e.g. the glTF parser can now extract embedded glTF inside 3D tile files).\n\n- New classes/functions:\n    - [`GLTFScenegraph`](/docs/api-reference/gltf/gltf-scenegraph) class <sup>NEW</sup> - A helper class that provides methods for structured access to and modification/creation of glTF data.\n    - [`postProcessGLTF`](/docs/api-reference/gltf/post-process-gltf) function <sup>NEW</sup> - Function that performs a set of transformations on loaded glTF data that simplify application processing.\n    - [`GLBLoader`](/docs/api-reference/gltf/glb-loader)/[`GLBWriter`] <sup>NEW</sup> - loader/writer pair that enables loading/saving custom (non-glTF) data in the binary GLB format.\n    - [`GLTFLoader`](/docs/api-reference/gltf/gltf-loader), letting application separately handle post-processing.\n\n### @loaders.gl/3d-tiles <sup>NEW MODULE</sup>\n\n- Support for the 3D tiles format is being developed in the new `@loaders.gl/3d-tiles` module.\n- Loading of individual point cloud tiles, including support for Draco compression and compact color formats such as RGB565 is supported.\n\n### @loaders.gl/polyfills <sup>NEW MODULE</sup>\n\nNode support now requires importing `@loaders.gl/polyfills` before use. This reduces the number of dependencies, bundle size and potential build complications when using other loaders.gl modules when not using Node.js support.\n\n### @loaders.gl/loader-utils <sup>NEW MODULE</sup>\n\nHelper functions for loaders have been broken out from `@loaders.gl/core`. Individual loaders no longer depend on`@loaders.gl/core` but only on `@loaders.gl/loader-utils`.\n\n## v1.0\n\nRelease Date: April 2019\n\n- First Official Release\n","slug":"docs/whats-new","title":"What's New"},{"excerpt":"Loader Object To be compatible with the parsing/loading functions in  such as  and , a parser needs to be described by a \"loader object…","rawMarkdownBody":"# Loader Object\n\nTo be compatible with the parsing/loading functions in `@loaders.gl/core` such as `parse` and `load`, a parser needs to be described by a \"loader object\" conforming to the following specification.\n\n## Loader Object Format v1.0\n\n### Common Fields\n\n| Field        | Type       | Default  | Description                                                     |\n| ------------ | ---------- | -------- | --------------------------------------------------------------- |\n| `name`       | `String`   | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension`  | `String`   | Required | Three letter (typically) extension used by files of this format |\n| `extensions` | `String[]` | Required | Array of file extension strings supported by this loader        |\n| `category`   | `String`   | Optional | Indicates the type/shape of data                                |\n\nNote: Only one of `extension` or `extensions` is required. If both are supplied, `extensions` will be used.\n\n### Test Function\n\n| Field      | Type       | Default | Description                                                                       |\n| ---------- | ---------- | ------- | --------------------------------------------------------------------------------- |\n| `test` | `Function`|`String`|`String[]` | `null`  | Guesses if a binary format file is of this format by examining the first bytes in the file. If the test is specified as a string or array of strings, the initial bytes are expected to be \"magic bytes\" matching one of the provided strings. |\n| `testText` | `Function` | `null`  | Guesses if a text format file is of this format by examining the first characters in the file |\n\n### Parser Function\n\nWhen creating a new loader object, at least one of the parser functions needs to be defined.\n\n| Parser function field               | Type       | Default | Description                                                                            |\n| ----------------------------------- | ---------- | ------- | -------------------------------------------------------------------------------------- |\n| `parseInBatches` (Experimental)     | `Function` | `null`  | Parses binary data chunks (`ArrayBuffer`) to output data \"batches\"                     |\n| `parseInBatchesSync` (Experimental) | `Function` | `null`  | Synchronously parses binary data chunks (`ArrayBuffer`) to output data \"batches\"       |\n| `parseSync`                         | `Function` | `null`  | Atomically and synchronously parses binary data (e.g. file contents) (`ArrayBuffer`)   |\n| `parseTextSync`                     | `Function` | `null`  | Atomically and synchronously parses a text file (`String`)                             |\n| `parse`                             | `Function` | `null`  | Asynchronously parses binary data (e.g. file contents) asynchronously (`ArrayBuffer`). |\n| `loadAndParse`                      | `Function` | `null`  | Asynchronously reads a binary file and parses its contents.                            |\n\nNote: Only one parser function is required. Synchronous parsers are more flexible as they can support synchronous parsing in addition to asynchronous parsing, and iterator-based parsers are more flexible as they can support batched loading in addition to atomic loading. You are encouraged to provide the most capable parser function you can (e.g. `parseSync` or `parseToIterator` if possible). Unless you are writing a completely new loader, the appropriate choice usually depends on the loader you are encapsulating.\n","slug":"docs/specifications/loader-object-format","title":"Loader Object"},{"excerpt":"Category: Scenegraph The Scenegraph category is intended to represent glTF scenegraphs. The data format is fairly raw, close to the unpacked…","rawMarkdownBody":"# Category: Scenegraph\n\nThe Scenegraph category is intended to represent glTF scenegraphs.\n\nThe data format is fairly raw, close to the unpacked glTF/GLB data structure, it is described by:\n- a parsed JSON object (with top level arrays for `scenes`, `nodes` etc)\n- a list of `ArrayBuffer`s representing binary blocks (into which `bufferViews` and `images` in the JSON point).\n\n## Helper Classes\n\nTo simplify higher-level processing of the loaded, raw glTF data, several helper classes are provided in the `@loaders.gl/gltf` module, these can:\n- unpack and remove certain glTF extensions\n- extract typed array views from the JSON objects into the binary buffers\n- create HTML images from image buffers\n- etc\n\n## Non-glTF Scenegraphs\n\nThe scenegraph \"category\" is quite specific glTF, and there are no plans to support other scenegraph formats in loaders.gl. Therefore, the current recommendation is to convert scenegraph files to glTF with external tools before loading them using loaders.gl.\n\nThat said, hypothetical new loaders for other scenegraph formats (e.g. a COLLADA loader) could potentially choose to belong to the Scenegraph category by \"converting\" the loaded data to this glTF format (and thus enable interoperability with applications that are already designed to use the `GLTFLoader`).\n\n## Data Structure\n\nA JSON object with the following top-level fields:\n\n| Field     | Type          | Default   | Description |\n| ---       | ---           | ---       | --- |\n| `magic`   | `Number`      | glTF      | The first four bytes of the file |\n| `version` | `Number`      | `2`       | The version number |\n| `json`    | `Object`      | `{}`      | The JSON chunk  |\n| `buffers` | `ArrayBuffer[]` | `[]`      | (glTF) The BIN chunk plus any base64 or BIN file buffers |\n\nBuffers can be objects in the shape of `{buffer, byteOffset, byteLength}`.\n\n## Loaders\n\n- [GLTFLoader](/docs/api-reference/gltf/gltf-loader)\n- [GLBLoader](/docs/api-reference/gltf/glb-loader)\n\n## Notes\n\n- [Tile3DLoader](/docs/api-reference/3d-tiles/tile-3d-loader) some tiles contain embedded glTF.\n","slug":"docs/specifications/category-scenegraph","title":"Category: Scenegraph"},{"excerpt":"Category: Mesh/PointCloud This category unifies the loader output for simple mesh and point clouds formats that describe a \"single geometry…","rawMarkdownBody":"# Category: Mesh/PointCloud\n\nThis category unifies the loader output for simple mesh and point clouds formats that describe a \"single geometry primitive\" (as opposed to e.g. a scenegraph consisting of multiple geometries).\n\nA single mesh is typically defined by a set of attributes, such as `positions`, `colors`, `normals` etc, as well as a draw mode.\n\n## Format Notes\n\nThe Pointcloud/Mesh loaders output mesh data in a common form that is optimized for use in WebGL frameworks:\n\n- All attributes (and indices if present) are stored as typed arrays of the proper type.\n- All attributes (and indices if present) are wrapped into glTF-style \"accessor objects\", e.g. `{size: 1-4, value: typedArray}`.\n- Attribute names are mapped to glTF attribute names (on a best-effort basis).\n- An `indices` field is added (only if present in the loaded geometry).\n- A primitive drawing `mode` value is added (the numeric value matches WebGL constants, e.g `GL.TRIANGLES`).\n\n## Data Structure\n\n| Field        | Type                | Contents     |\n| ------------ | ------------------- | ------------ |\n| `loaderData` | `Object` (Optional) | Loader and format specific data |\n| `header`     | `Object`            | See [Header](#header)    |\n| `mode`       | `Number`            | See [Mode](#mode)    |\n| `attributes` | `Object`            | Keys are [glTF attribute names](#gltf-attribute-name-mapping) and values are [accessor](#accessor) objects.  |\n| `indices`    | `Object` (Optional) | If present, describes the indices (elements) of the geometry as an [accessor](#accessor) object. |\n\n### Header\n\nThe `header` fields are only recommended at this point, applications can not assume they will be present:\n\n| `header` Field | Type     | Contents |\n| -------------- | -------- | -------- |\n| `vertexCount`  | `Number` |          |\n\n### Mode\n\nPrimitive modes are aligned with [OpenGL/glTF primitive types](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#primitive)\n\n| Value | Primitive Mode   | Comment           |\n| ----- | ---------------- | ----------------- |\n| `0`   | `POINTS`         | Used for point cloud category data |\n| `1`   | `LINES`          | Lines are rarely used due to limitations in GPU-based rendering |\n| `2`   | `LINE_LOOP`      | -                 |\n| `3`   | `LINE_STRIP`     | -                 |\n| `4`   | `TRIANGLES`      | Used for most meshes. Indices attributes are often used to reuse vertex data in remaining attributes |\n| `5`   | `TRIANGLE_STRIP` | -                 |\n| `6`   | `TRIANGLE_FAN`   | -                 |\n\n\n### Accessor\n\n`attributes` and `indices` are represented by glTF \"accessor objects\" with the binary data for that attribute resolved into a typed array of the proper type.\n\n| Accessors Fields | glTF? | Type         | Contents       |\n| ---------------- | ----- | ------------ | -------------- |\n| `value`          | No    | `TypedArray` | Contains the typed array (corresponds to `bufferView`). The type of the array will match the GL constant in `componentType`. |\n| `size`           | No    | `Number`      | Number of components, `1`-`4`. |\n| `byteOffset`     | Yes   | `Number`     | Starting offset into the bufferView. |\n| `count`          | Yes   | `Number`     | The number of elements/vertices in the attribute data. |\n| `originalName`   | No    | `String` (Optional) | If this was a named attribute in the original file, the original name (before substitution with glTF attribute names) will be made available here. |\n\n\n### glTF Attribute Name Mapping\n\nTo help applications manage attribute name differences between various formats, mesh loaders map known attribute names to [glTF 2.0 standard attribute names](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#geometry) a best-effort basis.\n\nWhen a loader can map an attribute name, it will replace ir with the glTF equivalent. This allows applications to use common code to handle meshes and point clouds from different formats.\n\n| Name         | Accessor Type(s) | Component Type(s) | Description   |\n| ------------ | ---------------- | ----------------- | ------------- |\n| `POSITION`   | `\"VEC3\"`         | `5126` (FLOAT)    | XYZ vertex positions |\n| `NORMAL`     | `\"VEC3\"`         | `5126` (FLOAT)    | Normalized XYZ vertex normals |\n| `TANGENT`    | `\"VEC4\"`         | `5126` (FLOAT)    | XYZW vertex tangents where the _w_ component is a sign value (-1 or +1) indicating handedness of the tangent basis |\n| `TEXCOORD_0` | `\"VEC2\"`         | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | UV texture coordinates for the first set |\n| `TEXCOORD_1` | `\"VEC2\"`         | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | UV texture coordinates for the second set  |\n| `COLOR_0`    | `\"VEC3\"`, `\"VEC4\"` | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | RGB or RGBA vertex color  |\n| `JOINTS_0`   | `\"VEC4\"`         | `5121` (UNSIGNED_BYTE), `5123` (UNSIGNED_SHORT) |  |\n| `WEIGHTS_0`  | `\"VEC4\"`         | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized |  |\n\n> Note that for efficiency reasons, mesh loaders are not required to convert the format of an attribute's binary data to match the glTF specifications (i.e. if normals were encoded using BYTES then that is what will be returned even though glTF calls out for FLOAT32). Any such alignment needs to be done by the application as a second step.\n\n\n## Limitations\n\n### Scenegraph support\n\nFor more complex, scenegraph-type formats (i.e. formats that don't just contain single geometric primitives), loaders.gl currently focuses on glTF 2.0 support.\n\nIt is assumed that other scenegraph-type format loaders (e.g. a hyptothetical COLLADA loader) could convert their loaded data to a similar structure, essentially converting to glTF 2.0 on-the-fly as they load.\n\nFor now it is best to convert such assets off-line to glTF before attempting to loade them with loaders.gl.\n\n### Material support\n\nMaterial support is provided by some mesh formats (e.g. OBJ/MTL) and is currently not implemented by loaders.gl, however the glTF loader has full support for PBR (Physically-Based Rendering) materials.\n\n## Loaders\n\n- [LASLoader](/docs/api-reference/las/las-loader)\n- [OBJLoader](/docs/api-reference/obj/obj-loader)\n- [PCDLoader](/docs/api-reference/pcd/pcd-loader)\n- [PLYLoader](/docs/api-reference/ply/ply-loader)\n\n","slug":"docs/specifications/category-mesh","title":"Category: Mesh/PointCloud"},{"excerpt":"Using Loaders loaders.gl has parser functions that use so called \"loaders\" to convert the raw data loaded from files into parsed objects…","rawMarkdownBody":"# Using Loaders\n\nloaders.gl has parser functions that use so called \"loaders\" to convert the raw data loaded from files into parsed objects. Each loader encapsulates a parsing function for one file format (or a group of related file formats) together with some metadata (like the loader name, common file extensions for the format etc).\n\n## Installing loaders\n\nloaders.gl provides a suite of pre-built loader objects packaged as scoped npm modules. The intention is that applications will install and import loaders only for the formats they need.\n\n## Using Loaders\n\nLoaders can be passed into utility functions in the loaders.gl core API to enable parsing of the chosen format.\n\n## Creating New Loaders\n\n> See the a detailed specification of the [loader object format API reference](docs/specifications/loader-object-format).\n\nApplications can also create new loader objects. E.g. if you have existing JavaScript parsing functionality that you would like to use with the loaders.gl core utility functions.\n\nYou would give a name to the loader object, define what file extension(s) it uses, and define a parser function.\n\n```js\nexport default {\n  name: 'JSON',\n  extensions: ['json'],\n  testText: null,\n  parseTextSync: JSON.parse\n};\n```\n\n| Field       | Type     | Default  | Description                                                     |\n| ----------- | -------- | -------- | --------------------------------------------------------------- |\n| `name`      | `String` | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension` | `String` | Required | Three letter (typically) extension used by files of this format |\n| `testText` | `Function` | `null`  | Guesses if a file is of this format by examining the first characters in the file |\n\nA loader must define a parser function for the format, a function that takes the loaded data and converts it into a parsed object.\n\nDepending on how the underlying loader works (whether it is synchronous or asynchronous and whether it expects text or binary data), the loader object can expose the parser in a couple of different ways, specified by provided one of the parser function fields.\n","slug":"docs/developer-guide/about-loaders","title":"Using Loaders"},{"excerpt":"Writer Object To be compatible with  functions such as , writer objects need to conform to the following specification: Common Fields Field…","rawMarkdownBody":"# Writer Object\n\nTo be compatible with `@loaders.gl/core` functions such as `encode`, writer objects need to conform to the following specification:\n\n### Common Fields\n\n| Field       | Type     | Default  | Description                                                     |\n| ----------- | -------- | -------- | --------------------------------------------------------------- |\n| `name`      | `String` | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension` | `String` | Required | Three letter (typically) extension used by files of this format |\n| `category`  | `String` | Optional | Indicates the type/shape of data                                |\n\n### Encoder Function\n\n| Field                            | Type       | Default | Description                                            |\n| -------------------------------- | ---------- | ------- | ------------------------------------------------------ |\n| `encodeSync`                     | `Function` | `null`  | Encodes synchronously                                  |\n| `encode`                         | `Function` | `null`  | Encodes asynchronously                                 |\n| `encodeInBatches` (Experimental) | `Function` | `null`  | Encodes and releases batches through an async iterator |\n\nNote: The format of the input data to the encoders depends on the loader. Several loader categories are defined to provided standardized data formats for similar loaders.\n","slug":"docs/specifications/writer-object-format","title":"Writer Object"},{"excerpt":"Error Handling Applications typically want to provide solid error handling when loading and saving data. Ideally the applications wants to…","rawMarkdownBody":"# Error Handling\n\nApplications typically want to provide solid error handling when loading and saving data. Ideally the applications wants to use a simple clean API for the loading, and yet have the confidence that errors are caught and meaningful messages are presented to the user.\n\n## Types of Errors\n\nThere are tree main types of errors that arise when attempting to load a data resource:\n1. There is some kind of network/resource access error, preventing the request for data from being issued\n2. A request is sent to a server, but the server is unable to service the request due to some error condition (often illegal access tokens or request parameters) and sends an error response.\n3. The server returns data, but the parser is unable to parse it (perhaps due to the data being malformatted, or formatted according to an unsupported version of that format).\n\nloaders.gl can detect all of these error conditions and report the resulting errors in a unified way (the errors will be available as exceptions or rejected promises depending on your async programming style, see below).\n\n### Error Messages\n\nloaders.gl aims to prodice concise, easy-to-understand error messages that can be presented directly to the end user.\n\nWhen the fetch call fails, the genereted exception is passed to the user, and the same is true when a loader fails. For server error responses, some basic information about the error is compiled into an error message (using e.g. `response.status`, `response.url` and occasionally `response.text`).\n\nNote that while servers often send some information about errors in `response.text()` when setting HTTP error codes, there are no universally adhered-to conventions for how servers format those error messages. The data is often a set of key-value pairs that are JSON or XML encoded, but even then the exact key names are usually server-specific.\n\nAt the moment loaders.gl does not provide any error formatting plugins, so if you know how your specific service formats errors and want to extract these in a way that you can present to the user, you may want to take control of the fetch `Response` status checking, see below.\n\n\n## parse Error Handling\n\n`parse` accepts fetch `Response` objects, and `parse` will check the status of the `Response` before attempting to parse, and generate an exception if appropriate.\n\n\n## Handling Errors from Async Functions\n\nNote that `parse` is an async function, and in JavaScript, errors generated by async functions will be reported either as an exception or as a rejected promise, depending on how the async funtion was called (using promises or the `await` keyword):\n\nWhen using `await`, errors are reported as exceptions\n```js\ntry {\n  const response = await fetch(url);\n  const data = await parse(response);\n} catch (error) {\n  console.log(error);\n}\n```\n\nA rejected promise is generated when using `Promise.then`.\n```js\nfetch(url)\n.then(response => parse(response))\n.catch(error => console.log(error));\n```\n\nAlso note that the Javascript runtime seamlessly converts errors between exceptions and promises in mixed code.\n\n\n## fetch Error Handling\n\nloaders.gl is designed around the use of the modern JavaScript `fetch` API, so for additional context, it may help to review of how the JavaSctipt `fetch` function handles errors.\n\n`fetch` separates between \"network errors\" that can be detected directly (these cause the `fetch` to throw an exception) and server side errors that are reported asynchronously with HTTP status codes (in this case the `Response` object offers accessors that must be called to check if the operation was successful before accessing data).\n\nExample: \"manually\" checking separately for fetch network errors and server errors:\n\n```js\n// Check for network error\nlet response;\ntry {\n   response = await fetch(url);\n} catch (error) {\n  console.log('Network error');\n}\n\n// Check for server error\nif (!response.ok) {\n  console.log(`fetch failed with status ${response.status}`);\n}\n```\n\nNote that servers often sends a message providing some detail about what went wrong, and that message can be accessed using the standard (asynchronous) `response.text()` or `response.json()` methods.\n\n```js\nif (!response.ok) {\n  const errorMessage = await response.text();\n  // Custom parsing can be done here, if you know how your particular service formats errors\n  console.log(`fetch failed with status ${errorMessage}`);\n}\n```\n","slug":"docs/developer-guide/error-handling","title":"Error Handling"},{"excerpt":"Get Started Installing Install loaders.gl core and loader for any modules you would like to use. Each format is published as a separate npm…","rawMarkdownBody":"# Get Started\n\n## Installing\n\nInstall loaders.gl core and loader for any modules you would like to use.\n\nEach format is published as a separate npm module.\n\n```bash\nyarn add @loaders.gl/core\nyarn add @loaders.gl/gltf\n...\n```\n\n## Usage\n\nYou can import a loader and use it directly with `parse`. Note that `parse` can accept a `fetch` response object as the source of data to be parsed:\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\nconst data = await parse(fetch('data.csv'), CSVLoader);\n```\n\nYou can register loaders after importing them\n\n```js\nimport {registerLoaders} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\nregisterLoaders(CSVLoader);\n```\n\nThen, in the same file (or some other file in the same app) that needs to load CSV, you no longer need to supply the loader to `parse`. It will autodetect the pre-registered loader:\n\n```js\nimport {parse} from '@loaders.gl/core';\n\n// The pre-registered CSVLoader gets auto selected based on file extension...\nconst data = await parse(fetch('data.csv'));\n```\n\n## Building\n\nYou can use your bundler of choice such as webpack or rollup. See the [`get-started-...`](https://github.com/uber-web/loaders.gl/tree/master/examples) examples for minimal working examples of how to bundle loaders.gl.\n\n## Supporting Older Browsers\n\nloaders.gl is designed to leverage modern JavaScript (ES2018) and to optimize functionality and performance on evergreen browsers.\n\nHowever, the default distribution is completely transpiled to ES5 so using loaders.gl with older or \"slower moving\" browsers such as IE11 and Edge is possible, assuming that the appropriate polyfills are installed.\n\nTo build on Edge and IE11, `TextEncoder` and `TextDecoder` must be polyfilled. There are several polyfills available on `npm`, but you can also use the polyfills provided by loaders.gl:\n\n```bash\nyarn install @loaders.gl/polyfills\n```\n\n```js\nimport '@loaders.gl/polyfills';\n```\n\n## Supporting Node.js\n\nA number of polyfills for `fetch`, `TextEncoder` etc are available to make loaders.gl work under Node.js, just install the `@loaders.gl/polyfills module` as described above.\n","slug":"docs/developer-guide/get-started","title":"Get Started"},{"excerpt":"Using Writers Writers and the  functions are still in development, and while they can be used may still have issues. For a detailed…","rawMarkdownBody":"# Using Writers\n\n> Writers and the `encode` functions are still in development, and while they can be used may still have issues.\n\n> For a detailed specification of the writer object format see the [API reference](docs/specifications/writer-object-format.md).\n\nTBA\n","slug":"docs/developer-guide/about-writers","title":"Using Writers"},{"excerpt":"Loader Categories To simplify working with multiple similar formats, loaders and writers in loaders.gl are grouped into categories. The idea…","rawMarkdownBody":"# Loader Categories\n\nTo simplify working with multiple similar formats, loaders and writers in loaders.gl are grouped into *categories*.\n\nThe idea is that many loaders return very similar data (e.g. point clouds loaders), which makes it possible to represent the loaded data in the same data structure, letting applications handle the output from multiple loaders without\n\nWhen a loader is documented as belonging to a specifc category, it converts the parsed data into the common format for that category. This allows an application to support multiple formats with a single code path, since all the loaders will return similar data structures.\n\n## Categories and Loader Registration\n\nThe fact that loaders belong to categories enable applications to flexibly register new loaders in the same category.\n\nFor instance, once an application has added support for one loader in a category, other loaders in the same category can be registered during application startup.\n\nOriginal code\n```js\nimport {parse, registerLoaders} from '@loaders.gl/core';\nimport {PCDLoader} from `@loaders.gl/pcd';\nregisterLoaders([PCDLoader]);\nasync function loadPointCloud(url) {\n  const pointCloud = await parse(fetch(url));\n  // Use some WebGL framework to render the parsed cloud\n}\n```\n\nNow support for additional point cloud formats can be added to the application without touching the original code:\n\n```js\nimport {LASLoader} from `@loaders.gl/las';\nimport {DracoLoader} from `@loaders.gl/draco';\nregisterLoaders([LASLoader, DracoLoader]);\n```\n\n## Data Format\n\nEach category documents the returned data format. loaders and writers reference the category documentation.\n\n## Writers and Categories\n\nWriters for a format that belongs to a category accept data objects with fields described by the documentation for that category.\n\n## Accessing Format-Specific Data\n\nSometimes, not all the properties provided by a certain file format can be mapped to common properties defined by the corresponding loader category.\n\nTo access format-specific properties, use the `loaderData` field in data object returned by the loader.\n\n## Available Categories\n\nCategories are described in the specifications section. Some currently defined categories are:\n\n- [Table](/docs/specifications/category-table)\n- [PointCloud/Mesh](/docs/specifications/category-mesh)\n- [Scenegraph](/docs/specifications/category-scenegraph)\n- [GIS](/docs/specifications/category-gis)\n","slug":"docs/developer-guide/loader-categories","title":"Loader Categories"},{"excerpt":"Polyfills Older browsers (mainly Edge and IE11) as well as Node.js do not provide certain APIs (,  etc) that loaders.gl depends on. The good…","rawMarkdownBody":"## Polyfills\n\nOlder browsers (mainly Edge and IE11) as well as Node.js do not provide certain APIs (`TextEncoder`, `fetch` etc) that loaders.gl depends on.\n\nThe good news is that these APIs can be provided by the application using the [polyfill](https://en.wikipedia.org/wiki/Polyfill_(programming)) technique.\n\nWhile there are many good polyfill modules for these classes available on `npm`, to make the search for a version that is guaranteed to work with loaders.gl a little easier, the `@loaders.gl/polyfills` module is provided.\n\nTo install these polyfills, just `import` the polyfills module before start using loaders.gl.\n\n```js\nimport '@loaders.gl/polyfills';\nimport {parse} from '@loaders.gl/core';\n```\n\n## Combining with other Polyfills\n\nloaders.gl only installs polyfills if the corresponding global symbol is `undefined`. This means that if another polyfill is already installed when `@loaders.gl/polyfills` is imported, the other polyfill will remain in effect. Since most polyfill libraries work this way, applications can mix and match polyfills by ordering the polyfill import statements appropriately (but see the remarks below for a possible caveat).\n\n\n## Provided Polyfills\n\nSee [API Reference](/docs/api-reference/polyfills).\n\n## Remarks\n\nApplications should typically only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, they will still be included in your application bundle, i.e. importing the polyfill module will increase your application's bundle size.\n\nWhen importing polyfills for the same symbol from different libraries, the import can depend on how the other polyfill is written. to control the order of installation, you may want to use `require` rather than `import` when importing `@loaders.gl/polyfills`. As a general rule, `import` statements execute before `require` statments.\n\n","slug":"docs/developer-guide/polyfills","title":" Polyfills"},{"excerpt":"AsyncIterators Streaming functionality in loaders.gl is built on the ES2018  concept. This page gives some background on AsyncIterator since…","rawMarkdownBody":"# AsyncIterators\n\nStreaming functionality in loaders.gl is built on the ES2018 `AsyncIterator` concept. This page gives some background on AsyncIterator since it is a recently introduced concept (at least as part of the JavaScript standard).\n\n## Availability\n\n`AsyncIterator` is a standard JavaScript ES2018 feature and is well supported by recent evergreen browsers and Node.js versions.\n\nThe `for await of` iteration syntax is supported as well as the babel transpiler.\n\n## Batched Parsing and Endcoding using AsyncIterators\n\nThe input and output from streaming loaders and writers can both be expressed in terms of async iterators.\n\n## Using AsyncIterator\n\nRemember tyhat an async iterator can be consumed (iterated over) via the for-await construct:\n\n```js\nfor await (const x of asyncIterable) {}\n```\n\n## Using Streams as AsyncIterators\n\nWith a little effort, streams in JavaScript can be treated as AsyncIterators. As the section about [Javascript Streams](docs/developer-guide/streams.md) explains, instead of registering callbacks on the stream, you can now work with streams in this way:\n\n```js\nfor await (const buf of fs.createReadStream('foo.txt')) {\n  // do something\n}\n```\n\n## Creating AsyncIterators\n\nRemember that any object in JavaScript that implements the `[Symbol.asyncIterator]()` method is an `AsyncIterable`. And the async generator syntax can be used to generate new async iterators\n\n```js\nasync function* asyncIterator() {\n  yield new Promise(...)\n}\n\nfor await (const x of asyncIterator()) {} // Notice parens after 'asyncIterator'\n```\n","slug":"docs/developer-guide/concepts/async-iterators","title":"AsyncIterators"},{"excerpt":"Binary Data The loaders.gl API consistently uses s to represent and transport binary data. Why ArrayBuffers? One of the design goals of…","rawMarkdownBody":"# Binary Data\n\nThe loaders.gl API consistently uses `ArrayBuffer`s to represent and transport binary data.\n\n## Why ArrayBuffers?\n\nOne of the design goals of loaders.gl is to provide applications with a single, consistent API that works across (reasonably modern) browsers, worker threads and Node.js. One of the characteristics of this API is how binary data is represented.\n\nloaders.gl \"standardizes\" on ArrayBuffers for a number of reasons:\n\n- ArrayBuffers are the \"canonical\" input format for the WebGL API, allowing efficient uploads of large binary data sets to the GPU.\n- ArrayBuffers allow ownership to be transferred between threads (Browser Main Thread and WebWorkers), massively improving performance when sending data back from loaders running on web worker to the application/main thread.\n- ArrayBuffers are used to transport raw data in most newer JavaScript APIs, including WebSockets, Web Intents, XMLHttpRequest version 2 etc.\n- ArrayBuffers are well supported by recent Node.js versions, in fact the traditional Node.js `Buffer` class is now backed by an `ArrayBuffer`.\n\n## ArrayBuffers and Typed Arrays\n\nRecall that typed arrays (e.g. `Float32Array`) are just views into array buffers. Every typed array has a `buffer` reference.\n\nMany loaders.gl functions directly accept typed arrays, which essentially means they accept the associated ArrayBuffer. However, be aware that typed arrays can represent partial views (i.e. they can have offsets) that sometimes need special handling in the application.\n\n## Converting between ArrayBuffers and Strings\n\nWe use the `TextEncoder` and `TextDecoder` classes in the JavaScript [string encoding/decoding library](https://github.com/inexorabletash/text-encoding).\n\nSince these classes are central to using ArrayBuffers correctly, loaders.gl provides polyfills for them under Node.js.\n\n## Binary Types in JavaScript\n\nBinary data types in JS:\n\n- `ArrayBuffer`\n- `Uint8Array` and other typed arrays, plus\n- `DataView`\n- `Blob`\n- `Buffer` nodejs\n\nExamples of \"semi-binary\" data types in JS:\n\n- `Array`: Array of bytes (elements are numbers between 0 and 255).\n- `String` (binary): string in “binary” form, 1 byte per char (2 bytes).\n- `String` (base64): string containing the binary data encoded in a base64 form.\n\n## Converting between ArrayBuffers and other Binary Formats.\n\nStandardizing on ArrayBuffers helps streamline the loaders.gl API. But occasionally applications need to interface with APIs that accept other binary data types/formats. To support this case, loaders.gl provides a small set of utilities (non-exhaustive) for converting from and to other binary JavaScript types/formats, e.g. `toArrayBuffer`:\n","slug":"docs/developer-guide/concepts/binary-data","title":"Binary Data"},{"excerpt":"Streaming Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node…","rawMarkdownBody":"# Streaming\n\n> Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node and DOM streams, through a consistent API and set of conventions (for both applications and loader/writer objects).\n\n## Streaming Loads\n\n### Incremental Parsing\n\nSome loaders offer incremental parsing (chunks of incomplete data can be parsed, and updates will be sent after a certain batch size has been exceeded). In many cases, parsing is fast compared to loading of data, so incremental parsing on its own may not provide a lot of value for applications.\n\n### Incremental Loading\n\nIncremental parsing becomes more interesting when it can be powered by incremental loading, whether through request updates or streams (see below).\n\n### Streamed Loading\n\nStreamed loading means that the entire data does not need to be loaded.\n\nThis is particularly advantageous when:\n\n- loading files with sizes that exceed browser limits (e.g. 1GB in Chrome)\n- doing local processing to files (tranforming one row at a time), this allows pipe constructions that can process files that far exceed internal memory.\n\n## Batched Updates\n\nFor incemental loading and parsing to be really effective, the application needs to be able to deal efficiently with partial batches as they arrive. Each loader category (or loader) may define a batch update conventions that are appropriate for the format being loaded.\n\n## Streaming Writes\n\nTBA\n\n## Node Streams vs DOM Streams\n\nStream support is finally arriving in browsers, however DOM Streams have a slightly different API than Node streams and the support across browsers is still spotty.\n\n## Polyfills\n\nStream support across browsers can be somewhat improved with polyfills. TBA\n\n## Stream Utilities\n\n- Stream to memory, ...\n- Automatically create stream if loader/writer only supports streaming\n- ...\n","slug":"docs/developer-guide/concepts/streaming","title":"Streaming"},{"excerpt":"@loaders.gl/polyfills Older browsers (mainly Edge and IE11) as well as versions of Node.js prior to v11 do not provide certain classes that…","rawMarkdownBody":"# @loaders.gl/polyfills\n\nOlder browsers (mainly Edge and IE11) as well as versions of Node.js prior to v11 do not provide certain classes that loaders.gl depends on.\n\nWhile there are many good polyfill modules available on `npm`, to make the search for a version that works perfectly with loaders.gl a little easier, a polyfill module is included.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/polyfills\n```\n\n## Usage\n\nJust import `@loaders.gl/polyfills` before you start using other loaders.gl modules.\n\n```js\nimport '@loaders.gl/polyfills';\nimport '@loaders.gl/core';\n```\n\n## Included Polyfills\n\n| Polyfill  | Node   | Browser  | Comments |\n| ---       | ---       | ---      |\n| `TextEncoder`/`TextDecoder` | Node.js < 11 | Yes (Older browsers) | Only UTF8 is guaranteed to be supported |\n| `atob`/`btoa` | All versions | No | Note: these functions are [not unicode safe](https://developer.mozilla.org/en-US/docs/Web/API/WindowBase64/Base64_encoding_and_decoding#The_Unicode_Problem), but OK to use for test cases. |\n| `fetch` | All versions | No | A subset of the fetch API is supported, see below. |\n\n## fetch Polyfill\n\nThe Node.js `fetch` polyfill supports a subset of the browser fetch API, including:\n- `Response.text()`, `Response.arrayBuffer()`.\n- `Response.body` stream\n- limited support for `headers`\n- data uri / base64 decoding\n\n\n# TextEncoder and TextDecoder Polyfills\n\n`TextEncoder` and `TextDecoder` polyfills are provided to ensure these APIs are always available. In modern browsers these will evaluate to the built-in objects of the same name, however under Node.js polyfills are transparently installed.\n\nNote: The provided polyfills only guarantee UTF8 support.\n\n## Remarks\n\n- Applications should only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, importing this module will increase the application's bundle size.\n- Refer to browser documentation for the usage of these classes, e.g. MDN.\n- In the browser, overhead of using these imports is very low, as most polyfills are only bundled under Node.js.\n- If working under older browsers, e.g. IE11, you may need to install your own TextEncoder/TextDecoder polyfills before loading this library\n\n## Attribution\n\nThe `Header` polyfill (for Node.js `fetch`) is a fork of the implementation in https://github.com/github/fetch (MIT license).\n","slug":"docs/api-reference/polyfills","title":"@loaders.gl/polyfills"},{"excerpt":"Worker Threads Reasons for moving loading to workers: When parsing is CPU heavy, the browser main thread can become blocked, freezing the…","rawMarkdownBody":"# Worker Threads\n\nReasons for moving loading to workers:\n\n- When parsing is CPU heavy, the browser main thread can become blocked, freezing the application until parsing completes.\n- Leverage multi-core CPUs when parsing multiple data items.\n\nConsiderations when moving loading and parsing to workers:\n\n- Data Transfer - Serializing/deserializing when transferring resuls back to main thread can more than defeat gains from loading on a separate thread.\n- Data Types - Due to data transfer issues there are constraints on what data types are appropriate\n- Configuration - Creating workers can require build system setup/configuration.\n- Message Passing - Parsing on workers requires message passing between threads. While simple it can add clutter to application code.\n- Debugging - Worker based code can be somewhat harder to debug. Being able to move the code back to the main thread can help.\n- Startup Times - Worker startup times can defeat speed gains from parsing on workers.\n- Thread Pool Management (TBA) -\n\n## Data Transfer\n\nThreads cannot share non-binary data structures and these have to be serialized/deserialized. This is a big issue for worker thread based loading as the purpose of loaders is typically to load and parse big datastructures, and main thread deserialization times are often comparable to or even exceed the time required to parse the data in the first place, defeating the value of moving parsing to a worker thread.\n\nThe solution is usually to use data types that support ownership transfer (see next section) as much as possible and minimize the amount of non-binary data returned from the parser.\n\n## Data Types\n\nJavaScript ArrayBuffers and Typed Arrays can be passed with minimal overhead (ownership transfer) and the value of worker based parsing usually depends on whether the loaded data can (mostly) be stored in these types.\n\n## Message Passing\n\nloaders.gl will handle message passing behind the scenes. Loading on a worker thread returns a promise that completes when the worker is done and the data has been transferred back to the main thread.\n\n## Build Configuration\n\nAll worker enabled loaders come with a pre-built, minimal worker \"executable\" to enable zero-configuration use in applications.\n\n## Bundle size concerns\n\nAll worker enabled loaders provide separate loader objects to ensure that tree-shaking bundlers will be able to remove the code for the unused case.\n\n## Debugging and Benchmarking\n\nLoaders.gl offers loader objects for main thread and worker threads. A simple switch lets you move your loading back to the main thread for easier debugging and benchmarking (comparing speeds to ensure you are gaining the benefits you expect from worker thread based loading).\n\n## Startup Times (TBA)\n\nThrough Thread Pool Management it will be possible to start worker threads before they ae needed to minimize worker loading delay when parsing.\n\n## Thread Pool Management (TBA)\n\nIt can be valuable to run muliple instances of the same worker thread to leverage multi-core CPUs. Being able to warm-up (pre-iniutilize) the thread pool and set limits of how many threads of each worker type...\n","slug":"docs/developer-guide/concepts/worker-threads","title":"Worker Threads"},{"excerpt":"ZipLoader Decodes a Zip Archive into a file map. Loader Characteristic File Extension  File Type Binary File Format ZIP Archive Data Format…","rawMarkdownBody":"# ZipLoader\n\nDecodes a Zip Archive into a file map.\n\n| Loader         | Characteristic |\n| -------------- | -------------- |\n| File Extension | `.zip`         |\n| File Type      | Binary         |\n| File Format    | [ZIP Archive](https://en.wikipedia.org/wiki/Zip_\\(file_format\\))   |\n| Data Format    | \"File Map\"     |\n| Decoder Type   | Asynchronous   |\n| Worker Thread  | No             |\n| Streaming      | No             |\n\n## Usage\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {ZipLoader} from '@loaders.gl/zip';\n\nconst fileMap = await parse(arrayBuffer, ZipLoader);\nfor (const fileName in FILE_MAP) {\n  const fileData = fileMap[key];\n  // Do something with the subfile\n}\n```\n\n## Data Format\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.loadAsync](https://stuk.github.io/jszip/documentation/api_jszip/load_async.html).\n","slug":"docs/api-reference/zip/zip-loader","title":"ZipLoader"},{"excerpt":"@loaders.gl/zip This module handles compressing and decompressing of the ZIP format. Installation Attributions ZipLoader is a wrapper around…","rawMarkdownBody":"# @loaders.gl/zip\n\nThis module handles compressing and decompressing of the [ZIP](https://en.wikipedia.org/wiki/Zip_\\(file_format\\)) format.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/zip\n```\n\n## Attributions\n\nZipLoader is a wrapper around the [JSZip module](https://stuk.github.io/jszip/). JSZip has extensive documentation on options (and more functionality than this loader object can expose).\n","slug":"docs/api-reference/zip","title":"@loaders.gl/zip"},{"excerpt":"@loaders.gl/ply This module handles the the Polygon file format, or the Stanford Trangle Format, a file format for 3D graphical objects…","rawMarkdownBody":"# @loaders.gl/ply\n\nThis module handles the the [Polygon file format](http://paulbourke.net/dataformats/ply/), or the Stanford Trangle Format, a file format for 3D graphical objects described as a collection of polygons.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/ply\n```\n\n## Attribution\n\nPLYLoader is a fork of the THREE.js PLYLoader under MIT License. The THREE.js source files contained the following attributions:\n\n@author Wei Meng / http://about.me/menway\n","slug":"docs/api-reference/ply","title":"@loaders.gl/ply"},{"excerpt":"ZipWriter Encodes a filemap into a Zip Archive. Returns an  that is a valid Zip Archive and can be written to file. Loader Characteristic…","rawMarkdownBody":"# ZipWriter\n\nEncodes a filemap into a Zip Archive. Returns an `ArrayBuffer` that is a valid Zip Archive and can be written to file.\n\n| Loader         | Characteristic |\n| -------------- | -------------- |\n| File Extension | `.zip`         |\n| File Type      | Binary         |\n| Data Format    | \"File Map\"     |\n| File Format    | [ZIP Archive](https://en.wikipedia.org/wiki/Zip_\\(file_format\\))   |\n| Encoder Type   | Asynchronous   |\n| Worker Thread  | No             |\n| Streaming      | No             |\n\n\n## Usage\n\n```js\nimport {encode, writeFile} from '@loaders.gl/core';\nimport {ZipWriter} from '@loaders.gl/zip';\n\nconst FILEMAP = {\n  filename1: arrayBuffer1,\n  'directory/filename2': ...\n};\n\nconst arrayBuffer = await encode(FILE_MAP, ZipWriter)\nwriteFile(zipFileName, arrayBuffer);\n```\n\n## File Format\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.generateAsync](https://stuk.github.io/jszip/documentation/api_jszip/generate_async.html), however type is always set to `arraybuffer` to ensure compatibility with writer driver functions in `@loaders.gl/core`.\n","slug":"docs/api-reference/zip/zip-writer","title":"ZipWriter"},{"excerpt":"PLYLoader The  parses simple meshes in the Polygon File Format or the Stanford Triangle Format. Loader Characteristic File Extension  File…","rawMarkdownBody":"# PLYLoader\n\nThe `PLYLoader` parses simple meshes in the Polygon File Format or the Stanford Triangle Format.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extension        | `.ply`          |\n| File Type             | Binary/Text     |\n| File Format           | [PLY](http://paulbourke.net/dataformats/ply/) |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md) |\n| Decoder Type          | Synchronous     |\n| Worker Thread Support | Yes             |\n| Streaming Support     | No              |\n\n## Usage\n\n```js\nimport {PLYLoader, PLYWorkerLoader} from '@loaders.gl/ply';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, PLYLoader, options);\n// Decode on worker thread\nconst data = await load(url, PLYWorkerLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n","slug":"docs/api-reference/ply/ply-loader","title":"PLYLoader"},{"excerpt":"PCDLoader The  loads point cloud in the Point Cloud Data (PCD) format. Loader Characteristic File Extension  File Type Text/Binary File…","rawMarkdownBody":"# PCDLoader\n\nThe `PCDLoader` loads point cloud in the Point Cloud Data (PCD) format.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extension        | `.pcd`          |\n| File Type             | Text/Binary     |\n| File Format           | [Point Cloud Data](http://pointclouds.org/documentation/tutorials/pcd_file_format.php) |\n| Data Format           | [PointCloud](docs/specifications/category-mesh.md) |\n| Decoder Type          | Synchronous     |\n| Worker Thread Support | Yes             |\n| Streaming Support     | No              |\n\nNote: Currently only `ascii` and `binary` subformats are supported. Compressed binary files are currently not supported.\n\n## Usage\n\n```js\nimport {PCDLoader, PCDWorkerLoader} from '@loaders.gl/pcd';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, PCSLoader, options);\n// Decode on worker thread\nconst data = await load(url, PCDWorkerLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n\n\n","slug":"docs/api-reference/pcd/pcd-loader","title":"PCDLoader"},{"excerpt":"OBJLoader The  parses the OBJ half of the classic Wavefront OBJ/MTL format.  Loader Characteristic File Extension  File Type Text File…","rawMarkdownBody":"# OBJLoader\n\nThe `OBJLoader` parses the OBJ half of the classic Wavefront OBJ/MTL format. \n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extension        | `.obj`          |\n| File Type             | Text            |\n| File Format           | [Wavefront OBJ file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md) |\n| Decoder Type          | Synchronous      |\n| Worker Thread Support | Yes              |\n| Streaming Support     | No               |\n\n## Usage\n\n```js\nimport {OBJLoader, OBJWorkerLoader} from '@loaders.gl/obj';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, OBJLoader, options);\n// Decode on worker thread\nconst data = await load(url, OBJWorkerLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n\n","slug":"docs/api-reference/obj/obj-loader","title":"OBJLoader"},{"excerpt":"@loaders.gl/obj This module handles the the Wavefront OBJ format, a simple ASCII format that defines 3D geometries as vertices, normals and…","rawMarkdownBody":"# @loaders.gl/obj\n\nThis module handles the the [Wavefront OBJ format](https://en.wikipedia.org/wiki/Wavefront_.obj_file), a simple ASCII format that defines 3D geometries as vertices, normals and faces.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/obj\n```\n\n## Attribution\n\nOBJLoader is a port of [three.js](https://github.com/mrdoob/three.js)'s OBJLoader under MIT License.\n","slug":"docs/api-reference/obj","title":"@loaders.gl/obj"},{"excerpt":"@loaders.gl/las This module handles the LASER file format (LAS) or its compressed version (LAZ), a public format for the interchange of…","rawMarkdownBody":"# @loaders.gl/las\n\nThis module handles the [LASER file format](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) (LAS) or its compressed version (LAZ), a public format for the interchange of 3-dimensional point cloud data data, developed for LIDAR mapping purposes.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/las\n```\n\n## Attribution\n\nLASLoader is a fork of Uday Verma and Howard Butler's [plasio](https://github.com/verma/plasio/) under MIT License.\n","slug":"docs/api-reference/las","title":"@loaders.gl/las"},{"excerpt":"@loaders.gl/pcd This module handles the the Point Cloud Data format, which encodes point cloud data for use inside Point Cloud Library (PCL…","rawMarkdownBody":"# @loaders.gl/pcd\n\nThis module handles the the [Point Cloud Data format](http://pointclouds.org/documentation/tutorials/pcd_file_format.php), which encodes point cloud data for use inside Point Cloud Library (PCL).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/pcd\n```\n\n## Attribution\n\nPCDLoader is a fork of the THREE.js PCDLoader under MIT License. The THREE.js source files contained the following attributions:\n\n- @author Filipe Caixeta / http://filipecaixeta.com.br\n- @author Mugen87 / https://github.com/Mugen87\n","slug":"docs/api-reference/pcd","title":"@loaders.gl/pcd"},{"excerpt":"LASLoader The  parses a point cloud in the LASER file format. Loader Characteristic File Extension ,  File Type Binary File Format LASER…","rawMarkdownBody":"# LASLoader\n\nThe `LASLoader` parses a point cloud in the LASER file format.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extension        | `.las`, `.laz`  |\n| File Type             | Binary          |\n| File Format           | [LASER file format](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) |\n| Data Format           | [PointCloud](docs/specifications/category-mesh.md) |\n| Decoder Type          | Synchronous     |\n| Worker Thread Support | Yes             |\n| Streaming Support     | No              |\n\n\n## Usage\n\n```js\nimport {LASLoader, LASWorkerLoader} from '@loaders.gl/las';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, LASLoader, options);\n// Decode on worker thread\nconst data = await load(url, LASWorkerLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `skip`        | Number    | `1`         | Read one from every _n_ points. |\n| `onProgress`  | Function  | -           | Callback when a new chunk of data is read. Only works on the main thread. |\n","slug":"docs/api-reference/las/las-loader","title":"LASLoader"},{"excerpt":"KMLLoader The  parses KML files into GeoJSON. Loader Characteristic File Extension  File Type Text File Format KML Data Format GIS Decoder…","rawMarkdownBody":"# KMLLoader\n\nThe `KMLLoader` parses KML files into GeoJSON.\n\n| Loader                | Characteristic   |\n| --------------------- | ---------------- |\n| File Extension        | `.kml`           |\n| File Type             | Text             |\n| File Format           | [KML](https://en.wikipedia.org/wiki/Keyhole_Markup_Language) |\n| Data Format           | [GIS](docs/specifications/category-gis.md) |\n| Decoder Type          | Synchronous      |\n| Worker Thread Support | No               |\n| Streaming Support     | No               |\n\n## Usage\n\n```js\nimport {KMLLoader} from '@loaders.gl/kml';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, KMLLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `useLngLatFormat` | Boolean | `true`  | KML longitudes and latitudes are specified as `[lat, lng]`. This option \"normalizes\" them to `[lng, lat]`. |\n| `useColorArrays`  | Boolean | `true`  | Convert color strings to arrays. |\n\n## Limitations\n\n- Currently XML parsing is only implemented in browsers, not in Node.js. Check `KMLLoader.supported` to check at run-time.\n","slug":"docs/api-reference/kml/kml-loader","title":"KMLLoader"},{"excerpt":"GLType Helper functions to work with WebGL data type constants. WebGL type constant JavaScript Typed Array Notes      Not yet directly…","rawMarkdownBody":"# GLType\n\nHelper functions to work with WebGL data type constants.\n\n| WebGL type constant | JavaScript Typed Array | Notes                                 |\n| ------------------- | ---------------------- | ------------------------------------- |\n| `GL.FLOAT`          | `Float32Array`         |                                       |\n| `GL.DOUBLE`         | `Float64Array`         | Not yet directly usable in WebGL/GLSL |\n| `GL.UNSIGNED_SHORT` | `Uint16Array`          |                                       |\n| `GL.UNSIGNED_INT`   | `Uint32Array`          |                                       |\n| `GL.UNSIGNED_BYTE`  | `Uint8Array`           |                                       |\n| `GL.UNSIGNED_BYTE`  | `Uint8ClampedArray`    |                                       |\n| `GL.BYTE`           | `Int8Array`            |                                       |\n| `GL.SHORT`          | `Int16Array`           |                                       |\n| `GL.INT`            | `Int32Array`           |                                       |\n\n## Usage\n\n```js\nimport {GL, GLType} from '@loaders.gl/math';\n// Returns Int8Array.BYTES_PER_ELEMENT\nvar size = GLType.getSizeInBytes(GL.BYTE);\n```\n\n## Static Methods\n\n### GLType.fromTypedArray(typedArray: Typed Array | Function) : Number\n\nReturns the size, in bytes, of the corresponding datatype.\n\n- `glType` The component datatype to get the size of.\n\nReturns\n\nThe size in bytes.\n\nThrows\n- glType is not a valid value.\n\n\nGets the {@link ComponentDatatype} for the provided TypedArray instance.\n\n-  array The typed array.\n\nReturns\n\nThe ComponentDatatype for the provided array, or undefined if the array is not a TypedArray.\n\n### GLType.getArrayType(glType: Number) : Function\n\nreturns the constructor of the array\n\n### static GLType.getByteSize(glType: Number) : Number\n\nReturns the size in bytes of one element of the provided WebGL type.\n\nEquivalent to `GLType.getArrayType(glType).BYTES_PER_ELEMENT`.\n\n### static GLType.validate(glType) : Boolean\n\nReturns `true` if `glType` is a valid WebGL data type.\n\n### static GLType.createTypedArray(glType : Number, buffer : ArrayBuffer [, byteOffset : Number [, length : Number]]) : TypedArray\n\nCreates a typed view of an array of bytes.\n\n- `glType` The type of typed array (ArrayBuffer view) to create.\n- `buffer` The buffer storage to use for the view.\n- `byteOffset`=`0` The offset, in bytes, to the first element in the view.\n- `length`= The number of elements in the view. Defaults to buffer length.\n\nReturns\n\n`Int8Array`|`Uint8Array`|`Int16Array`|`Uint16Array`|`Int32Array`|`Uint32Array`|`Float32Array`|`Float64Array` A typed array view of the buffer.\n\nThrows\n- `glType` is not a valid value.\n","slug":"docs/api-reference/math/gl-type","title":"GLType"},{"excerpt":"@loaders.gl/kml KML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google…","rawMarkdownBody":"# @loaders.gl/kml\n\nKML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google Earth (originally named \"Keyhole Earth Viewer\"). It can be used with any 2D or 3D maps.\n\nReferences:\n\n- [Keyhole Markup Language - Wikipedia](https://en.wikipedia.org/wiki/Keyhole_Markup_Language)\n- [KML Tutorial - Google](https://developers.google.com/kml/documentation/kml_tut)\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/kml\n```\n\n## Attribution\n\n`XMLLoader` is an adaptation of Nick Blackwell's [`js-simplekml`](https://github.com/nickolanack/js-simplekml) module under MIT license.\n","slug":"docs/api-reference/kml","title":"@loaders.gl/kml"},{"excerpt":"@loaders.gl/gltf This module provides loaders and writers of the GLB/gltF formats. Installation GLTFScenegraph API To simplify traversing…","rawMarkdownBody":"# @loaders.gl/gltf\n\nThis module provides loaders and writers of the GLB/gltF formats.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/draco @loaders.gl/gltf\n```\n\n## GLTFScenegraph API\n\nTo simplify traversing and building glTF data objects, the [`GLTFScenegraph`](docs/api-reference/gltf/gltf-scenegraph) class can be used.\n\nA glTF data object can also be built programmatically using the GLTFScenegraph's \"fluent API\":\n\n```js\nimport {encode} from '@loaders.gl/gltf';\nimport {GLTFScenegraph, GLTFWriter} from '@loaders.gl/gltf';\nconst gltfScenegraph = new GLTFScenegraph()\n  .addApplicationData(...)\n  .addExtras(...)\n  .addExtension(...)\n  .addRequiredExtension(...);\n\nconst arrayBuffer = encode(gltfScenegraph, GLTFWriter);\n```\n\n## GLTF Post Processing\n\nThe [`postProcessGLTF`](docs/api-reference/gltf/post-process-gltf) function implements a number of transformations on the loaded glTF data that would typically need to be performed by the application after loading the data, and is provided as an optional function that applications can call after loading glTF data. Refer to the reference page for details on what transformations are performed.\n\nContext: the glTF data object returned by the GLTF loader contains the \"raw\" glTF JSON structure (to ensure generality and \"data fidelity\" reasons). However, most applications that are going to use the glTF data to visualize it in (typically in WebGL) will need to do some processing of the loaded data before using it.\n\n## Using GLB as a \"Binary Container\" for Arbitrary Data\n\nThe GLB binary container format used by glTF addresses a general need to store a mix of JSON and binary data, and can potentially be used as a foundation for building custom loaders and writers.\n\nTo allow for this (and also to generally improve the glTF code structure), the `GLTFLoader` and `GLTFBuilder` classes are built on top of GLB focused classes (`GLBLoader` and `GLBBuilder`) that can be used independently of the bigger glTF classes.\n\n## glTF Extension Support\n\nCertain glTF extensions are fully or partially supported by the glTF classes. For details on which extensions are supported, see [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions).\n\n## Draco Mesh and Point Cloud Compression\n\nDraco encoding and decoding is supported by the `GLTFBuilder` and `GLTFParser` classes but requires the DracoWriter and DracoLoader dependencies to be \"injected\" by the application.\n\n```js\nimport {GLTFBuilder} from '@loaders.gl/gltf';\nimport {DracoWriter, DracoLoader} from '@loaders.gl/draco';\n\nconst gltfBuilder = new GLTFBuilder({DracoWriter, DracoLoader});\n```\n\n","slug":"docs/api-reference/gltf","title":"@loaders.gl/gltf"},{"excerpt":"GLBLoader The  parses a GLB binary \"envelope\". Note: applications that want to parse GLB-formatted glTF files use the  instead. The  is…","rawMarkdownBody":"# GLBLoader\n\nThe `GLBLoader` parses a GLB binary \"envelope\".\n\nNote: applications that want to parse GLB-formatted glTF files use the `GLTFLoader` instead. The `GLBLoader` is intended to be used to load custom data that combines JSON and binary resources.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extensions       | `.glb`          |\n| File Type             | Binary          |\n| File Format           | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Data Format           | See below       |\n| Decoder Type          | Synchronous     |\n| Worker Thread Support | No              |\n| Streaming Support     | No              |\n\n## Usage\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {GLBLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLBLoader);\n```\n\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `magic`       | Number    | glTF        | The magic number to be save in the file. |\n\n\n## Data Format\n\nReturns\n```json\n{\n  header: {\n    byteLength: number,\n    byteOffset: number\n  },\n\n  type: string,\n  version: number,\n\n  // JSON Chunk\n  json: any,\n\n  // BIN Chunk\n  hasBinChunk: boolean,\n  binChunks: [{\n    arrayBuffer: ArrayBuffer,\n    byteOffset: Number,\n    byteLength: Number\n  }]\n}\n```\n\n| Field         | Type          | Default   | Description        |\n| ---           | ---           | ---       | ---                |\n| `type`    | `String`      | `glTF`      | String containing the first four bytes of the file |\n| `version` | `Number`      | `2`       | The version number, only version 2 is supported |\n| `json`    | `Object`      | `{}`      | Parsed JSON from the JSON chunk     |\n| `binChunks`  | `ArrayBuffer` | `null`  | The binary chunk   |\n| `binChunks[\\*].arrayBuffer`  | `ArrayBuffer` | `null`  | The binary chunk   |\n| `binChunks[\\*].byteOffset`  | `Number` | `null`  | offset of BIN  (e.g. embedded in larger binary block)   |\n| `binChunks[\\*].byteLength`  | `ArrayBuffer` | `null`  |length of BIN (e.g. embedded in larger binary block)   |\n| `header.byteLength` | `Number`      | -       | length of GLB (e.g. embedded in larger binary block) |\n| `header.byteOffset` | `Number`      | 0       | offset of GLB  (e.g. embedded in larger binary block) |\n\n","slug":"docs/api-reference/gltf/glb-loader","title":"GLBLoader"},{"excerpt":"GLBWriter The  is a writer for the GLB binary \"envelope\". Note: applications that want to encode GLB-formatted glTF files use the  instead…","rawMarkdownBody":"# GLBWriter\n\nThe `GLBWriter` is a writer for the GLB binary \"envelope\".\n\nNote: applications that want to encode GLB-formatted glTF files use the `GLTFWriter` instead. The `GLBWriter` is intended to be used to save custom data that combines JSON and binary resources.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extensions       | `.glb`          |\n| File Type             | Binary          |\n| Data Format           | See below       |\n| File Format           | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Encoder Type          | Synchronous     |\n| Worker Thread Support | No              |\n| Streaming Support     | No              |\n\n## Usage\n\n```js\nimport {GLBWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLBWriter, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `magic`       | Number    | glTF        | The magic number to be save in the file. |\n\n\n## Data Format\n\nSee `GLBLoader`.\n\n| Field     | Type          | Default   | Description        |\n| ---       | ---           | ---       | ---                |\n| `magic`   | `Number`      | glTF      | The first four bytes of the file |\n| `version` | `Number`      | `2`       | The version number |\n| `json`    | `Object`      | `{}`      | The JSON chunk     |\n| `binary`  | `ArrayBuffer` | `null`    | The binary chunk   |\n","slug":"docs/api-reference/gltf/glb-writer","title":"GLBWriter"},{"excerpt":"glbdump  is a utility for inspecting the structure of GLB/glTF binary container files. Installing loaders.gl/gltf makes the  command line…","rawMarkdownBody":"## glbdump\n\n`glbdump` is a utility for inspecting the structure of GLB/glTF binary container files.\n\nInstalling loaders.gl/gltf makes the `glbdump` command line tool available. It can be run using `npx`.\n\n```\n$ npx glbdump <filename>\n```\n","slug":"docs/api-reference/gltf/glbdump","title":" glbdump"},{"excerpt":"glTF Extensions Arbitrary glTF extensions can be present in glTF files, and will remain present in the parsed JSON as you would expect. Such…","rawMarkdownBody":"# glTF Extensions\n\nArbitrary glTF extensions can be present in glTF files, and will remain present in the parsed JSON as you would expect. Such extensions can supported by applications by inspecting the `extensions` fields inside glTF objects, and it is up to each application to handle or ignore them.\n\nMany glTF extensions affect e.g. rendering which is outside of the scope of loaders.gl, however in a few cases it is possible to provide support for extensions directly during loading. This article describes glTF extensions that are fully or partially processed by the `@loaders.gl/gltf` classes.\n\n## Official Extensions\n\n### KHR_draco_mesh_compression\n\nSupports compression of mesh attributes (geometry).\n\nSpecification: [KHR_draco_mesh_compression](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_draco_mesh_compression).\n\nParsing Support:\n\n- By adding the `decompress: true` options to the `GLTFParser` any decompressed by the `GLTFParser`.\n- The expanded attributes are placed in the mesh object (effectively making it look as if it had never been compressed).\n- The extension objects are removed from the glTF file.\n\nEncoding Support:\n\n- Meshes can be compressed as they are added to the `GLTFBuilder`.\n\n### KHR_lights_punctual\n\nSupports specification of point light sources and addition of such sources to the scenegraph node.\n\nSpecification: [KHR_lights_punctual](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_lights_punctual)\n\nParsing Support:\n\n- Any nodes with a `KHR_lights_punctual` extension will get a `light` field with value containing a light definition object with properties defining the light (this object will be resolved by index from the global `KHR_lights_punctual` extension object's `lights` array) .\n- The `KHR_lights_punctual` extensions will be removed from all nodes.\n- Finally, the global `KHR_lights_punctual` extension (including its light list)) will be removed.\n\nEncoding Support:\n\n- N/A\n\n## Custom Extensions\n\n### UBER_draco_point_cloud_compression\n\nSpecification: Similar to `KHR_draco_mesh_compression`, but supports point clouds (draw mode 0). Also does not support any fallback or non-compressed accessors/attributes.\n\nParsing support:\n\n- The primitive's accessors field will be populated after decompression.\n- After decompression, the extension will be removed (as if the point cloud was never compressed).\n\nEncoding support:\n\n- Point clouds can be compressed as they are added to the `GLTFBuilder` and decompressed by the `GLTFParser`.\n","slug":"docs/api-reference/gltf/gltf-extensions","title":"glTF Extensions"},{"excerpt":"GLTFScenegraph The  class provides an API for accessing and modifying glTF data. Caveat: Modification of existing binary data chunks has…","rawMarkdownBody":"# GLTFScenegraph\n\nThe `GLTFScenegraph` class provides an API for accessing and modifying glTF data.\n\n> Caveat: Modification of existing binary data chunks has limitations, this class is not intended to be a generic utility for modifying existing glTF data.\n\n## Usage\n\n```js\nimport {GLTFLoader, GLTFScenegraph} from '@loaders.gl/gltf';\nimport {load} from '@loaders.gl/core';\n\n// Load and parse a file\nconst gltfData = await parse(fetch(GLTF_URL), GLTFLoader);\n\n// Create a parser\nconst gltf = new GLTFScenegraph(gltfData);\n\n// Get the complete glTF JSON structure\nconst gltfJson = gltf.getJSON();\n\n// Get specific top-level fields from the glTF JSON chunk\nconst appData = gltf.getApplicationData('customData');\n\n// Get a top level extension from the glTF JSON chunk\nconst topLevelExtension = gltf.getExtension('ORGNAME_extensionName');\nif (topLevelExtension) {\n  ...\n}\n\n// Get images from the binary chunk (together with metadata)\nconst imageIndex = 0;\nconst image = gltf.getImage(imageIndex);\n\n// Get default glTF scenegraph\nconst scenegraph = gltf.getScene();\n// Get specific glTF scenegraph\nconst scenegraph = gltf.getScene(2);\n```\n\n\n## Accessor Methods\n\n### constructor(gltf : Object)\n\nCreates a new `GLTFScenegraph` instance from a pure JavaScript object.\n\n#### json()\n\n#### getApplicationData(key : String) : Object\n\nReturns the given data field in the top-level glTF JSON object.\n\n#### getExtraData(key : String) : Object?\n\nReturns a key in the top-level glTF `extras` JSON object.\n\n#### getExtension(name : String) : Object?\n\nReturns the top-level extension by `name`, if present.\n\n#### getUsedExtensions() : String[]\n\nReturns an array of extension names (covering all extensions used at any level of the glTF hierarchy).\n\n#### getRequiredExtensions() : String[]\n\nReturns an array of extensions at any level of the glTF hierarchy that are required to properly display this file (covering all extensions used at any level of the glTF hierarchy).\n\n#### getObjectExtension(object, extensionName)\n\n\n#### getScene([index : Number]) : Object?\n\nReturns the scene (scenegraph) with the given index, or the default scene if no index is specified.\n\n#### getScene(index : Number) : Object\n\n#### getNode(index : Number) : Object\n\n#### getSkin(index : Number) : Object\n\n#### getMesh(index : Number) : Object\n\n#### getMaterial(index : Number) : Object\n\n#### getAccessor(index : Number) : Object\n\n#### getCamera(index : Number) : Object\n\n#### getTexture(index : Number) : Object\n\n#### getSampler(index : Number) : Object\n\n#### getImage(index : Number) : Object\n\nReturns the image with specified index\n\n#### getBufferView(index : Number) : Object\n\n#### getBuffer(index : Number) : Object\n\n#### getTypedArrayForBufferView(bufferView : Number | Object) : Uint8Array\n\nAccepts buffer view index or buffer view object\n\n#### getTypedArrayForAccessor(accessor : Number | Object) : Uint8Array | Float32Array | ...\n\nAccepts accessor index or accessor object.\n\nReturns a typed array with type that matches the types\n\n#### getTypedArrayForImageData(image : Number | Object) : Uint8Array\n\naccepts accessor index or accessor object\n\n## Modifiers\n\n#### addApplicationData(key, data)\n\nAdd an extra application-defined key to the top-level data structure\n\n#### addExtraData(key, data)\n\n`extras` - Standard GLTF field for storing application specific data\n\nAdd to GLTF top level extension object, mark as used\n\n##### addRequiredExtension(extensionName, data)\n\nAdd GLTF top level extension object, mark as used and required\n\n#### registerUsedExtension(extensionName)\n\nAdd extensionName to list of used extensions\n\n#### registerRequiredExtension(extensionName)\n\nAdd extensionName to list of required extensions\n\n#### removeExtension(extensionName)\n\nRemoves an extension from the top-level list\n\n#### setObjectExtension(object, extensionName, data)\n\n#### addMesh(attributes, indices, mode = 4)\n\n#### addPointCloud(attributes)\n\n#### addBufferView(buffer)\n\nAdd one untyped source buffer, create a matching glTF `bufferView`, and return its index\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n#### addAccessor(bufferViewIndex, accessor)\n\nAdds an accessor to a bufferView\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n#### addImage(imageData, mimeType)\n\nAdds a binary image. Builds glTF \"JSON metadata\" and saves buffer reference\nBuffer will be copied into BIN chunk during \"pack\"\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n#### createBinChunk()\n\nPacks any pending binary data into the first binary glTF buffer.\n\nNote: Overwrites the existing first buffer if present.\n","slug":"docs/api-reference/gltf/gltf-scenegraph","title":"GLTFScenegraph"},{"excerpt":"@loaders.gl/math This library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes. Classes…","rawMarkdownBody":"# @loaders.gl/math\n\n> This library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes.\n\nClasses and utilities to help working with geometries (arrays of vertices) stored in typed arrays according to WebGL/OpenGL layout rules.\n\n## Usage Examples\n\n## Framework Independence\n\nLike all non-core math.gl modules, this library can be used without the math.gl core classes.\n\n- Any input vectors can be supplied as length 3 JavaScript `Array` instances.\n- Any result vectors can be treated as length 3 JavaScript `Array` instances (they may be math.gl `Vector3`).\n- The core math.gl classes inherit from JavaScript `Array` and can be used directly as input.\n","slug":"docs/api-reference/math","title":"@loaders.gl/math"},{"excerpt":"GLTFWriter The  is a writer for glTF scenegraphs. Loader Characteristic File Extensions , File Types Binary, JSON, Linked Assets Data Format…","rawMarkdownBody":"# GLTFWriter\n\nThe `GLTFWriter` is a writer for glTF scenegraphs.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extensions       | `.glb`,`.gltf`  |\n| File Types            | Binary, JSON, Linked Assets |\n| Data Format           | [Scenegraph](/docs/specifications/category-scenegraph) |\n| File Format           | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Encoder Type          | Synchronous (limited), Asynchronous |\n| Worker Thread Support | No              |\n| Streaming Support     | No              |\n\n## Usage\n\n```js\nimport {GLTFWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLTFWriter, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `DracoWriter` | [DracoWriter](/docs/api-reference/draco/draco-writer) | `null` | To enable DRACO encoding, the application needs to import and supply the `DracoWriter` class. |\n| `DracoLoader` | [DracoLoader](/docs/api-reference/draco/draco-loader) | `null` | To enable DRACO encoding, the application needs to import and supply the `DracoLoader` class. |\n","slug":"docs/api-reference/gltf/gltf-writer","title":"GLTFWriter"},{"excerpt":"GLTFLoader Parses a glTF file. Can load both the  (binary) and  (text/json) file format variants. A glTF file contains a hierarchical…","rawMarkdownBody":"# GLTFLoader\n\nParses a glTF file. Can load both the `.glb` (binary) and `.gltf` (text/json) file format variants.\n\nA glTF file contains a hierarchical scenegraph description that can be used to instantiate corresponding hierarcy of actual `Scenegraph` related classes in most WebGL libraries.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extensions       | `.glb`, `.gltf` |\n| File Type             | Binary, JSON, Linked Assets |\n| File Format           | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Data Format           | [Scenegraph](/docs/specifications/category-scenegraph) |\n| Decoder Type          | Synchronous (limited), Asynchronous |\n| Worker Thread Support | No              |\n| Streaming Support     | No              |\n\nThe `GLTFLoader` aims to take care of as much processing as possible, while remaining framework-independent.\n\nThe GLTF Loader returns an object with a `json` field containing the glTF Scenegraph. In its basic mode, the `GLTFLoader` does not modify the loaded JSON in any way. Instead, the results of additional processing are placed in parallel top-level fields such as `buffers` and `images`. This ensures that applications that want to work with the standard glTF data structure can do so.\n\nOptionally, the loaded gltf can be \"post processed\", which lightly annotates and transforms the loaded JSON structure to make it easier to use. Refer to [postProcessGLTF](docs/api-reference/gltf-loaders/gltf-extensions.md) for details.\n\nIn addition, certain glTF extensions, in particular Draco mesh encoding, can be fully or partially processed during loading. When possible (and extension processing is enabled), such extensions will be resolved/decompressed and replaced with standards conformant representations. See [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions.md) for more information.\n\nNote: while supported, synchronous parsing of glTF (e.g. using `parseSync()`) has significant limitations. When parsed asynchronously (using `await parse()` or `await load()`), the following additional capabilities are enabled:\n\n- linked binary resource URI:s will be loaded and resolved (assuming a valid base url is available).\n- base64 encoded binary URI:s inside the JSON payload will be decoded.\n- linked image URI:s can be loaded and decoded.\n- Draco meshes can be decoded asynchronously on worker threads (in parallel!).\n\n## Usage\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLTFLoader);\n```\n\nTo decompress Draco-compressed meshes:\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nimport {DracoLoader} from '@loaders.gl/draco';\nconst gltf = load(url, GLTFLoader, {DracoLoader, decompress: true});\n```\n\n## Options\n\n| Option        | Type      | Default Async | Sync  | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `fetchLinkedResources` | Boolean  | `true`  | No | Fetch any linked .BIN files, decode base64 encoded URIS. Async only. |\n| `fetchImages`          | Boolean  | `false` | No     | Fetch any referenced image files (and decode base64 encoded URIS). Async only. |\n| `createImages`         | Boolean  | `false` | Create image objects from loaded image data. |\n| `fetch`                | Function | `fetch` | N/A | Function used to fetch linked resources. |\n| `uri`                  | String | `fetch` | N/A | Function used to fetch linked resources. |\n| `decompress`           | Boolean  | `true`  | Yes | Decompress Draco compressed meshes (if DracoLoader available). |\n| `DracoLoader`          | [DracoLoader](/docs/api-reference/draco/draco-loader) | `null`  | Yes\\*    | Supply to enable decoding of Draco compressed meshes. \\* `DracoWorkerLoader` is async only. |\n| `postProcess`          | Boolean  | `false` | Perform additional [post processing](docs/api-reference/post-process-gltf) to simplify use in WebGL libraries. |\n\n\n\n## Data Format\n\nReturns\n```json\n{\n  // The base URI used to load this glTF, if any. For resolving relative uris to linked resources.\n  baseUri: String,\n\n  // JSON Chunk\n  json: Object,\n\n  // Length and indices of this array will match `json.buffers`\n  // The GLB bin chunk, if present, will be found in buffer 0.\n  // Additional buffers are fetched or base64 decoded from the JSON uri:s.\n  buffers: [{\n    arrayBuffer: ArrayBuffer,\n    byteOffset: Number,\n    byteLength: Number\n  }],\n\n  // Images can optionally be loaded and decoded, they will be stored here\n  // Length and indices of this array will match `json.buffers`\n  images: Image[],\n\n  // GLBLoader output, if this was a GLB encoded glTF\n  _glb?: Object\n}\n```\n\n| Field         | Type     | Default   | Description        |\n| ---           | ---      | ---       | ---                |\n| `baseUri` | `String`     | ``        | length of GLB (e.g. embedded in larger binary block) |\n| `json`    | `Object`     | `{}`      | Parsed JSON from the JSON chunk     |\n| `buffers` | `Object[]`   | `[]`      | The version number |\n| `buffers[\\*].arrayBuffer` | `ArrayBuffer` | `null`  | The binary chunk   |\n| `buffers[\\*].byteOffset`  | `Number`  | `null`  | offset of buffer (embedded in larger binary block)   |\n| `buffers[\\*].byteLength`  | `ArrayBuffer` | `null`  | length of buffer (embedded in larger binary block)   |\n| `_glb`?     | `Object`    | N/A       | The output of the GLBLoader if the parsed file was GLB formatted |\n\n","slug":"docs/api-reference/gltf/gltf-loader","title":"GLTFLoader"},{"excerpt":"@loaders.gl/images This module contains loader and writers for images that follow loaders.gl conventions and work under both node and…","rawMarkdownBody":"# @loaders.gl/images\n\nThis module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/images\n```\n","slug":"docs/api-reference/images","title":"@loaders.gl/images"},{"excerpt":"postProcessGLTF The  function transforms parsed GLTF JSON to make it easier to use. It adds loaded buffers and images to the glTF JSON…","rawMarkdownBody":"# postProcessGLTF\n\nThe `postProcessGLTF` function transforms parsed GLTF JSON to make it easier to use.\n- It adds loaded buffers and images to the glTF JSON objects\n- It creates typed arrays for buffer views\n\n## Usage\n\nTo post process just pass a gltf object to the `GLTFPostProcessor`\n```js\nimport {GLTFLoader, postProcessGLTF} from '@loaders.gl/gltf';\nconst gltf = await parse(..., GLTFLoader);\nconst processedGLTF = postProcesssGLTF(gltf);\n````\n\nAfter post-processing, the gltf scenegraphs are now easier to iterate over\n```js\nconst scenegraph = processedGLTF.scenegraphs[0];\nfor (const node of scenegraph.nodes) { // no need to resolve indices\n  if (node.mesh.primitives) { // Ditto\n  \t// ...\n  }\n}\n```\n\n## Functions\n\n### postProcessGLTF(gltf : Object, options? : Object) : Object\n\n- `gltf` is expected to have `json` and `buffers` fields per the GLTF Data Format Category.\n- `options.uri` - Set base URI (for image loading)\n\nThe GLTF post processor copies objects in the input gltf json field as necessary to avoid modifying the input JSON, but does not do a deep copy on sub-objects that do not need to be modified.\n\n\n## General Post Processing\n\n### Replace indices with references\n\nThe GLTF file format links nodes through indices. The `nodes` field in an object in the top-level glTF `scenegraph` array. is an array of indices into the top-level `nodes` array. Each node has a `mesh` attribute that is an index into to the `meshes` array, and so on.\n\nHaving to follow indices is inconvenient when working with the gltf data in JavaScript. So during post processing, indices will be replaced with references to the indexed objects, enabling applications to use simple iteration to follow the scenegraph.\n\n\n### Adds `id` to every node\n\nUnless already present.\n\n## Node Specific Post Processing\n\n### Buffers\n\nThe following fields will be populated from the supplied `gltf.buffers` parameter (this parameter is populated by the loader via `options.loadLinkedResources: true`):\n\n- `buffer.arrayBuffer` -\n- `buffer.byteOffset` -\n- `buffer.byteLength` -\n\n### BufferViews\n\n- `bufferView.data` - Typed arrays (`Uint8Arrays`) will be created for buffer views and stored in this field. These typed arrays can be used to upload data to WebGL buffers.\n\n### Accessors\n\nThe accessor parameters which are textual strings in glTF will be resolved into WebGL constants.\n\n## Images\n\n- `image.image` - Populated from the supplied `gltf.images` array. This array is populated by the `GLTFLoader` via `options.loadImages: true`):\n- `image.uri` - If loaded image in the `images` array is not available, uses `gltf.baseUri` or `options.baseUri` is available, to resolve a relative URI and replaces this value.\n\n### Materials\n\n- `...texture` - Since each texture object in the material has an `...index` field next to other fields, the post processor will add a `...texture` field instead of replacing the `...index` field.\n\n### Samplers\n\nModifies\n- `parameters` - see table\n\nSampler parameters (which are textual in glTF) will be resolved into WebGL constants.\n\n| glTF constant | WebGL constant |\n| --- | --- |\n| `magFilter` | `GL.TEXTURE_MAG_FILTER` |\n| `minFilter` | `GL.TEXTURE_MIN_FILTER` |\n| `wrapS` | `GL.TEXTURE_WRAP_S` |\n| `wrapT` | `GL.TEXTURE_WRAP_T` |\n\n\n### Texture\n\nModifies\n- `sampler` - will be resolved the the corresponding image object.\n- `source` - will be resolved the the corresponding image object.\n","slug":"docs/api-reference/gltf/post-process-gltf","title":"postProcessGLTF"},{"excerpt":"ImageLoader An image loader that works under both Node.js (requires ) and the browser. Loader Characteristic File Extension…","rawMarkdownBody":"# ImageLoader\n\nAn image loader that works under both Node.js (requires `@loaders.gl/polyfills`) and the browser.\n\n| Loader         | Characteristic |\n| -------------- | -------------- |\n| File Extension | `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.bmp`, `.ico`, `.svg`         |\n| File Type      | Binary         |\n| File Format    | Image          |\n| Data Format    | `Image`, `ImageBitmap` (web worker) or ndarray (node.js) |\n| Decoder Type   | Asynchronous   |\n| Worker Thread  | No             |\n| Streaming      | No             |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfill';  // only if using under Node\nimport {ImageLoader} from '@loaders.gl/images';\nimport {load} from '@loaders.gl/core';\n\nconst image = await load(url, ImageLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `crossOrigin` | String    | -           | passed to [Image.crossorigin](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img). |\n\n## Remarks\n\n- While generic, the `ImageLoader` is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a `WebGLTexture` both in the browser and in headless gl under Node.js\n- Node.js support requires import `@loaders.gl/polyfills` before installing this module.\n","slug":"docs/api-reference/images/image-loader","title":"ImageLoader"},{"excerpt":"Image Utilities A set of functions to help determine the type and size of binary images. Usage Functions isImage(imageData : ArrayBuffer…","rawMarkdownBody":"# Image Utilities\n\nA set of functions to help determine the type and size of binary images.\n\n## Usage\n\n```js\nconst arrayBuffer = await fetchFile(imageUrl).then(response => response.arrayBuffer());\n\nconst mimeType = getImageMIMEType(arrayBuffer);\nconst {width, height} = getImageSize(arrayBuffer, mimeType);\n```\n\n## Functions\n\n### isImage(imageData : ArrayBuffer [, mimeType : String]) : Boolean\n\nParameters:\n- `imageData`: Binary encoded image data.\n- `mimeType`: A MIME type string.\n\nReturns `true` if the binary data represents a known binary image format or matches the supplied `mimeType`.\n\nParameters:\n- `mimeType`: If supplied, checks if the image is of that type. If not supplied, returns `true` if imageData corresponds to a know supported image format.\n\n### getImageMIMEType(imageData : ArrayBuffer) : String | null\n\nParameters:\n- `imageData`: Binary encoded image data.\n\nReturns:\n- the MIME type of the image represented by the data, or `null` if it could not be identified.\n\n### getImageSize(imageData : ArrayBuffer [, mimeType : String]) : Object\n\nExtracts the size of the image in `imageData`. If `mimeType` is supplied, assumes the image is of that type. If not supplied, first attempts to auto deduce the image format (see `getImageMIMEType`).\n\nParameters:\n- `imageData`: Binary encoded image data.\n- `mimeType`: A MIME type string.\n\nReturns:\n- an object with fields containing the size of the image represented by the data.\n\n```js\n{\n  width: Number,\n  height: Number\n}\n```\n\nThrows:\n- if image is not in a supported format.\n\n### getImageMetadata(imageData : ArrayBuffer [, mimeType : String]) : Object\n\nExtracts the size of the image in `imageData`. If `mimeType` is supplied, assumes the image is of that type. If not supplied, first attempts to auto deduce the image format (see `getImageMIMEType`).\n\nParameters:\n- `imageData`: Binary encoded image data.\n- `mimeType`: A MIME type string.\n\nReturns:\n- an object with fields containing the size and mimeType of the image represented by the data.\n\n```js\n{\n  width: Number,\n  height: Number,\n  mimeType: String\n}\n```\n\nThrows:\n- if image is not in a supported format.\n\n## Supported Formats\n\nCurrently supported image formats and MIME types are:\n\n| Format | MIME Type    |\n| ---    | ---          |\n| PNG    | `image/png`  |\n| JPEG   | `image/jpeg` |\n| GIF    | `image/gif`  |\n| BMP    | `image/bmp`  |\n","slug":"docs/api-reference/images/image-utilities","title":"Image Utilities"},{"excerpt":"ImageWriter The  class can encode an image into  both under browser and Node.js Loader Characteristic File Extension , ,  File Format Binary…","rawMarkdownBody":"# ImageWriter\n\nThe `ImageWriter` class can encode an image into `ArrayBuffer` both under browser and Node.js\n\n| Loader         | Characteristic |\n| -------------- | -------------- |\n| File Extension | `.png`, `.jpg`, `.jpeg`  |\n| File Format    | Binary         |\n| Data Format    | `ArrayBuffer`  |\n| File Format    | Image          |\n| Encoder Type   | Asynchronous   |\n| Worker Thread  | No             |\n| Streaming      | No             |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfill';  // only if using under Node\nimport {ImageWriter} from '@loaders.gl/images';\nimport {encode} from '@loaders.gl/core';\n\nconst image = await encode(arrayBuffer, ImageWriter, options);\n```\n\n## Options\n\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `type`        | String    | `'png'`     | image type \\*     |\n\n\\* Supported image types (MIME types) depends on the environment. Typically PNG and JPG are supported.\n","slug":"docs/api-reference/images/image-writer","title":"ImageWriter"},{"excerpt":"loadImage Functions loadImage(url : String , options : Object) : Image / HTMLImageElement This is a minimal basic image loading function…","rawMarkdownBody":"# loadImage\n\n## Functions\n\n### loadImage(url : String [, options : Object]) : Image / HTMLImageElement\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/browser-only-red.svg?style=flat-square\" alt=\"browser only\" />\n</p>\n\nThis is a minimal basic image loading function that only works in the browser main threaqd. For image loading and writing that works across both browser and node, refer to the `@loaders.gl/images` module.\n\n`options.crossOrigin` - Provides control of the requests cross origin field.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n","slug":"docs/api-reference/images/load-image","title":"loadImage"},{"excerpt":"@loaders.gl/draco This module handles compressing and decompressing of 3D meshes and point clouds with DRACO. Installation Attributions…","rawMarkdownBody":"# @loaders.gl/draco\n\nThis module handles compressing and decompressing of 3D meshes and point clouds with [DRACO](https://github.com/google/draco).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/draco\n```\n\n## Attributions\n\nBased on Draco examples, under the Apache 2.0 license.\n","slug":"docs/api-reference/draco","title":"@loaders.gl/draco"},{"excerpt":"DracoWriter The  encodes a mesh or point cloud (maps of attributes) using Draco3D compression. Loader Characteristic File Extension  File…","rawMarkdownBody":"# DracoWriter\n\nThe `DracoWriter` encodes a mesh or point cloud (maps of attributes) using [Draco3D](https://google.github.io/draco/) compression.\n\n| Loader                | Characteristic   |\n| --------------------- | ---------------- |\n| File Extension        | `.drc`           |\n| File Typoe            | Binary           |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md) |\n| File Format           | [Draco](https://google.github.io/draco/) |\n| Encoder Type          | Synchronous      |\n| Worker Thread Support | Yes              |\n| Streaming Support     | No               |\n\n## Usage\n\n```js\nimport {DracoWriter} from '@loaders.gl/draco';\nimport {encode} from '@loaders.gl/core';\n\nconst mesh = {\n  attributes: {\n    POSITION: {...}\n  }\n};\n\nconst data = await encode(mesh, DracoWriter, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `pointcloud`  | Boolean   | `false`     | set to `true` to compress pointclouds (mode=`0` and no `indices`). |\n| `method`      | String    | `MESH_EDGEBREAKER_ENCODING` | set Draco encoding method (applies to meshes only). |\n| `speed`       | [Number, Number] | set Draco speed options. |\n| `log`         | Function  | callback for debug info. |\n","slug":"docs/api-reference/draco/draco-writer","title":"DracoWriter"},{"excerpt":"DracoLoader The  decodes a mesh or point cloud (maps of attributes) using DRACO compression. Loader Characteristic File Extension  File Type…","rawMarkdownBody":"# DracoLoader\n\nThe `DracoLoader` decodes a mesh or point cloud (maps of attributes) using [DRACO](https://google.github.io/draco/) compression.\n\n| Loader                | Characteristic   |\n| --------------------- | ---------------- |\n| File Extension        | `.drc`           |\n| File Type             | Binary           |\n| File Format           | [Draco](https://google.github.io/draco/) |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md) |\n| Decoder Type          | Synchronous      |\n| Worker Thread Support | Yes              |\n| Streaming Support     | No               |\n\n## Usage\n\n```js\nimport {DracoLoader} from '@loaders.gl/draco';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, DracoLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n","slug":"docs/api-reference/draco/draco-loader","title":"DracoLoader"},{"excerpt":"@loaders.gl/csv This module handles tabular data stored in the CSV/DSV file format.  Installation Attributions CSVLoader is based on a…","rawMarkdownBody":"# @loaders.gl/csv\n\nThis module handles tabular data stored in the [CSV/DSV file format](https://en.wikipedia.org/wiki/Comma-separated_values). \n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/csv\n```\n\n## Attributions\n\nCSVLoader is based on a minimal fork of the [papaparse](https://github.com/mholt/PapaParse) module, under MIT license.\n","slug":"docs/api-reference/csv","title":"@loaders.gl/csv"},{"excerpt":"CSVLoader Streaming loader for comma-separated value and delimiter-separated value encoded files. Loader Characteristic File Extension…","rawMarkdownBody":"# CSVLoader\n\nStreaming loader for comma-separated value and [delimiter-separated value](https://en.wikipedia.org/wiki/Delimiter-separated_values) encoded files.\n\n| Loader                | Characteristic   |\n| --------------------- | ---------------- |\n| File Extension        | `.csv`, `.dsv`   |\n| File Type             | Text             |\n| File Format           | [RFC4180](https://tools.ietf.org/html/rfc4180) |\n| Data Format           | [Classic Table](/docs/specifications/category-table)   |\n| Decoder Type          | Synchronous, Asynchronous |\n| Worker Thread Support | No               |\n| Streaming Support     | Yes              |\n\n## Usage\n\n```js\nimport {CSVLoader} from '@loaders.gl/obj';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, CSVLoader, options);\n```\n\n## Options\n\nThe following options are passed on to [papaparse](https://www.papaparse.com/docs#config):\n\n\n| Option        | Type      | Default      | Description       |\n| ------------- | --------- | ------------ | ----------------- |\n| `delimiter`   | String    | auto-detect  | The delimiting character. |\n| `header`      | Boolean   | auto-detect  | If `true`, the first row of parsed data will be interpreted as field names. If `false`, the first row is interpreted as data. |\n| `newline`     | String    | auto-detect  | The newline sequence. Must be `\\r`, `\\n`, or `\\r\\n`. |\n| `quoteChar`   | String    | `\"`          | The character used to quote fields. |\n| `escapeChar`  | String    | `\"`          | The character used to escape the quote character within a field. |\n| `dynamicTyping` | Boolean | `true`       | If `true`, numeric and boolean data values will be converted to their type (instead if strings). |\n| `comments`    | String   | `false`      | Comment indicator (for example, \"#\" or \"//\"). Lines starting with this string are skipped. |\n| `skipEmptyLines` | String | `false`    | If `true`, lines that are completely empty (those which evaluate to an empty string) will be skipped. If set to `'greedy'`, lines that don't have any content (those which have only whitespace after parsing) will also be skipped. |\n| `transform`   | Function | -           | A function to apply on each value. The function receives the value as its first argument and the column number or header name when enabled as its second argument. The return value of the function will replace the value it received. The transform function is applied before dynamicTyping. |\n| `delimitersToGuess` | Array | `[',', '\\t', '|', ';']` | An array of delimiters to guess from if the `delimiter` option is not set. |\n| `fastMode`   | Boolean   | auto-detect  | Force set \"fast mode\". Fast mode speeds up parsing significantly for large inputs but only works when the input has no quoted fields. Fast mode will be auto enabled if no `\"` characters appear in the input. |\n","slug":"docs/api-reference/csv/csv-loader","title":"CSVLoader"},{"excerpt":"Binary Utilities loaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal…","rawMarkdownBody":"# Binary Utilities\n\nloaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal with binary data in the JavaScript APIs for browser and Node.js, and some small but annoying \"gotchas\" that can trip up programmers when working with binary data.\n\n## Usage\n\n```js\nimport {toArrayBuffer} from '@loaders.gl/core';\n```\n\n## Functions\n\n### toArrayBuffer(binaryData : \\*) : ArrayBuffer\n\n\"Repackages\" a binary data in non-array-buffer form as an `ArrayBuffer`.\n\n- binaryData - ArrayBuffer, Buffer (Node.js), typed array, blob, ...\n\n## Remarks\n\n- Most functions in loaders.gl that accept binary data call `toArrayBuffer(...)` on input parameters before starting processing, thus ensuring that functions work on all types of input data.\n","slug":"docs/api-reference/core/binary-utilities","title":"Binary Utilities"},{"excerpt":"encode Functions encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object , url : String]) : Promise.Any Encodes…","rawMarkdownBody":"# encode\n\n## Functions\n\n### encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object [, url : String]]) : Promise.Any\n\nEncodes data asynchronously using the provided writer.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n\n- `options.log`=`console` Any object with methods `log`, `info`, `warn` and `error`. By default set to `console`. Setting log to `null` will turn off logging.\n\n### encodeSync(fileData : ArrayBuffer | String, writer : Object | Array, [, options : Object [, url : String]]) : any\n\nEncodes data synchronously using the provided writer, if possible. If not, returns `null`, in which case asynchronous loading is required.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n","slug":"docs/api-reference/core/encode","title":"encode"},{"excerpt":"parse / parseSync / parseInBatches This functions parse data. As important special cases, the async  function can also load (and then parse…","rawMarkdownBody":"# parse / parseSync / parseInBatches\n\nThis functions parse data. As important special cases, the async `parse` function can also load (and then parse) data from a `fetch` (or `fetchFile`) `Response` object, and the streaming `parseInBatches` version can parse incrementally from a stream as data arrives.\n\n## Usage\n\nThe return value from `fetch` or `fetchFile` is a `Promise` that resolves to the fetch `Response` object and can be passed directly to the non-sync parser functions:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\ndata = await parse(fetchFile(url), OBJLoader);\n// Application code here\n...\n```\n\nBatched (streaming) parsing is supported by some loaders\n\n```js\nimport {fetchFile, parseInBatches} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/obj';\n\nconst batchIterator = await parseInBatches(fetchFile(url), CSVLoader);\nfor await (const batch of batchIterator) {\n  console.log(batch.length);\n}\n```\n\nHandling errors\n\n```js\ntry {\n  const response = await fetch(url); // fetch can throw in case of network errors\n  const data = await parse(response); // parse will throw if server reports an error\n} catch (error) {\n  console.log(error);\n}\n```\n\n## Functions\n\n### parse(data : ArrayBuffer | String, loaders : Object | Object\\[] [, options : Object [, url : String]]) : Promise.Any\n\n### parse(data : ArrayBuffer | String, [, options : Object [, url : String]]) : Promise.Any\n\nParses data asynchronously either using the provided loader or loaders, or using the pre-registered loaders (see `register-loaders`).\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n  - `Response` - response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `File` - A browser file object (from drag-and-drop or file selection operations).\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of pre-registered loaders (see `registerLoaders`)\n\n- `options`: optional, options for the loader (see documentation of the specific loader).\n\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nOptions:\n\n- `options.log`=`console` Any object with methods `log`, `info`, `warn` and `error`. By default set to `console`. Setting log to `null` will turn off logging.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\nNotes:\n\n- If multiple `loaders` are provided (or pre-registered), an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\n\n### parseSync(fileData : ArrayBuffer | String, loaders : Object | Object\\[], [, options : Object [, url : String]]) : any\n\n### parseSync(fileData : ArrayBuffer | String, [, options : Object [, url : String]]) : any\n\n> Synchronous parsing is not supported by all _loaders_\n\nParses data synchronously using the provided loader, if possible. If not, returns `null`, in which case asynchronous parsing is required.\n\n- `data`: already loaded data, either in binary or text format. This parameter can be any of the following types:\n  - `Response`: `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer`: Parse from binary data in an array buffer\n  - `String`: Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator`: Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n    can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\n### parseInBatches(data : any, loaders : Object | Object\\[] [, options : Object [, url : String]]) : AsyncIterator\n\n### parseInBatches(data : any [, options : Object [, url : String]]) : AsyncIterator\n\n> Batched loading is not supported by all _loaders_\n\nParses data in batches from a stream, releasing each batch to the application while the stream is still being read.\n\nParses data with the selected _loader object_. An array of `loaders` can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\nThe `loaders` parameter can also be ommitted, in which case any _loaders_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n  - `Response` - `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Returns an async iterator that yields batches of data. The exact format for the batches depends on the _loader object_ category.\n","slug":"docs/api-reference/core/parse","title":"parse / parseSync / parseInBatches"},{"excerpt":"fetchFile The  function is a wrapper around  which provides support for path prefixes and some additional loading capabilities. Usage Use…","rawMarkdownBody":"# fetchFile\n\nThe `fetchFile` function is a wrapper around `fetch` which provides support for path prefixes and some additional loading capabilities.\n\n## Usage\n\nUse the `fetchFile` function as follows:\n\n```js\nimport {fetchFile} from '@loaders.gl/core';\n\nconst response = await fetchFile(url);\n\n// Now use standard browser Response APIs\n\n// Note: headers are case-insensitive\nconst contentLength = response.headers.get('content-length');\nconst mimeType = response.headers.get('content-type');\n\nconst arrayBuffer = await response.arrayBuffer();\n```\n\nThe `Response` object from `fetchFile` is usually passed to `parse` as follows:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\nconst data = await parse(fetchFile(url), OBJLoader);\n```\n\nNote that if you don't need the extra features in `fetchFile`, you can just use the browsers built-in `fetch` method.\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\nconst data = await parse(fetch(url), OBJLoader);\n```\n\n## Functions\n\n### fetchFile(url : String [, options : Object]) : Promise.Response\n\nA wrapper around the platform [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/fetch) function with some additions:\n\n- Supports `setPathPrefix`: If path prefix has been set, it will be appended if `url` is relative (e.g. does not start with a `/`).\n- Supports `File` and `Blob` objects on the browser (and returns \"mock\" fetch response objects).\n\nReturns:\n\n- A promise that resolves into a fetch [`Response`](https://developer.mozilla.org/en-US/docs/Web/API/Response) object, with the following methods/fields:\n    - `headers`: `Headers` - A [`Headers`](https://developer.mozilla.org/en-US/docs/Web/API/Headers) object.\n    - `arrayBuffer()`: Promise.ArrayBuffer` - Loads the file as an `ArrayBuffer`.\n    - `text()`: Promise.String` - Loads the file and decodes it into text.\n    - `json()`: Promise.String` - Loads the file and decodes it into JSON.\n    - `body` : ReadableStream` - A stream that can be used to incrementally read the contents of the file.\n\nOptions:\n\nUnder Node.js, options include (see [fs.createReadStream](https://nodejs.org/api/fs.html#fs_fs_createreadstream_path_options)):\n\n- `options.highWaterMark` (Number) Default: 64K (64 \\* 1024) - Determines the \"chunk size\" of data read from the file.\n\n### readFileSync(url : String [, options : Object]) : ArrayBuffer | String\n\n> This function only works on Node.js or using data URLs.\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- `fetchFile` will delegate to `fetch` after resolving the URL.\n- For some data sources such as node.js and `File`/`Blob` objects a mock `Response` object will be returned, and not all fields/members may be implemented.\n- When possible, `Content-Length` and `Content-Type` `headers` are also populated for non-request data sources including `File`, `Blob` and Node.js files.\n- `fetchFile` is intended to be a small (in terms of bundle size) function to help applications work with files in a portable way. The `Response` object returned on Node.js does not implement all the functionality the browser does. If you run into the need\n- In fact, the use of any of the file utilities including `readFile` and `readFileAsync` functions with other loaders.gl functions is entirely optional. loader objects can be used with data loaded via any mechanism the application prefers, e.g. directly using `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n- The stream utilities are intended to be small optional helpers that facilitate writing platform independent code that works with streams. This can be valuable as JavaScript Stream APIs are still maturing and there are still significant differences between platforms. However, streams and iterators created directly using platform specific APIs can be used as parameters to loaders.gl functions whenever a stream is expected, allowing the application to take full control when desired.\n","slug":"docs/api-reference/core/fetch-file","title":"fetchFile"},{"excerpt":"fetchProgress This function is still experimental A function that tracks a fetch response object and calls  callbacks. Usage _fetchProgress…","rawMarkdownBody":"# fetchProgress\n\n> This function is still experimental\n\nA function that tracks a fetch response object and calls `onProgress` callbacks.\n\n## Usage\n\n```js\nimport {_fetchProgress} from '@loaders.gl/core';\n\nfunction onProgress(percent, {loadedBytes, totalBytes}) {\n  console.log(`${percent}% ${Math.round(loadedBytes/1000)} of ${Math.round(totalBytes/1000)} Kbytes`);\n}\n\nasync function main() {\n  const response = await _fetchProgress(fetch(PROGRESS_IMAGE_URL, onProgress),\n  const data = await response.arrayBuffer();\n  // At this point, onProgress will have been called one or more times.\n  ...\n}\n```\n\n## _fetchProgress(response : Response | Promise, onProgress : function, onDone : function, onError : function) : Response\n\n`onProgress: (percent: number, {loadedBytes : number, totalBytes : number}) => void`\n\n","slug":"docs/api-reference/core/fetch-progress","title":"fetchProgress"},{"excerpt":"load The  function can be used with any loader object. They takes a  and one or more loader objects, checks what type of data that loader…","rawMarkdownBody":"# load\n\nThe `load` function can be used with any _loader object_. They takes a `url` and one or more _loader objects_, checks what type of data that loader prefers to work on (e.g. text, JSON, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n\n### load(url : String | File, loaders : Object | Object[][, options : object]) : Promise.Response\n\n### load(url : String | File [, options : Object]) : Promise.Response\n\nThe `load` function is used to load and parse data with a specific _loader object_. An array of loader objects can be provided, in which case `load` will attempt to autodetect which loader is appropriate for the file.\n\nThe `loaders` parameter can also be omitted, in which case any _loader objects_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `url` - Urls can be data urls (`data://`) or a request (`http://` or `https://`) urls, or a file name (Node.js only). Also accepts `File` or `Blob` object (Browser only). Can also accept any format that is accepted by [`parse`](https://github.com/uber-web/loaders.gl/blob/master/docs/api-reference/core/parse.md), with the exception of strings that are interpreted as urls.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options` - optional, contains both options for the read process and options for the loader (see documentation of the specific loader).\n\n- `options.dataType`=`arraybuffer` - Default depends on loader object. Set to 'text' to read as text.\n\n`url` values\n\nReturns:\n\n- Return value depends on the _loader category_.\n\nNotes:\n\n- If `url` is not a `string`, `load` will call `parse` directly.\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n- `load` takes a `url` and a loader object, checks what type of data that loader prefers to work on (e.g. text, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n- If `@loaders.gl/polyfills` is installed, `load` will work under Node.js as well.\n\n","slug":"docs/api-reference/core/load","title":"load"},{"excerpt":"Iterator Utils Functions getStreamIterator(stream : Stream) : AsyncIterator Returns an async iterator that can be used to read chunks of…","rawMarkdownBody":"# Iterator Utils\n\n## Functions\n\n### getStreamIterator(stream : Stream) : AsyncIterator\n\nReturns an async iterator that can be used to read chunks of data from the stream (or write chunks of data to the stream, in case of writable streams).\n\nWorks on both Node.js 8+ and browser streams.\n","slug":"docs/api-reference/core/iterator-utilities","title":"Iterator Utils"},{"excerpt":"save Needs update  and  function can be used with any writer.  takes a  and a writer object, checks what type of data that writer prefers to…","rawMarkdownBody":"# save\n\n> Needs update\n\n`save` and `saveSync` function can be used with any writer. `save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n## Functions\n\n### save(url : String | File, writer : Object [, options : Object]) : Promise.ArrayBuffer| Promi\nse.String\n\nThe `save` function can be used with any writer.\n\n`save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n- `url` - Can be a string, either a data url or a request url, or in Node.js, a file name, or in the browser, a File object.\n- `data` - saveed data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, contains both options for the read process and options for the writer (see documentation of the specific writer).\n- `options.dataType`=`arraybuffer` - By default reads as binary. Set to 'text' to read as text.\n\nReturns:\n\n- Return value depends on the category\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### saveSync(url : String [, options : Object]) : ArrayBuffer | String\n\nSimilar to `save` except saves and parses data synchronously.\n\nNote that for `saveSync` to work, the `url` needs to be saveable synchronously _and_ the writer used must support synchronous parsing. Synchronous saveing only works on data URLs or files in Node.js. In many cases, the asynchronous `save` is more appropriate.\n","slug":"docs/api-reference/core/save","title":"save"},{"excerpt":"selectLoader  is considered experimental as loader auto detection is still being improved. A core feature of loaders.gl is the ability to…","rawMarkdownBody":"# selectLoader\n\n> `selectLoader` is considered experimental as loader auto detection is still being improved.\n\nA core feature of loaders.gl is the ability to automatically select an appropriate loader for a specific file among a list of candidate loaders. This feature is built-in to the `parse` and `load` functions, but applications can also access this feature directly through the `selectLoader` API.\n\nLoader selection heuristics are based on both filename (url) extensions as well as comparison of initial data content against known headers for each file format.\n\n`selectLoader` is also aware of the [loader registry](docs/api-reference/core/register-loaders.md). If no loaders are provided (by passing in a falsy value such as `null`) `selectLoader` will search the list of pre-registered loaders.\n\n## Usage\n\nSelect a loader from a list of provided loaders:\n\n```js\nimport {_selectLoader} from '@loaders.gl/core';\nimport {ArrowLoader} from '@loaders.gl/arrow';\nimport {CSVLoader} from '@loaders.gl/csv';\n\n_selectLoader([ArrowLoader, CSVLoader], 'filename.csv'); // => CSVLoader\n```\n\nSelect a loader from pre-registered loaders in the loader registry:\n\n```js\nimport {registerLoaders, _selectLoader} from '@loaders.gl/core';\nimport {ArrowLoader} from '@loaders.gl/arrow';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders(ArrowLoader, CSVLoader);\n\n// By passing null instead of a loader list, selectLoader returns null.\n_selectLoader(null, 'filename.csv'); // => CSVLoader\n```\n\n## Functions\n\n### \\_selectLoader(loaders : Object | Object[] | null, url? : String, data? : ArrayBuffer | String, options? : Object)\n\nSelects an appropriate loader for a file from a list of candidate loaders by examining a URL and/or an initial data chunk.\n\nParameters:\n- `loaders` - can be a single loader or an array of loaders, or null.\n- `url` - An optional URL to perform autodetection against.\n- `data` - Optional data to perform autodetection against\n- `options.nothrow`=`false` - Return null instead of throwing exception if no loader can be found\n\nReturns:\n- A single loader (or null if `options.nothrow` was set and no matching loader was found).\n\nThrows:\n- If no matching loader was found, and `options.nothrow` was not set.\n\nRegarding the `loaders` parameter:\n- A single loader object will be returned without matching.\n- a `null` loader list will use the pre-registered list of loaders.\n- A supplied list of loaders will be searched for a matching loader.\n\n## Remarks\n\n* File extensions - An attempt will be made to extract a file extension by stripping away query parameters and base path before matching against known loader extensions.\n* Stream autodetection - Currently not well supported.\n","slug":"docs/api-reference/core/select-loader","title":"selectLoader"},{"excerpt":"registerLoaders The loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing…","rawMarkdownBody":"# registerLoaders\n\nThe loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing just the loaders they need and registering them during initialization.\n\nApplications can then make all those imported loaders available (via format autodetection) to all subsequent `parse` and `load` calls, without those calls having to specify which loaders to use.\n\n## Usage\n\nSample application initialization code that imports and registers loaders:\n\n```js\nimport {registerLoaders} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders(CSVLoader);\n```\n\nSome other file that needs to load CSV:\n\n```js\nimport {load} from '@loaders.gl/core';\n\n// The pre-registered CSVLoader gets auto selected based on file extension...\nconst data = await load('data.csv');\n```\n\n## Functions\n\n### registerLoaders(loaders : Object | Object[])\n\nRegisters one or more _loader objects_ to a global _loader object registry_, these loaders will be used if no loader object is supplied to `parse` and `load`.\n\n- `loaders` - can be a single loader or an array of loaders. The specified loaders will be added to any previously registered loaders.\n","slug":"docs/api-reference/core/register-loaders","title":"registerLoaders"},{"excerpt":"setPathPrefix resolvePath(path : String) : String Applies aliases and path prefix, in that order. Returns an updated path. addAliases…","rawMarkdownBody":"# setPathPrefix\n\n### resolvePath(path : String) : String\n\nApplies aliases and path prefix, in that order. Returns an updated path.\n\n### addAliases(aliases : Object)\n\nSets a map of aliases (file name substitutions).\n\n### setPathPrefix(prefix : String)\n\nThis sets a path prefix that is automatically prepended to relative path names provided to load functions.\n\n### getPathPrefix() : String\n\nReturns the current path prefix set by `setPathPrefix`.\n","slug":"docs/api-reference/core/set-path-prefix","title":"setPathPrefix"},{"excerpt":"writeFile A file save utilities that (attempts to) work consistently across browser and node. Usage Functions writeFile(url : String…","rawMarkdownBody":"# writeFile\n\nA file save utilities that (attempts to) work consistently across browser and node.\n\n## Usage\n\n```js\nimport {writeFile} from '@loaders.gl/core';\nimport {DracoWriter} from '@loaders.gl/draco';\n\nawait writeFile(url, DracoWriter);\n```\n\n## Functions\n\n### writeFile(url : String [, options : Object]) : Promise.ArrayBuffer\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### writeFileSync(url : String [, options : Object]) : ArrayBuffer\n\n> Only works on Node.js or using data URLs.\n\nReads the raw data from a \"file\" synchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- The use of the loaders.gl `writeFile` and `writeFileAsync` functions is optional, loaders.gl loaders can be used with any data loaded via any mechanism the application prefers, e.g. `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n","slug":"docs/api-reference/core/write-file","title":"writeFile"},{"excerpt":"ArrowLoader The Arrow loaders are still under development. The  parses the Apache Arrow columnar table format. Loader Characteristic File…","rawMarkdownBody":"# ArrowLoader\n\n> The Arrow loaders are still under development.\n\nThe `ArrowLoader` parses the Apache Arrow columnar table format.\n\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extension        | `.arrow`        |\n| File Type             | Binary          |\n| File Format           | [IPC: Encapsulated Message Format](http://arrow.apache.org/docs/ipc.html) |\n| Data Format           | [Columnar Table](/docs/specifications/category-table) |\n| Decoder Type          | Synchronous     |\n| Worker Thread Support | Yes             |\n| Streaming Support     | Yes             |\n\n## Usage\n\n```js\nimport {ArrowLoader, ArrowWorkerLoader} from '@loaders.gl/arrow';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, ArrowLoader, options);\n// Decode on worker thread\nconst data = await load(url, ArrowWorkerLoader, options);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n","slug":"docs/api-reference/arrow/arrow-loader","title":"ArrowLoader"},{"excerpt":"@loaders.gl/arrow This module handles Apache Arrow, an emerging standard for large in-memory columnar data.  Installation Attributions…","rawMarkdownBody":"# @loaders.gl/arrow\n\nThis module handles [Apache Arrow](https://arrow.apache.org/), an emerging standard for large in-memory columnar data. \n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/arrow\n```\n\n## Attributions\n\nArrowLoader development benefitted from extensive technical advice from Paul Taylor @ Graphistry.\n","slug":"docs/api-reference/arrow","title":"@loaders.gl/arrow"},{"excerpt":"Tileset3D The 3D tiles loaders are still under development. This class can be instantiated with tileset data formatted according to the 3D…","rawMarkdownBody":"# Tileset3D\n\n> The 3D tiles loaders are still under development.\n\nThis class can be instantiated with tileset data formatted according to the [3D Tiles Category](docs/specifications/3d-tiles), which is supported by the [Tileset3DLoader](docs/api-reference/3d-tiles/tileset-3d-loader).\n\nReferences\n- [3D Tiles](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification).\n\n## Usage\n\nLoading a tileset and instantiating a `Tileset3D` instance.\n\n```js\nimport {Tileset3DLoader, Tileset3D} from '@loaders.gl/3d-tiles';\nimport {parse} from '@loaders.gl/core';\n\nconst tilesetJSON = await parse(fetch(tileset))\nconst tileset = new Tileset3D(tilesetJson);\n\nconsole.log(`Maximum building height: ${tileset.properties.height.maximum}`);\nconsole.log(`Minimum building height: ${tileset.properties.height.minimum}`);\n```\n\nCommon setting for the `skipLevelOfDetail` optimization\n\n```js\nimport {Tileset3D} from '@loaders.gl/3d-tiles';\n\nconst tileset = new Tileset3D(tilesetJson, {\n  url: 'http://localhost:8002/tilesets/Seattle/tileset.json',\n  baseScreenSpaceError: 1024,\n  skipScreenSpaceErrorFactor: 16,\n});\n```\n\nCommon settings for the `dynamicScreenSpaceError` optimization\n\n```js\nimport {Tileset3D} from '^loaders.gl/3d-tiles';\nconst tileset = new Tileset3D({\n  url: 'http://localhost:8002/tilesets/Seattle/tileset.json',\n  dynamicScreenSpaceError: true,\n  dynamicScreenSpaceErrorDensity: 0.00278,\n  dynamicScreenSpaceErrorFactor: 4.0\n});\n```\n\n### Properties\n\n### asset : Object (readonly)\n\nGets the tileset's asset object property, which contains metadata about the tileset.\n\nSee the [asset schema reference](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#reference-asset) in the 3D Tiles spec for the full set of properties.\n\n\n### properties : Object (readonly)\n\nGets the tileset's properties dictionary object, which contains metadata about per-feature properties.\n\nSee the [properties schema reference](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#reference-properties) in the 3D Tiles spec for the full set of properties.\n\n\n### tilesLoaded : boolean (readonly)\n\nWhen `true`, all tiles that meet the screen space error this frame are loaded. The tileset is\ncompletely loaded for this view.\n\nSee Tileset3D#allTilesLoaded\n\n\n### url : String (readonly)\n\nThe url to a tileset JSON file.\n\n### basePath : String (readonly) (deprecated)\n\nThe base path that non-absolute paths in tileset JSON file are relative to.\n\n### maximumScreenSpaceError : Number\n\nThe maximum screen space error used to drive level of detail refinement. This value helps determine when a tile refines to its descendants, and therefore plays a major role in balancing performance with visual quality.\n\n\nA tile's screen space error is roughly equivalent to the number of pixels wide that would be drawn if a sphere with a\nradius equal to the tile's <b>geometric error</b> were rendered at the tile's position. If this value exceeds\n`maximumScreenSpaceError` the tile refines to its descendants.\n\nDepending on the tileset, `maximumScreenSpaceError` may need to be tweaked to achieve the right balance. Higher values provide better performance but lower visual quality.\n *\n\n### maximumMemoryUsage : Number\n\n^default 16\n *\n^exception `maximumScreenSpaceError` must be greater than or equal to zero.\n\nThe maximum amount of GPU memory (in MB) that may be used to cache tiles. This value is estimated from\ngeometry, textures, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata.\n\n\nTiles not in view are unloaded to enforce this.\n\nIf decreasing this value results in unloading tiles, the tiles are unloaded the next frame.\n\nIf tiles sized more than `maximumMemoryUsage` are needed\nto meet the desired screen space error, determined by `Tileset3D.maximumScreenSpaceError `,\nfor the current view, then the memory usage of the tiles loaded will exceed\n`maximumMemoryUsage`.  For example, if the maximum is 256 MB, but\n300 MB of tiles are needed to meet the screen space error, then 300 MB of tiles may be loaded.  When\nthese tiles go out of view, they will be unloaded.\n\n^default 512\n *\n^exception `maximumMemoryUsage` must be greater than or equal to zero.\n^see Tileset3D#totalMemoryUsageInBytes\n\n### root : Tile3DHeader\n\nThe root tile header.\n\n\n### boundingSphere : BoundingSphere\n\nThe tileset's bounding sphere.\n\n\n```js\nvar tileset = viewer.scene.primitives.add(new Tileset3D({\nurl : 'http://localhost:8002/tilesets/Seattle/tileset.json'\n}));\n\ntileset.readyPromise.then(function(tileset) {\n// Set the camera to view the newly added tileset\nviewer.camera.viewBoundingSphere(tileset.boundingSphere, new HeadingPitchRange(0, -0.5, 0));\n});\n```\n\n### modelMatrix : Matrix4\n\nA 4x4 transformation matrix that transforms the entire tileset.\n\n```js\n// Adjust a tileset's height from the globe's surface.\nvar heightOffset = 20.0;\nvar boundingSphere = tileset.boundingSphere;\nvar cartographic = Cartographic.fromCartesian(boundingSphere.center);\nvar surface = Cartesian3.fromRadians(cartographic.longitude, cartographic.latitude, 0.0);\nvar offset = Cartesian3.fromRadians(cartographic.longitude, cartographic.latitude, heightOffset);\nvar translation = Cartesian3.subtract(offset, surface, new Cartesian3());\ntileset.modelMatrix = Matrix4.fromTranslation(translation);\n```\n\n### maximumMemoryUsage : Number\n\n### totalMemoryUsageInBytes : Number\n\nThe total amount of GPU memory in bytes used by the tileset. This value is estimated from\ngeometry, texture, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata.\n\n### stats : Stats\n\nAn instance of a probe.gl `Stats` object that contains information on how many tiles have been loaded etc. Easy to display using a probe.gl `StatsWidget`.\n\n\n### ellipsoid : Ellipsoid\n\nGets an ellipsoid describing the shape of the globe.\n\nReturns the `extras` property at the top-level of the tileset JSON, which contains application specific metadata.\nReturns `undefined` if `extras` does not exist.\n\nException The tileset is not loaded. Use Tileset3D.readyPromise or wait for Tileset3D.ready to be true.\n\nSee [Extras](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#specifying-extensions-and-application-specific-extras) in the 3D Tiles specification.}\n\n\n### unloadTileset\n\nUnloads all tiles that weren't selected the previous frame. This can be used to\nexplicitly manage the tile cache and reduce the total number of tiles loaded below\n`Tileset3D.maximumMemoryUsage`.\n\nTile unloads occur at the next frame to keep all the WebGL delete calls\nwithin the render loop.\n\n### isDestroyed() : Boolean\n\nReturns true if this object was destroyed; otherwise, false.\n\nIf this object was destroyed, it should not be used; calling any function other than\n`isDestroyed` will result in an exception.\n\n^returns `Boolean`: `true` if this object was destroyed; otherwise, `false`.\n\n### destroy()\n\nDestroys the WebGL resources held by this object. Destroying an object allows for deterministic\nrelease of WebGL resources, instead of relying on the garbage collector to destroy this object.\n\nOnce an object is destroyed, it should not be used; calling any function other than `isDestroyed` will result in an exception. Therefore, assign the return value `undefined` to the object as done in the example.\n\nWxception This object was destroyed, i.e., destroy() was called.\n\n\n## Methods\n\n### constructor(tileset : Object, url : String [, options : Object])\n\n- `tileset`: The loaded tileset (parsed JSON)\n- `url`: The url to a tileset JSON file.\n- `options`: Options object, see the options section below for available options.\n\nNotes:\n- The `version` tileset must be 3D Tiles version 0.0 or 1.0.\n\n\n### hasExtension(extensionName : String) : Boolean\n\n`true` if the tileset JSON file lists the extension in extensionsUsed; otherwise, `false`.\n^param {String} extensionName The name of the extension to check. \\*\n^returns {Boolean} `true` if the tileset JSON file lists the extension in extensionsUsed; otherwise, `false`.\n\n\n## Options\n\n> Tileset3D class is still being developed, not all options are guaranteed to be working.\n\nThe `Tileset3D` class supports a number of options\n\n- `options.url` (`Resource|String|Promise.Resource|Promise.String`) The url to a tileset JSON file.\n- `options.show`=`true` (`Boolean`) - Determines if the tileset will be shown.\n- `options.modelMatrix`=`Matrix4.IDENTITY` (`Matrix4`) - A 4x4 transformation matrix that transforms the tileset's root tile.\n- `options.maximumScreenSpaceError`=`16`] (`Number`) - The maximum screen space error used to drive level of detail refinement.\n- `options.maximumMemoryUsage`=`512`] (`Number`) - The maximum amount of memory in MB that can be used by the tileset.\n- `options.dynamicScreenSpaceError`=`false`] (`Boolean`) - Optimization option. Reduce the screen space error for tiles that are further away from the camera.\n- `options.dynamicScreenSpaceErrorDensity`=`0.00278`] (`Number`) - Density used to adjust the dynamic screen space error, similar to fog density.\n- `options.dynamicScreenSpaceErrorFactor`=`4.0`] (`Number`) - A factor used to increase the computed dynamic screen space error.\n- `options.baseScreenSpaceError`=`1024` (`Number`) - When `skipLevelOfDetail` is `true`, the screen space error that must be reached before skipping levels of detail.\n- `options.skipScreenSpaceErrorFactor`=`16` (`Number`) - When `skipLevelOfDetail` is `true`, a multiplier defining the minimum screen space error to skip. Used in conjunction with `skipLevels` to determine which tiles to load.\n- `options.ellipsoid`=`Ellipsoid.WGS84` (`Ellipsoid`) - The ellipsoid determining the size and shape of the globe.\n\nCallbacks\n- `options.onTileLoad` (`(tileHeader : Tile3DHeader) : void`) -\n- `options.onTileUnload` (`(tileHeader : Tile3DHeader) :void`) -\n- `options.onTileLoadFailed` (`void(tileHeader : Tile3DHeader, message : String) : void`) -\n\n\n### dynamicScreenSpaceError\n\n=`false`\n\nOptimization option. Whether the tileset should refine based on a dynamic screen space error. Tiles that are further away will be rendered with lower detail than closer tiles. This improves performance by rendering fewer tiles and making less requests, but may result in a slight drop in visual quality for tiles in the distance.\n\nThe algorithm is biased towards \"street views\" where the camera is close to the ground plane of the tileset and looking at the horizon. In addition results are more accurate for tightly fitting bounding volumes like box and region.\n\n### dynamicScreenSpaceErrorDensity\n\n=`0.00278`\n\nA scalar that determines the density used to adjust the dynamic screen space error (similar to \"fog\"). Increasing this value has the effect of increasing the maximum screen space error for all tiles, but in a non-linear fashion.\n\nThe error starts at 0.0 and increases exponentially until a midpoint is reached, and then approaches 1.0 asymptotically. This has the effect of keeping high detail in the closer tiles and lower detail in the further tiles, with all tiles beyond a certain distance all roughly having an error of 1.0.\n\n\nThe dynamic error is in the range [0.0, 1.0) and is multiplied by `dynamicScreenSpaceErrorFactor` to produce the\nfinal dynamic error. This dynamic error is then subtracted from the tile's actual screen space error.\n\nIncreasing `dynamicScreenSpaceErrorDensity` has the effect of moving the error midpoint closer to the camera.\nIt is analogous to moving fog closer to the camera.\n\n### dynamicScreenSpaceErrorFactor\n\n= 4.0;\n\nA factor used to increase the screen space error of tiles for dynamic screen space error. As this value increases less tiles\nare requested for rendering and tiles in the distance will have lower detail. If set to zero, the feature will be disabled.\n\n### dynamicScreenSpaceErrorHeightFalloff\n\n= 0.25;\n\nA ratio of the tileset's height at which the density starts to falloff. If the camera is below this height the\nfull computed density is applied, otherwise the density falls off. This has the effect of higher density at\nstreet level views.\n\n\nValid values are between 0.0 and 1.0.\n\n### onTileLoad(tileHeader : Tile3DHeader) : void\n\nIndicate ssthat a tile's content was loaded.\n\nThe loaded `Tile3DHeader` is passed to the event listener.\n\nThis event is fired during the tileset traversal while the frame is being rendered\nso that updates to the tile take effect in the same frame.  Do not create or modify\nentities or primitives during the event listener.\n\n```js\n  new Tileset3D({\n    onTileLoad(tileHeader => console.log('A tile was loaded.'));\n  });\n```\n\n### onTileUnload(tileHeader : Tile3DHeader) : void\n\nIndicates that a tile's content was unloaded.\n\nThe unloaded `Tile3DHeaders` is passed to the event listener.\n\nThis event is fired immediately before the tile's content is unloaded while the frame is being\nrendered so that the event listener has access to the tile's content.  Do not create\nor modify entities or primitives during the event listener.\n\n```js\n  new Tileset3D({\n    onTileUnload(tile =>  console.log('A tile was unloaded from the cache.'));\n  });\n```\n\nSee\n- Tileset3D#maximumMemoryUsage\n- Tileset3D#trimLoadedTiles\n\n\n### onTileLoadFail(tileHeader : Tile3DHeader) : void\n\nCalled to indicate that a tile's content failed to load. By default, error messages will be logged to the console.\n\nThe error object passed to the listener contains two properties:\n- `url`: the url of the failed tile.\n- `message`: the error message.\n\n```js\n  new Tileset3D({\n    onTileFailed(tileHeader, url, message) {\n      console.log('An error occurred loading tile: ', url);\n      console.log('Error: ', message);\n    }\n  });\n```\n\n\n### skipLevelOfDetail : Boolean\n\nDefault: true\n\nOptimization option. Determines if level of detail skipping should be applied during the traversal.\n\nThe common strategy for replacement-refinement traversal is to store all levels of the tree in memory and require\nall children to be loaded before the parent can refine. With this optimization levels of the tree can be skipped\nentirely and children can be rendered alongside their parents. The tileset requires significantly less memory when\nusing this optimization.\n\n\n### baseScreenSpaceError : Number\n\nDefault: 1024\n\nThe screen space error that must be reached before skipping levels of detail.\n\nOnly used when `skipLevelOfDetail` is `true`.\n\n### skipScreenSpaceErrorFactor : Number\n\nDefault: 16\n\nMultiplier defining the minimum screen space error to skip.\nFor example, if a tile has screen space error of 100, no tiles will be loaded unless they\nare leaves or have a screen space error `<= 100 / skipScreenSpaceErrorFactor`.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### skipLevels\n\nDefault: 1\n\nConstant defining the minimum number of levels to skip when loading tiles. When it is 0, no levels are skipped.\nFor example, if a tile is level 1, no tiles will be loaded unless they are at level greater than 2.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### immediatelyLoadDesiredLevelOfDetail : false\n\nWhen true, only tiles that meet the maximum screen space error will ever be downloaded.\nSkipping factors are ignored and just the desired tiles are loaded.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### loadSiblings: false\n\nDetermines whether siblings of visible tiles are always downloaded during traversal.\nThis may be useful for ensuring that tiles are already available when the viewer turns left/right.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n","slug":"docs/api-reference/3d-tiles/tileset-3d","title":"Tileset3D"},{"excerpt":"Tileset3DLoader The 3D tile loaders are still under development. Parses a main tileset JSON file as the entry point to define a 3D tileset…","rawMarkdownBody":"# Tileset3DLoader\n\n> The 3D tile loaders are still under development.\n\nParses a main tileset JSON file as the entry point to define a 3D tileset.\n\n| Loader                | Characteristic   |\n| --------------------- | ---------------- |\n| File Extensions       | `.json`          |\n| File Type             | JSON             |\n| File Format           | [3D Tileset JSON](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#tileset-json) |\n| Data Format           | JSON             |\n| Decoder Type          | Synchronous      |\n| Worker Thread Support | No               |\n| Streaming Support     | No               |\n\n## Usage\n\n```js\nimport {Tileset3DLoader, Tileset3D} from '^loaders.gl/3d-tiles';\nconst tilesetJson = await load('http://localhost:8002/tilesets/Seattle/tileset.json', Tileset3DLoader);\nconst tileset = new Tileset3D(tilesetJson);\n```\n\n## Options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n","slug":"docs/api-reference/3d-tiles/tileset-3d-loader","title":"Tileset3DLoader"},{"excerpt":"@loaders.gl/3d-tiles The 3D tile loaders are still under development. Support for loading and traversing 3D Tile Sets. Installation Roadmap…","rawMarkdownBody":"# @loaders.gl/3d-tiles\n\n> The 3D tile loaders are still under development.\n\nSupport for loading and traversing [3D Tile Sets](https://github.com/AnalyticalGraphicsInc/3d-tiles).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/3d-tiles\n```\n\n## Roadmap\n\nThe plan is to provide the following loaders/writers:\n- `Tile3DLoader` for individual tiles\n- `Tileset3DLoader` for the tileset\n- `Tile3DWriter` for individual tiles\n\nAnd the following helper classes\n- `Tileset3D` to help access the loaded tileset.\n- `Tile3D` to help access a loaded tile.\n- `Tileset3DTraversal` class that accepts view frustum parameters and returns a culled, prioritized list of tiles to show.\n- `Tileset3DCache` class that loads and LRU discards tiles based on prioritized lists (from Tileset3DTraversal).\n\n## Attribution\n\n3D tiles support in loaders.gl is developed in collaboration with the Cesium team.\n\nThe `Tile3DLoader` is a fork of 3d tile related code in Cesium (https://github.com/AnalyticalGraphicsInc/cesium) under Apache 2 License, in collabration with Sean Lilley, Josh Lawrence and Patrick Cozzi at Cesium.\n","slug":"docs/api-reference/3d-tiles","title":"@loaders.gl/3d-tiles"},{"excerpt":"Tile3DLoader The 3D tile loaders are still under development. Parses a 3D tile. glTF file into a hirearchical scenegraph description that…","rawMarkdownBody":"# Tile3DLoader\n\n> The 3D tile loaders are still under development.\n\nParses a [3D tile](https://github.com/AnalyticalGraphicsInc/3d-tiles). glTF file into a hirearchical scenegraph description that can be used to instantiate an actual Scenegraph in most WebGL libraries. Can load both binary `.glb` files and JSON `.gltf` files.\n\n| Loader                | Characteristic  |\n| --------------------- | --------------- |\n| File Extensions       | `.b3dm`,`.i3dm`, `.pnts`, `.cmpt` |\n| File Type             | Binary (with linked assets) |\n| File Format           | [glTF](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#tile-format-specifications)    |\n| Data Format           | [Scenegraph](/docs/specifications/category-scenegraph) |\n| Decoder Type          | Synchronous (limited), Asynchronous |\n| Worker Thread Support | No              |\n| Streaming Support     | No \\*           |\n\n\\* Streaming is not supported for invididual tiles, however tilesets are streamed by loading only the tiles needed for the current view.\n\n## Usage\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {Tile3DLoader} from '@loaders.gl/3d-tiles';\nconst gltf = await load(url, Tile3DLoader);\n```\n\nTo decompress tiles containing Draco compressed glTF models or Draco compressed point clouds:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {Tile3DLoader} from '@loaders.gl/3d-tiles';\nimport {DracoLoader} from '@loaders.gl/draco';\nconst gltf = await load(url, Tile3DLoader, {DracoLoader, decompress: true});\n```\n\n## Options\n\nPoint cloud options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `DracoLoader`        | [DracoLoader](/docs/api-reference/draco/draco-loader)  | `null`      |\n\n\nglTF tile options\n\n| Option        | Type      | Default     | Description       |\n| ------------- | --------- | ----------- | ----------------- |\n| `DracoLoader`        | [DracoLoader](/docs/api-reference/draco/draco-loader)  | `null`      | Supply to enable decoding of Draco compressed meshes. |\n| `fetchLinkedResources` | Boolean | `true`      | Fetch any linked .BIN files, decode base64 encoded URIS. Only supported in asynchronous parsing. |\n| `fetch`              | Function  | `fetch` | Function used to fetch linked resources. |\n| `decompress`         | Boolean | `true`      | Decompress Draco compressed meshes (if DracoLoader available). |\n| `postProcess`        | Boolean | `false`     | Perform additional post processing to simplify use in WebGL libraries. |\n| `createImages`       | Boolean  | `false`     | Create image objects from loaded image data. |\n","slug":"docs/api-reference/3d-tiles/tile-3d-loader","title":"Tile3DLoader"}]}}}