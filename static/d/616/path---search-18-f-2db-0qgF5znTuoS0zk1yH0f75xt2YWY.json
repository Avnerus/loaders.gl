{"pageContext":{"isCreatedByStatefulCreatePages":false,"data":[{"excerpt":"Introductionloaders.gl is suite of portable, framework-independent loaders (parsers) and writers (encoders). The suite including loaders for…","rawMarkdownBody":"# Introduction\n\nloaders.gl is suite of portable, framework-independent loaders (parsers) and writers (encoders). The suite including loaders for 3D point clouds, geometries, 3D assets, geospatial formats as well as tabular data.\n\nIn spite of the name, loaders.gl does not have any WebGL dependencies, however the format of data returned by various loaders is optimized for use with WebGL (e.g. by using typed arrays when possible, enabling data to be directly uploaded to the GPU without additional processing).\n\n## Why loaders.gl?\n\nThe open source community has already created many excellent loaders for 3D and geospatial formats available under permissive licenses on github, npm and similar sources, and loaders.gl is an effort to collect some of the best of those and package them in a unified, portable, framework-independent way.\n\nMany of these loaders were created for specific use cases (e.g. as part of a frameworks like THREE.js) and contain code that depend on use-case specific classes (e.g. a loader might be 90% framework independent work but end by creating a `THREE.Mesh`, meaning that the returned objects are not easoly usable in Javascript applications that don't use that framework).\n\nIn addition, the functionality offered by open source loaders can vary quite a bit:\n- The API, where e.g. some loaders accept URLs and implements the loading themselves, and then return promises or take callbacks, others are synchronous and just parse already loaded data, leaving more flexibility (and work) to the application.\n- Installation procedures can vary.\n- The format of the parsed data is usually different, even when parsing very similar file formats. This makes it harder to write applications that can accept data from multiple similar formats (e.g. an app that can load point clouds in PCD, PLY, LAS, Draco formats would need to deal with the idiosynchracies of multiple loaders).\n- Does the loader can run under both browser and Node.js and worker threads, or does it e.g. need to be \"commented out\" in Node-based unit tests?\n- Does the loader support streaming?\n- What options does the loader accept?\n- How is error handling implemented.\n- etc.\n\n## Major Components\n\n- **Loaders** - The primary offering is a growing set of loaders (parsers) for various major geometry formats.\n\n- **Writers** - A secondary offering is a smaller set of writers (encoders) for selected key formats to support saving data.\n\n- **Core Functions** - Since the loaders and writers themselves only implement parsing and encoding (typically from strings or array buffers), i.e. they don't actually \"load\" or \"save\" any data, loaders.gl also provides a set of utility functions that accept loader objects perform actual loading from files, urls etc.\n\n- **Polyfills** - Since the loaders are written in modern JavaScript, it depends on some features like `TextDecoder`, `fetch` etc that are not availble in all browsers/Node.js versions. A polyfill module is provided to help when support for older environments.\n\n\n## Main Design Goals\n\nSome of the key design goals for loaders.gl.\n\n**Framework Agnostic** - loaders.gl is not tied to any specific framework or use case. The provided loaders parse data from supported file formats into clearly documented, pure JavaScript data structures, but stops short of creating any rendering specific objects. This means that loaders.gl can be used with any JavaScript framework or application.\n\n**Unified Data Structure for Similar Formats** - All loaders in the same category return a \"standardized\" JavaScript object. For instance point cloud loaders use a header key-value map and a map of glTF2-style accessor objecs with typed arrays representing binary data attributes. This can significantly simplify applications that want to support multiple similar data formats.\n\n**Binary Data** - Contiguous numeric arrays (e.g. mesh attributes) will be loaded using JavaScript typed arrays rather than native Arrays. Such binary arrays can be uploaded directly to GPU buffers and used for rendering or GPGPU calculations.\n\n**Loader Objects** - Loaders are exported as objects that include metadata such as the name of the loader, the default extension, an optional test function and of course the parser function for the format. This allows applications to work with multiple loaders in a unified way, even allowing loaders.gl to automatically pick the right loader for a file.\n\n**Optimized for Tree Shaking** - Each loader's metadata object is an independent named export, meaning that any loaders not explicitly imported by the application will be removed from the application bundle during tree-shaking.\n\n**Works in Browsers, Worker Threads and Node.js** - TextEncoder polyfills? ArrayBuffers vs Buffers? Whether optimizing interactivity, working with isomorpic applications, writing test suites etc, you'll know that running loaders on worker threads and Node is supported.\n\n## Licenses\n\nloaders.gl currently contains a collection of MIT licensed loaders. Each loader comes with its own license, so if the distinction matters to you, please check and decide accordingly. However, loaders.gl will not include any loaders with non-permissive, commercial or copy-left licenses.\n\n## Credits and Attributions\n\nloaders.gl is to a large extent just a simple curation and repackaging of the superb work done by many others in the open source community. We want to, and try to be as explicit as we can about the origins and attributions of each loader. Even though we try, sometimes we may not have based the code in loaders.gl on the original source, and we may not have a clear picture of the full history of the code we are reusing. If you feel that we have missed something, or that we could do better in this regard, please let us know.\n\nAlso check each loader directory for additional details, we strive to keep intact any comments inside the source code relating to authorship and contributions.\n\n## Support\n\nloaders.gl is part of the vis.gl framework ecosystem, and was mainly created to support various frameworks and apps within these frameworks such as luma.gl and deck.gl, but is intentionally designed in a framework-agnostic way.\n\nOur intention is for loaders.gl to work well on recent versions of the major evergreen browsers (Chrome, Firefox, Safari) and Node.js (v10+).\n\nAssuming appropriate polyfills are installed, we also want loaders.gl to run on Edge, IE11 and Node.js v8.\n","slug":"docs","title":"Introduction"},{"excerpt":"ContributingContributions are welcome, assuming that they align with the general design goals and philosophy of the repo.Unless you just…","rawMarkdownBody":"# Contributing\n\nContributions are welcome, assuming that they align with the general design goals and philosophy of the repo.\n\nUnless you just want to contribute a small bug fix, it is a good idea to start by opening an issue and discuss your idea with the maintainers. This maximizes the chances that your contribution will be accepted once you open a pull request.\n\n## Configuring Your Development Environment\n\nTo contribute, you will likely want to clone the loaders.gl repository and make sure you can install, build and run tests.\n\nOur primary development environment is MacOS, but it is possible to build loaders.gl on Linux and Windows (using a Linux environment).\n\n### Setting up Linux Environment on Windows 10\n\nInstall [WSL (Windows Subsystem for Linux)](https://docs.microsoft.com/en-us/windows/wsl/install-win10) on Windows 10.\n\n### Install Node and NPM\n\n```sh\nsudo apt update\nsudo apt install nodejs\n```\n\n### Option: Install NVM\n\n- `https://www.liquidweb.com/kb/how-to-install-nvm-node-version-manager-for-node-js-on-ubuntu-12-04-lts/`\n- `https://github.com/nvm-sh/nvm/releases`\n\n### Install yarn\n\nhttps://www.hostinger.com/tutorials/how-to-install-yarn-on-ubuntu/\n\n```sh\nsudo apt update\nsudo apt install yarn nodejs\nyarn –version\n```\n\n### Install jq\n\n```sh\nsudo apt-get install jq\n```\n\n### Configuring your System\n\nOn Linux Systems Install packages\n\n- mesa-utils\n- xvfb\n- libgl1-mesa-dri\n- libglapi-mesa\n- libosmesa6\n- libxi-dev\n\nTo get the headless tests working: export DISPLAY=:99.0; sh -e /etc/init.d/xvfb start\n\n## Running Tests\n\n- `yarn lint`: Check coding standards and formatting\n- `yarn lint fix`: Fix errors with formatting\n- `yarn test node`: Quick test run under Node.js\n- `yarn test browser`: Test run under browser, good for interactive debugging\n- `yarn test`: Run lint, node test, browser tests (in headless mode)\n","slug":"docs/contributing","title":"Contributing"},{"excerpt":"RoadmapWe are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the…","rawMarkdownBody":"# Roadmap\n\nWe are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the following ways:\n\n- **[RFCs](https://github.com/uber-web/loaders.gl/tree/master/dev-docs/RFCs)** - RFCs are technical writeups that describe proposed features in upcoming releases.\n- **[Roadmap Document](https://github.com/uber-web/loaders.gl/tree/master/docs/overview/roadmap)** - (this document) A high-level summary of our current direction for future releases.\n- **[Blog](https://medium.com/@vis.gl)** - We use the vis.gl blog to share information about what we are doing.\n- **[Github Issues](https://github.com/uber-web/loaders.gl/issues)** - The traditional way to start or join a discussion.\n\n## Feature Roadmap\n\n**Off-thread parsing support** - Off thread parsing is an obvious optimization however it has some major complications that often eat up any performance gains: and serialization/deserialization overhead. loaders.gl is designed to avoid serialization through direct transfer of typed arrays.\n\n**Loader Worker Thread Pool** - Another performance \"killer\" for worker threads is multi-second startup time. loaders.gl exports an optional \"loader worker manager\" class that can help keep a loader thread pool loaded and primed and ready to start off-thread parsing as soon as data arrives on the wire.\n\n**Progress Tracking** - loaders can provide progress callbacks and a `ProgressTracker` class is provided to track the progress of a set of parallel loads.\n\n**Format Auto-Discovery** - Each loader can optionally expose a test function that can examine the \"head\" of a file to test if it is likely to be in a format this loader will be able to parse.\n\n**Stream Support** - Support stream based loaders... `loader.loadStream`\n\n## Format Roadmap\n\nThe emergence of glTF as a major Khronos standard with the ensuing massive industry adoption is a huge deal for the WebGL/3D community. The need to support long list of obscure loaders for e.g. various 3D asset authoring packages is quickly becoming a thing of the past as most major applications have started to offer high-quality, maintained glTF exporters.\n\nObviously we expect loaders.gl to have very solid glTF/GLB support. Also we will most likely not try to pursue \"competing\" scene/mesh description formats.\n\nStill, for special data sets such as large point clouds or complex geospatial data, the need for special formats for (e.g. compactness or expressivity) is unchanged, so this is the direction we expect most new loaders.gl loaders to focus on.\n\nFinally, some \"unusual\" loaders may be included just for fun, e.g. SVG tesselation.\n\n## Implemented\n\n**Test Data** - Ideally loaders.gl will include test data for each format to ensure that the regression suite is as effective as possible.\n","slug":"docs/roadmap","title":"Roadmap"},{"excerpt":"Upgrade Guidev1.1A couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings. Start replacing…","rawMarkdownBody":"# Upgrade Guide\n\n## v1.1\n\nA couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings. Start replacing your use of these functions now to remove the console warnings and ensure a smooth future upgrade to v2.0.\n\n\n### @loaders.gl/core\n\n### @loaders.gl/gltf\n\n- Deprecated: `GLBParser`/`GLBBuilder` - These will be merged into GLTF classes\n\n\n## v1.0\n\nFirst official release of loaders.gl\n","slug":"docs/upgrade-guide","title":"Upgrade Guide"},{"excerpt":"What's NewThis module is currently in unofficial \"soft pre-release\" stage. You are welcome to start using this module however be aware that…","rawMarkdownBody":"# What's New\n\nThis module is currently in unofficial \"soft pre-release\" stage. You are welcome to start using this module however be aware that APIs will be slightly fluent until version 1.0 is released.\n\n## v1.1\n\n### @loaders.gl/loader-utils: New module\n\nHelper functions for loaders have been broken out from `@loaders.gl/core`. Individual loaders no longer depend on @loaders.gl/core but only on @loaders.gl/loader-utils.\n\n### GLTF\n\n- New: `GLBLoader`/`GLBWriter` objects for custom GLB-formatted data\n- Deprecated: `GLBParser`/`GLBBuilder` - These will be merged into GLTF classes\n\n## v1.0\n\n- First Official Release\n","slug":"docs/whats-new","title":"What's New"},{"excerpt":"Get StartedInstallingInstall loaders.gl core and loader for any modules you would like to use.Each format is published as a separate npm…","rawMarkdownBody":"# Get Started\n\n## Installing\n\nInstall loaders.gl core and loader for any modules you would like to use.\n\nEach format is published as a separate npm module.\n\n```bash\nyarn add @loaders.gl/core\nyarn add @loaders.gl/gltf\n...\n```\n\n## Building\n\nloaders.gl is designed to leverage modern JavaScript (ES2018) and to optimize functionality and performance on evergreen browsers.\n\nHowever, the default distribution is completely transpiled to ES5 so using loaders.gl with older or \"slower moving\" browsers such as IE11 and Edge is possible if polyfills are added.\n\n\n## Supporting Older Browsers\n\nTo build on Edge and IE11, `TextEncoder` and `TextDecoder` must be polyfilled. There are several polyfills available on `npm`, but you can also use the polyfills provided by loaders.gl\n\n```js\nimport '@loaders.gl/polyfills';\n```\n\n\n## Running glbdump\n\nInstalling `@loaders.gl/gltf` makes the `glbdump` command line tool available to inspect content of gltf files. It can be run using `npx`.\n\n```bash\nnpx glbdump <filename>\n```\n\n","slug":"docs/get-started","title":"Get Started"},{"excerpt":"Creating New LoadersThis is an overview, see the a detailed specification of the loader object format API reference.loaders.gl has parser…","rawMarkdownBody":"# Creating New Loaders\n\n> This is an overview, see the a detailed specification of the [loader object format API reference](docs/api-reference/specifications/loader-object-formats).\n\nloaders.gl has parser functions that use so called \"loaders\" (or \"loader objects\") to convert the raw data loaded from files into parsed objects. Each loader object encapsulates a loader for one file format and essentially provides a parsing function and some metadata (like the loader name, common file extensions for the format etc). Loader object can be passed into utility functions in the loaders.gl core API to enable parsing of the chosen format.\n\nloaders.gl provides a suite of pre-built loader objects packaged as scoped npm modules. Your application can install and combine these as desired. It is also easy to create your own loader objects, e.g. if you have existing javascript loaders that you would like to use with the loaders.gl core utility functions.\n\n### Common Fields\n\nYou would give a name to the loader object, define what file extension(s) it uses.\n\n| Field       | Type     | Default  | Description                                                     |\n| ----------- | -------- | -------- | --------------------------------------------------------------- |\n| `name`      | `String` | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension` | `String` | Required | Three letter (typically) extension used by files of this format |\n\n### Test Function\n\n| Field      | Type       | Default | Description                                                                       |\n| ---------- | ---------- | ------- | --------------------------------------------------------------------------------- |\n| `testText` | `Function` | `null`  | Guesses if a file is of this format by examining the first characters in the file |\n\n### Parser Function\n\nA loader must define a parser function for the format, a function that takes the loaded data and converts it into a parsed object. Depending on how the underlying loader works (whether it is synchronous or asynchronous and whether it expects text or binary data), the loader object can expose the parser in a couple of different ways, specified by provided one of the parser function fields.\n\nThe loaders.gl `load` and `parse` functions accept one or more loader objects. These functions examines what format the loader needs (text or binary), reads data into the required format, and then calls one of the loader object's parser functions with that data.\n\nWhen creating a new loader object, at least one of the parser functions needs to be defined:\n\n| Parser function field | Type       | Default | Description                                         |\n| --------------------- | ---------- | ------- | --------------------------------------------------- |\n| `parseTextSync`       | `Function` | `null`  | Parses a text file synchronously (`String`)         |\n| `parseSync`           | `Function` | `null`  | Parses a binary file synchronously (`ArrayBuffer`)  |\n| `parse`               | `Function` | `null`  | Parses a binary file asynchronously (`ArrayBuffer`) |\n| `load`                | `Function` | `null`  | Reads and parses a binary file asynchronously       |\n\n- The preferred option is to provide a synchronous parser that works on loaded data (using the `parseSync` or `parseTextSync` fields). This allows the use of the `parseSync` function with your loader.\n- The second preference is to provide an asynchronous parser that works on loaded data (`parse`). This allows the user to load the data using any preferred mechanism, and only use loaders.gl for parsing by calling `parse` on the loaded data.\n- Finally, some existing parsers combine both loading and parsing, and loaders.gl provides an accommodation for packaging such loaders into loader options (`load`). The `load` parser field is for instance used to define a loader object using the classic browser image loading technique of creating a new `Image` and setting its `src` and `onload` fields.\n\n## Remarks\n\n- The reason synchronous parsers are preferred is e.g. that it is often easier to debug synchronous parsers, so it is nice to have the option to run things synchronously. And no functionality is lost, as loaders.gl core utlities will still transparently run these parsers asynchronously on worker threads or wrap them in promises if desired.\n- The reason pure parsers that don't read/reuqest data but just parse it are preferred is because it gives more freedom to the application to select how data is loaded.\n","slug":"docs/developer-guide/about-loaders","title":"Creating New Loaders"},{"excerpt":"ArrayBuffersloaders.gl API consistently uses ArrayBuffers to represent and transport binary data.Why ArrayBuffers?One of the design goals of…","rawMarkdownBody":"# ArrayBuffers\n\nloaders.gl API consistently uses ArrayBuffers to represent and transport binary data.\n\n## Why ArrayBuffers?\n\nOne of the design goals of loaders.gl is to provide applications with a single, consistent API that works across (reasonably modern) browsers, worker threads and Node.js. One of the characteristics of this API is how binary data is represented.\n\nloaders.gl \"standardizes\" on ArrayBuffers for a number of reasons:\n\n- ArrayBuffers are the \"canonical\" input format for the WebGL API, allowing efficient uploads of large binary data sets to the GPU.\n- ArrayBuffers allow ownership to be transferred between threads (Browser Main Thread and WebWorkers), massively improving performance when parsing on loaders.\n- ArrayBuffers are used to transport raw data and most newer JavaScript APIs rely on them, including WebSockets, Web Intents, XMLHttpRequest version 2 etc.\n- ArrayBuffers are well supported by recent Node.js versions, in fact the traditional Node.js `Buffer` class is now backed by an `ArrayBuffer`.\n\n## ArrayBuffers and Typed Arrays\n\nRecall that typed arrays (e.g. `Float32Array`) are just views into array buffers. Every typed array has a `buffer` reference.\n\nMany loaders.gl functions directly accept typed arrays.\n\nCaveat: typed arrays that are partial views (e.g. with offsets) sometimes need special handling in the application.\n\n## Converting between ArrayBuffers and Strings\n\nUse `TextEncoder` and `TextDecoder` in the JavaScript [string encoding/decoding library](https://github.com/inexorabletash/text-encoding).\n\nSince these classes are central to using ArrayBuffers correctly, loaders.gl re-exports these symbols, transparently polyfilling them under Node.js.\n\n## Converting between ArrayBuffers and other Binary Formats.\n\nWhile standardizing on ArrayBuffers helps streamline the loaders.gl API and application code, occaionally applications need to interface with APIs that accept other binary data types/formats. To support this case, loaders.gl provides a small set of utilities (non-exhaustive) for converting from and to other binary JavaScript types/formats, e.g. `toArrayBuffer`:\n\nBinary formats in JS:\n\n- ArrayBuffer\n- Uint8Array\n- Blob\n- nodejs Buffer\n\nExamples of semi-\"binary\" formats in JS:\n\n- array: Array of bytes (numbers between 0 and 255).\n- string (binary): string in “binary” form, 1 byte per char (2 bytes).\n- string (base64): string containing the binary data encoded in a base64 form.\n\n## Utilities\n\n- [BufferReader/BufferWriter](https://github.com/yuntonyx/arraybuffer-utils) - Helps keep track of current position when working with DataView's through a tightly packed binary object.\n","slug":"docs/developer-guide/array-buffers","title":"ArrayBuffers"},{"excerpt":"Creating New WritersThis is an overview, for a detailed specification of the writer object format see the API reference.TBA","rawMarkdownBody":"# Creating New Writers\n\n> This is an overview, for a detailed specification of the writer object format see the [API reference](docs/api-reference/specifications/writer-object-formats.md).\n\nTBA\n","slug":"docs/developer-guide/about-writers","title":"Creating New Writers"},{"excerpt":"AsyncIteratorsStreaming functionality in loaders.gl is built on the ES2018  concept. This page gives some background on AsyncIterator since…","rawMarkdownBody":"# AsyncIterators\n\nStreaming functionality in loaders.gl is built on the ES2018 `AsyncIterator` concept. This page gives some background on AsyncIterator since it is a recently introduced concept (at least as part of the JavaScript standard).\n\n## Availability\n\n`AsyncIterator` is a standard JavaScript ES2018 feature and is well supported by recent evergreen browsers and Node.js versions.\n\nThe `for await of` iteration syntax is supported as well as the babel transpiler.\n\n## Batched Parsing and Endcoding using AsyncIterators\n\nThe input and output from streaming loaders and writers can both be expressed in terms of async iterators.\n\n## Using AsyncIterator\n\nRemember tyhat an async iterator can be consumed (iterated over) via the for-await construct:\n\n```js\nfor await (const x of asyncIterable) {}\n```\n\n## Using Streams as AsyncIterators\n\nWith a little effort, streams in JavaScript can be treated as AsyncIterators. As the section about [Javascript Streams](docs/developer-guide/streams.md) explains, instead of registering callbacks on the stream, you can now work with streams in this way:\n\n```js\nfor await (const buf of fs.createReadStream('foo.txt')) {\n  // do something\n}\n```\n\n## Creating AsyncIterators\n\nRemember that any object in JavaScript that implements the `[Symbol.asyncIterator]()` method is an `AsyncIterable`. And the async generator syntax can be used to generate new async iterators\n\n```js\nasync function* asyncIterator() {\n  yield new Promise(...)\n}\n\nfor await (const x of asyncIterator()) {} // Notice parens after 'asyncIterator'\n```\n","slug":"docs/developer-guide/async-iterators","title":"AsyncIterators"},{"excerpt":"Loader Categoriesloaders.gl defines \"categories\" of loaders that load very similar data (e.g. point clouds). When it is reasonably easy to…","rawMarkdownBody":"# Loader Categories\n\nloaders.gl defines \"categories\" of loaders that load very similar data (e.g. point clouds). When it is reasonably easy to do so, loaders.gl converts the returned data in a standardized format for that category. This allows an application to support multiple formats with a single code path, since all the loaders will return similar data structures.\n\nCurrently defined categories:\n\n- Meshes/Point Cloud (overlaps with Point Clouds and shares format)\n- GeoJSON\n\n## Common Format of Loaded Data\n\nOn a successful parse, all loaders will return a data object with a minimal standardized payload as follows:\n\n| Field        | Type                | Contents                                                                                                                                                                       |\n| ------------ | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `loaderData` | `Object` (Optional) | Loader and format specific data, such as e.g. original header fields. Can correspond one-to-one with the data in the specific file format itself, or be defined by the loader. |\n| `header`     | `Object`            | Standardized header information - can contain number of vertices, etc.                                                                                                         |\n| `...`        | `*`                 | Standardized data, depends on which \"category\" the loader conforms to                                                                                                          |\n\n> `loaderData` should not be considered stable between releases, since loaders.gl can choose to replace the underlying loader for performance or feature reasons.\n\n### Binary Data\n\nloaders.gl will attempt to load any data that is a contiguous array of numbers as a typed array (causing that data to be stored as a contiguous array of binary memory).\n","slug":"docs/developer-guide/loader-categories","title":"Loader Categories"},{"excerpt":"Inspecting Files is a utility for inspecting the structure of GLB/glTF binary container files.Installing  makes the  command line tool…","rawMarkdownBody":"# Inspecting Files\n\n`glbdump` is a utility for inspecting the structure of GLB/glTF binary container files.\n\nInstalling `@loaders.gl/gltf` makes the `glbdump` command line tool available. It can be run using `npx`.\n\n```\n$ npx glbdump <filename>\n```\n","slug":"docs/developer-guide/inspecting-files","title":"Inspecting Files"},{"excerpt":"PolyfillsOlder browsers (mainly Edge and IE11) as well as Node.js versions prior to v11 do not provide certain classes that loaders.gl…","rawMarkdownBody":"## Polyfills\n\nOlder browsers (mainly Edge and IE11) as well as Node.js versions prior to v11 do not provide certain classes that loaders.gl depends on.\n\nWhile there are many good polyfill modules available on `npm`, to make the search for a version that works directly with loaders.gl a little easier, a polyfill module is provided.\n\nTo install the polyfills, just import it before you start using loaders.gl.\n\n```js\nimport '@loaders.gl/polyfills';\nimport {fetchFile} from '@loaders.gl/core';\n```\n\n## Combining with other Polyfills\n\nloaders.gl only installs polyfills if the corresponding global symbol is `undefined`. This means that if another polyfill is already installed when `@loaders.gl/polyfills` is imported, the other polyfill will remain in effect. Since most polyfill libraries work this way, applications can mix and match polyfills by ordering the polyfill import statements appropriately (but see the remarks below for a possible caveat).\n\n\n## Provided Polyfills\n\nSee [API Reference](/docs/api-reference/polyfills).\n\n## Remarks\n\nApplications should typically only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, they will still be included in your application bundle, i.e. importing the polyfill module will increase your application's bundle size.\n\nWhen importing polyfills for the same symbol from different libraries, the import can depend on how the other polyfill is written. to control the order of installation, you may want to use `require` rather than `import` when importing `@loaders.gl/polyfills`. As a general rule, `import` statements execute before `require` statments.\n\n","slug":"docs/developer-guide/polyfills","title":" Polyfills"},{"excerpt":"Streaming SupportStreaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both…","rawMarkdownBody":"# Streaming Support\n\n> Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node and DOM streams, through a consistent API and set of conventions (for both applications and loader/writer objects).\n\n## Streaming Loads\n\n### Incremental Parsing\n\nSome loaders offer incremental parsing (chunks of incomplete data can be parsed, and updates will be sent after a certain batch size has been exceeded). In many cases, parsing is fast compared to loading of data, so incremental parsing on its own may not provide a lot of value for applications.\n\n### Incremental Loading\n\nIncremental parsing becomes more interesting when it can be powered by incremental loading, whether through request updates or streams (see below).\n\n### Streamed Loading\n\nStreamed loading means that the entire data does not need to be loaded.\n\nThis is particularly advantageous when:\n\n- loading files with sizes that exceed browser limits (e.g. 1GB in Chrome)\n- doing local processing to files (tranforming one row at a time), this allows pipe constructions that can process files that far exceed internal memory.\n\n## Batched Updates\n\nFor incemental loading and parsing to be really effective, the application needs to be able to deal efficiently with partial batches as they arrive. Each loader category (or loader) may define a batch update conventions that are appropriate for the format being loaded.\n\n## Streaming Writes\n\nTBA\n\n## Node Streams vs DOM Streams\n\nStream support is finally arriving in browsers, however DOM Streams have a slightly different API than Node streams and the support across browsers is still spotty.\n\n## Polyfills\n\nStream support across browsers can be somewhat improved with polyfills. TBA\n\n## Stream Utilities\n\n- Stream to memory, ...\n- Automatically create stream if loader/writer only supports streaming\n- ...\n","slug":"docs/developer-guide/streaming","title":"Streaming Support"},{"excerpt":"Worker Thread LoadingReasons for moving loading to workers:When parsing is CPU heavy, the browser main thread can become blocked, freezing…","rawMarkdownBody":"# Worker Thread Loading\n\nReasons for moving loading to workers:\n\n- When parsing is CPU heavy, the browser main thread can become blocked, freezing the application until parsing completes.\n- Leverage multi-core CPUs when parsing multiple data items.\n\nConsiderations when moving loading and parsing to workers:\n\n- Data Transfer - Serializing/deserializing when transferring resuls back to main thread can more than defeat gains from loading on a separate thread.\n- Data Types - Due to data transfer issues there are constraints on what data types are appropriate\n- Configuration - Creating workers can require build system setup/configuration.\n- Message Passing - Parsing on workers requires message passing between threads. While simple it can add clutter to application code.\n- Debugging - Worker based code can be somewhat harder to debug. Being able to move the code back to the main thread can help.\n- Startup Times - Worker startup times can defeat speed gains from parsing on workers.\n- Thread Pool Management (TBA) -\n\n## Data Transfer\n\nThreads cannot share non-binary data structures and these have to be serialized/deserialized. This is a big issue for worker thread based loading as the purpose of loaders is typically to load and parse big datastructures, and main thread deserialization times are often comparable to or even exceed the time required to parse the data in the first place, defeating the value of moving parsing to a worker thread.\n\nThe solution is usually to use data types that support ownership transfer (see next section) as much as possible and minimize the amount of non-binary data returned from the parser.\n\n## Data Types\n\nJavaScript ArrayBuffers and Typed Arrays can be passed with minimal overhead (ownership transfer) and the value of worker based parsing usually depends on whether the loaded data can (mostly) be stored in these types.\n\n## Message Passing\n\nloaders.gl will handle message passing behind the scenes. Loading on a worker thread returns a promise that completes when the worker is done and the data has been transferred back to the main thread.\n\n## Build Configuration\n\nAll worker enabled loaders come with a pre-built, minimal worker \"executable\" to enable zero-configuration use in applications.\n\n## Bundle size concerns\n\nAll worker enabled loaders provide separate loader objects to ensure that tree-shaking bundlers will be able to remove the code for the unused case.\n\n## Debugging and Benchmarking\n\nLoaders.gl offers loader objects for main thread and worker threads. A simple switch lets you move your loading back to the main thread for easier debugging and benchmarking (comparing speeds to ensure you are gaining the benefits you expect from worker thread based loading).\n\n## Startup Times (TBA)\n\nThrough Thread Pool Management it will be possible to start worker threads before they ae needed to minimize worker loading delay when parsing.\n\n## Thread Pool Management (TBA)\n\nIt can be valuable to run muliple instances of the same worker thread to leverage multi-core CPUs. Being able to warm-up (pre-iniutilize) the thread pool and set limits of how many threads of each worker type...\n","slug":"docs/developer-guide/worker-threads","title":"Worker Thread Loading"},{"excerpt":"GeoJSON Loader CategoryMany geospatial formats can be converted to GeoJSON (sometimes with a loss of some information). Since most…","rawMarkdownBody":"# GeoJSON Loader Category\n\nMany geospatial formats can be converted to GeoJSON (sometimes with a loss of some information). Since most geospatial pplications can consume geojson, it can make sense to provide a GeoJSON conversion option for geospatial loaders.\n","slug":"docs/api-reference/geojson-loaders/category-geojson","title":"GeoJSON Loader Category"},{"excerpt":"KMLLoaderKML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google Earth…","rawMarkdownBody":"# KMLLoader\n\nKML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google Earth (originally named \"Keyhole Earth Viewer\"). It can be used with any 2D or 3D maps.\n\nReferences:\n\n- [Keyhole Markup Language - Wikipedia](https://en.wikipedia.org/wiki/Keyhole_Markup_Language)\n- [KML Tutorial - Google](https://developers.google.com/kml/documentation/kml_tut)\n\n## Usage\n\n```js\nimport {KMLLoader} from '@loaders.gl/kml';\nimport {load} from '@loaders.gl/core';\n```\n\n## Structure of Loaded Data\n\nThe parser will return a JavaScript object with a number of top-level array-valued fields:\n\n| Field           | Description                        |\n| --------------- | ---------------------------------- |\n| `documents`     |                                    |\n| `folders`       |                                    |\n| `links`         |                                    |\n| `points`        | Points                             |\n| `lines`         | Lines                              |\n| `polygons`      | Polygons                           |\n| `imageoverlays` | Urls and bounds of bitmap overlays |\n\n## Parser Options\n\n> Work in progress\n\n| Option            | Default | Description                                                                                                |\n| ----------------- | ------- | ---------------------------------------------------------------------------------------------------------- |\n| `useLngLatFormat` | `true`  | KML longitudes and latitudes are specified as `[lat, lng]`. This option \"normalizes\" them to `[lng, lat]`. |\n| `useColorArrays`  | `true`  | Convert color strings to arrays                                                                            |\n\n## Limitations\n\n- Currently XML parsing is only implemented in browsers, not in Node.js. Check `KMLLoader.supported` to check at run-time.\n\n## License/Credits/Attributions\n\nLicense: MIT\n\n`XMLLoader` is an adaptation of Nick Blackwell's [`js-simplekml`](https://github.com/nickolanack/js-simplekml) module.\n","slug":"docs/api-reference/geojson-loaders/kml-loader","title":"KMLLoader"},{"excerpt":"GLBBuilder (Experimental)The  class allows applications to use a \"fluent\" API to dynamically build up a hybrid JSON/binary GLB file. The…","rawMarkdownBody":"# GLBBuilder (Experimental)\n\nThe `GLBBuilder` class allows applications to use a \"fluent\" API to dynamically build up a hybrid JSON/binary GLB file. The `GLBBuilder` would normally be used if you want to save custom mixed JSON/binary data in a \"GLB envelope\".\n\nReferences:\n\n- For more information, see [glTF and GLB support](docs/) in the Developer's Guide.\n\n## Usage\n\nAdding binary data sub chunks to the GLB file, then calling encode to generate the complete `arrayBuffer`.\n\n```js\nimport {GLBBuilder} from '@loaders.gl/gltf';\nimport {saveBinaryFile} from '@loaders.gl/core';\n\nconst gltfBuilder = new GLBBuilder();\n\nconst IMAGE_DATA = ...; // Image as ArrayBuffer\nconst imageIndex = gltfBuilder.addImage(IMAGE_DATA);\n\n// Add custom JSON in top-level glTF object\ngltfBuilder.addApplicationData('app-key', {...});\n\n// All data added, we can encode\nconst arrayBuffer = gltfBuilder.encodeAsGLB();\n\n// The encoded `ArrayBuffer` represents a complete binary representation of the data that can be written atomically to file\nsaveBinaryFile(filename, arrayBuffer);\n```\n\n## Methods\n\n### constructor()\n\nCreates a new `GLBBuilder` instance.\n\n### encodeAsGLB(options : Object) : ArrayBuffer\n\nCombines your added JSON data structures () with any generated JavaScript and any binary subchunks, into a single GLB encoded `ArrayBuffer` that can be written directly to file.\n\nNote: `encode()` is a one time operation. It should only be called once all data and binary buffers have been added to the builder.\n\n### encodeAsGLBWithJSON(options : Object) : Object\n\nA version of `encode` that returns the final arrayBuffer together with the generated JSON. Note that the returned `arrayBuffer` contains the JSON and is identical to the `encodeAsGLB`.\n\n### addApplicationData(key : String, data : any [, packOptions: Object])\n\nStores the supplied `data` in the given top-level field given by `key`.\n\nThe data object will be encoded as JSON before being stored. By default, any typed arrays in the data object will be removed fromn the data payload and packed in the binary chunk.\n\n- `packOptions.packTypedArrays` - Packs typed arrays into the binary chunk\n- `packOptions.flattenArrays` - Flatten \"nested\" standard JavaScript arrays into typed arrays (and then pack them into the binary chunk).\n\n### addBuffer(typedArray : TypedArray, accessor = {size: 3} : Object) : Number\n\nAdds one binary array intended to be loaded back as a WebGL buffer.\n\n- `typedArray` -\n- `accessor` - {size, type, ...}.\n\nType is autodeduced from the type of the typed array.\n\nThe binary data will be added to the GLB BIN chunk, and glTF `bufferView` and `accessor` fields will be populated.\n\n### addImage(typedArray: TypedArray) : Number\n\nAdds a glTF image. The binary image data will be added to the GLB BIN chunk, and glTF `bufferView` and `image` fields will be populated.\n","slug":"docs/api-reference/deprecated/glb-builder","title":"GLBBuilder (Experimental)"},{"excerpt":"GLBParser (Deprecated)The  class lets the application access data encoded in a GLB binary \"envelope\". Most applications would use the  class…","rawMarkdownBody":"# GLBParser (Deprecated)\n\nThe `GLBParser` class lets the application access data encoded in a GLB binary \"envelope\". Most applications would use the `glTFParser` class instead (which uses the `GLBParser` class under the hood to parse GLB-encoded glTF files).\n\nHowever, the GLB encoding can potentially also be used independently to save mixed JSON and binary data, in which case having access to the `GLBParser` class can be helpful.\n\nReferences:\n\n- For more information, see [GLB and GLB support](docs/) in the Developer's Guide.\n\n## Usage\n\n```js\nimport {GLBParser} from '@loaders.gl/gltf';\nimport {load} from '@loaders.gl/core';\n\n// Create a parser\nconst glbParser = new GLBParser();\n\n// Load and parse a file\nconst GLB_BINARY = await load(...);\nglbParser.parse(GLB_BINARY);\n\n// Get the complete GLB JSON structure\nconst gltfJson = glbParser.getJSON();\n\n// Get specific fields from the JSON structure\nconst appData = glbParser.getApplicationData('customData');\n```\n\n## Methods\n\n### constructor(options : Object)\n\nCreates a new `GLBParser` instance.\n\n### parse(arrayBuffer : ArrayBuffer) : Object\n\nParses an in-memory, GLB formatted `ArrayBuffer` into:\n\n- `arrayBuffer` - just returns the input array buffer\n- `binaryByteOffset` - offset to the first byte in the binary chunk\n- `json` - a JavaScript \"JSON\" data structure with inlined binary data fields.\n","slug":"docs/api-reference/deprecated/glb-parser","title":"GLBParser (Deprecated)"},{"excerpt":"GLTF Loader CategoryData formatA gltf scenegraph is described by a parsed JSON object (with top level arrays for ,  etc) together with a…","rawMarkdownBody":"# GLTF Loader Category\n\n## Data format\n\nA gltf scenegraph is described by a parsed JSON object (with top level arrays for `scenes`, `nodes` etc) together with a list of `ArrayBuffer`s representing binary blocks into which `bufferViews` and `images` in the JSON point).\n\nAt their core glTF and GLB loaders extract this information, however additional classes are provided to make processing of the returned data easier.\n\nWhile the glTF \"category\" is obviously quite specific glTF, loaders for other scenegraph formats (e.g. COLLADA) could potentially also choose to \"convert\" the loaded data to this glTF format and thus enable interoperabiity with applications that are already designed to use the `GLTFLoader`.\n\n## GLTFBuilder API\n\nA glTF file can be built programmatically using the GLTFBuilder's \"fluent API\":\n\n```js\nconst builder = new GLTFBuilder(...)\n  .addApplicationData(...);\n  .addExtras(...);\n  .addExtension(...);\n  .addRequiredExtension(...)\n  .encodeAsGLB(...);\n```\n\n## Adding Custom Payloads to glTF files\n\nglTF provides multiple mechanisms for adding custom data to a glTF conformant file. The application just needs to decide where to store it. Normally it should be added using one of the `addApplicationData`, `addExtras`, `addExtension` or `addRequiredExtension` methods.\n\n## Binary Packing of Typed Arrays in JSON Data\n\nThe `GLTFLoader` and `GLTFBuilder` classes include support for moving (packing) typed arrays in the application JSON into the binary GLB chunk.\n\nThe packing process extracts binary (typed) arrays from the supplied `json` data structure, placing these in compact binary chunks (by calling the appropriate `add...` methods on the builder). The \"stripped\" JSON chunk will contain \"JSON pointers\" that the `GLTFParser` can later use to restore the JSON structure on load.\n\n### Flattening Nested Numeric Arrays\n\nAs an option, standard JavaScript arrays can also be stored in the binary chunk under certain conditions. This feature supports arrays that contain only numbers. It also supports arrays that also contain nested arrays that only contain numbers.\n\nThe major complication when packing nested arrays is that the internal structure is lost. For instance, a coordinate array `[[1, 2, 0], [2, 1, 0]]` will be packed and unpacked as `[1, 2, 0, 2, 1, 0]`. To assist with recovering this information, the flattening process estimates the `size` of top-level elements and stored as a named field on the typed array.\n\n## Using GLB as a \"Binary Container\" for Arbitrary Data\n\nThe GLB binary container format used by glTF addresses a general need to store a mix of JSON and binary data, and can potentially be used as a foundation for building custom loaders and writers.\n\nTo allow for this (and also to generally improve the glTF code structure), the `GLTFLoader` and `GLTFBuilder` classes are built on top of GLB focused classes (`GLBLoader` and `GLBBuilder`) that can be used independently of the bigger glTF classes.\n\n## glTF Extension Support\n\nCertain glTF extensions are fully or partially supported by the glTF classes. For details on which extensions are supported, see [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions).\n\n## Draco Mesh and Point Cloud Compression\n\nDraco encoding and decoding is supported by the `GLTFBuilder` and `GLTFParser` classes but requires the DracoWriter and DracoLoader dependencies to be \"injected\" by the application.\n\n```js\nimport {GLTFBuilder} from '@loaders.gl/gltf';\nimport {DracoWriter, DracoLoader} from '@loaders.gl/draco';\n\nconst gltfBuilder = new GLTFBuilder({DracoWriter, DracoLoader});\n```\n","slug":"docs/api-reference/gltf-loaders/category-gltf","title":"GLTF Loader Category"},{"excerpt":"GLBLoaderThe  is a writer for GLB binary \"envelope\".Note: applications that want to parse GLB-formatted glTF files use the  instead. The  is…","rawMarkdownBody":"# GLBLoader\n\nThe `GLBLoader` is a writer for GLB binary \"envelope\".\n\nNote: applications that want to parse GLB-formatted glTF files use the `GLTFLoader` instead. The `GLBLoader` is intended to be used to load custom data that combines JSON and binary resources.\n\n| Loader                | Characteristic                                                                                          |\n| --------------------- | ------------------------------------------------------------------------------------------------------- |\n| File Extensions       | `.glb`                                                                                                  |\n| File Types            | Binary                                                                                                  |\n| File Format           | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Format Category       | N/A (GLB Payload)                                                                                       |\n| Writer Type           | Synchronous                                                                                             |\n| Worker Thread Support | No                                                                                                      |\n| Streaming Support     | No                                                                                                      |\n\n## Usage\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLBLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLBLoader);\n```\n\n## Data Format\n\n```js\n{\n  magic: Number,\n  version: Number,\n  json: Object,\n  binary: ArrayBuffer\n}\n```\n\n## Options\n\n| Option  | Default | Description                              |\n| ------- | ------- | ---------------------------------------- |\n| `magic` | GLTF    | The magic number to be save in the file. |\n\n## Attributions\n\nThe `GLBLoader` was developed for loaders.gl.\n","slug":"docs/api-reference/gltf-loaders/glb-loader","title":"GLBLoader"},{"excerpt":"GLBWriter (Deprecated)The  is a writer for the GLB binary \"envelope\".Note: applications that want to encode GLB-formatted glTF files use the…","rawMarkdownBody":"# GLBWriter (Deprecated)\n\nThe `GLBWriter` is a writer for the GLB binary \"envelope\".\n\nNote: applications that want to encode GLB-formatted glTF files use the `GLTFWriter` instead. The `GLBWriter` is intended to be used to save custom data that combines JSON and binary resources.\n\n| Writer                | Characteristic                                                                                          |\n| --------------------- | ------------------------------------------------------------------------------------------------------- |\n| File Extensions       | `.glb`                                                                                                  |\n| File Type             | Binary                                                                                                  |\n| File Format           | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Format Category       | N/A (GLB Payload)                                                                                       |\n| Writer Type           | Synchronous                                                                                             |\n| Worker Thread Support | No                                                                                                      |\n| Streaming Support     | No                                                                                                      |\n\n## Usage\n\n```js\nimport {GLBWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLBWriter, options);\n```\n\n## Data Format\n\n```js\n{\n  version: Number,\n  json: Object,\n  binary: ArrayBuffer\n}\n```\n\n## Options\n\n- `magic` - The first four bytes of the file\n\n## Attributions\n\nThe `GLBWriter` was developed for loaders.gl.\n","slug":"docs/api-reference/gltf-loaders/glb-writer","title":"GLBWriter (Deprecated)"},{"excerpt":"glbdump is a utility for inspecting the structure of GLB/glTF binary container files.Installing loaders.gl/gltf makes the  command line tool…","rawMarkdownBody":"## glbdump\n\n`glbdump` is a utility for inspecting the structure of GLB/glTF binary container files.\n\nInstalling loaders.gl/gltf makes the `glbdump` command line tool available. It can be run using `npx`.\n\n```\n$ npx glbdump <filename>\n```\n","slug":"docs/api-reference/gltf-loaders/glbdump","title":" glbdump"},{"excerpt":"GLTFBuilderThe  class allows applications to use a \"fluent\" API to dynamically build up a hybrid JSON/binary GLB file. The  would normally…","rawMarkdownBody":"# GLTFBuilder\n\nThe `GLTFBuilder` class allows applications to use a \"fluent\" API to dynamically build up a hybrid JSON/binary GLB file. The `GLTFBuilder` would normally be used if you want to save custom mixed JSON/binary data in a \"GLB envelope\".\n\nReferences:\n\n- For more information, see [glTF and GLB support](docs/) in the Developer's Guide.\n\n## Usage\n\nAdding binary data sub chunks to the GLB file, then calling encode to generate the complete `arrayBuffer`.\n\n```js\nimport {GLTFBuilder} from '@loaders.gl/gltf';\nimport {saveBinaryFile} from '@loaders.gl/core';\n\n\nconst gltfBuilder = new GLTFBuilder();\n\nconst IMAGE_DATA = ...; // Image as ArrayBuffer\nconst imageIndex = gltfBuilder.addImage(IMAGE_DATA);\n\n// Add custom JSON in top-level glTF object\ngltfBuilder.addApplicationData('app-key', {...});\n\n// Add custom JSON in glTF extras field\ngltfBuilder.addExtraData('app-key', {...});\n\n// Add custom JSON in an optional glTF extension object.\ngltfBuilder.addExtension('ORGNAME_extension_1', {...});\n\n// Add custom JSON into a required glTF extension object.\ngltfBuilder.addRequiredExtension('ORGNAME_extension_2', {...});\n\n// All data added, we can encode\nconst arrayBuffer = gltfBuilder.encodeAsGLB();\n\n// The encoded `ArrayBuffer` represents a complete image of the data\nsaveBinaryFile(filename, arrayBuffer);\n```\n\n## Methods\n\n### constructor(options : Object)\n\nCreates a new `GLTFBuilder` instance.\n\n### encodeSync(options : Object) : ArrayBuffer\n\nCombines your added JSON data structures (in extras, extensions etc) with any generated JavaScript and any binary subchunks, into a single GLB encoded `ArrayBuffer` that can be written directly to file.\n\n- `options.DracoWriter` - To enable DRACO encoding, the application needs to import and supply the `DracoWriter` _writer object_.\n- `options.DracoLoader` - To enable DRACO encoding, the application needs to import and supply the `DracoLoader` _loader object_.\n\nNote: `encodeSync()` is a one time operation. It should only be called once all data and binary buffers have been added to the builder.\n\n### encodeAsGLBWithJSON(options : Object) : Object\n\nA version of `encode` that returns the final arrayBuffer together with the generated JSON. Note that the returned `arrayBuffer` contains the JSON and is identical to the `encodeAsGLB`.\n\n### setApplicationData(key : String, data : any [, options: Object])\n\nStores the supplied `data` in the given top-level field given by `key`.\n\n- `options.packTypedArrays` - Packs typed arrays into the binary chunk\n- `options.flattenArrays` - Pack (and flatten nested) standard JavaScript arrays into the binary chunk.\n\n### setExtraData(key : String, data : any [, options: Object])\n\nPopulates (merges into) the top-level glTF `extras` field, which the glTF specification reserves for application specific data.\n\n- `options.packTypedArrays` - Packs typed arrays into the binary chunk\n- `options.flattenArrays` - Pack (and flatten nested) standard JavaScript arrays into the binary chunk.\n\n### addExtension(extensionName : String, extension : any [, options: Object])\n\nAdds a top-level glTF extension object, and marks it as used.\n\n- `options.packTypedArrays` - Packs typed arrays into the binary chunk\n- `options.flattenArrays` - Pack (and flatten nested) standard JavaScript arrays into the binary chunk.\n\n### addRequiredExtension(extensionName : String, extension : any [, options: Object])\n\nAdds a top-level glTF extension object, and marks it as used and required.\n\nNote: If possible, use `addExtension` instead of `addRequiredExtension`. It is recommended to avoid using required extensions if possible, as they can reduce the ability to use glTF tools on the resulting file.\n\n- `options.packTypedArrays` - Packs typed arrays into the binary chunk\n- `options.flattenArrays` - Pack (and flatten nested) standard JavaScript arrays into the binary chunk.\n\n### isImage(imageData)\n\nReturns `true` if the binary data represents a well-known binary image format.\n\nNote: This is a utility that is provided to make it easier for decoders to choose whether a binary chunk of data should be stored as an \"image\" or a \"buffer\".\n\n### addBuffer(typedArray : TypedArray, accessor = {size: 3} : Object) : Number\n\nAdds one binary array intended to be loaded back as a WebGL buffer.\n\n- `typedArray` -\n- `accessor` - {size, type, ...}.\n\nType is autodeduced from the type of the typed array.\n\nThe binary data will be added to the GLB BIN chunk, and glTF `bufferView` and `accessor` fields will be populated.\n\n### addImage(typedArray: TypedArray) : Number\n\nAdds a glTF image. The binary image data will be added to the GLB BIN chunk, and glTF `bufferView` and `image` fields will be populated.\n\n### addMesh(attributes: Object [, indices : TypedArray [, mode = 4 : Number ]]) : Number\n\nAdds a glTF mesh. The glTF Mesh will contain one primitive with the supplied attributes.\n\n### addCompressedMesh(attributes: Object [, indices : TypedArray [, mode = 4 : Number ]]) : Number\n\nAdds a glTF mesh. The glTF Mesh will contain one primitive with the supplied attributes, compressed using DRACO compression.\n\n### addPointCloud(attributes: Object) : Number\n\nAdds a glTF mesh. The glTF Mesh will contain one primitive with the supplied attributes, representing a point cloud (no `indices`, mode will default to `0` etc).\n\n### addCompressedPointCloud(attributes: Object) : Number\n\nAdds a glTF mesh. The glTF Mesh will contain one primitive with the supplied attributes, representing a point cloud (no `indices`, mode will default to `0` etc). The point cloud will be compressed using DRACO compression.\n","slug":"docs/api-reference/gltf-loaders/gltf-builder","title":"GLTFBuilder"},{"excerpt":"glTF ExtensionsArbitrary glTF extensions can be present in glTF files, and will remain present in the parsed JSON as you would expect. Such…","rawMarkdownBody":"# glTF Extensions\n\nArbitrary glTF extensions can be present in glTF files, and will remain present in the parsed JSON as you would expect. Such extensions can supported by applications by inspecting the `extensions` fields inside glTF objects, and it is up to each application to handle or ignore them.\n\nMany glTF extensions affect e.g. rendering which is outside of the scope of loaders.gl, however in a few cases it is possible to provide support for extensions directly during loading. This article describes glTF extensions that are fully or partially processed by the `@loaders.gl/gltf` classes.\n\n## Official Extensions\n\n### KHR_draco_mesh_compression\n\nSupports compression of mesh attributes (geometry).\n\nSpecification: [KHR_draco_mesh_compression](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_draco_mesh_compression).\n\nParsing Support:\n\n- By adding the `decompress: true` options to the `GLTFParser` any decompressed by the `GLTFParser`.\n- The expanded attributes are placed in the mesh object (effectively making it look as if it had never been compressed).\n- The extension objects are removed from the glTF file.\n\nEncoding Support:\n\n- Meshes can be compressed as they are added to the `GLTFBuilder`.\n\n### KHR_lights_punctual\n\nSupports specification of point light sources and addition of such sources to the scenegraph node.\n\nSpecification: [KHR_lights_punctual](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_lights_punctual)\n\nParsing Support:\n\n- Any nodes with a `KHR_lights_punctual` extension will get a `light` field with value containing a light definition object with properties defining the light (this object will be resolved by index from the global `KHR_lights_punctual` extension object's `lights` array) .\n- The `KHR_lights_punctual` extensions will be removed from all nodes.\n- Finally, the global `KHR_lights_punctual` extension (including its light list)) will be removed.\n\nEncoding Support:\n\n- N/A\n\n## Custom Extensions\n\n### UBER_draco_point_cloud_compression\n\nSpecification: Similar to `KHR_draco_mesh_compression`, but supports point clouds (draw mode 0). Also does not support any fallback or non-compressed accessors/attributes.\n\nParsing support:\n\n- The primitive's accessors field will be populated after decompression.\n- After decompression, the extension will be removed (as if the point cloud was never compressed).\n\nEncoding support:\n\n- Point clouds can be compressed as they are added to the `GLTFBuilder` and decompressed by the `GLTFParser`.\n","slug":"docs/api-reference/gltf-loaders/gltf-extensions","title":"glTF Extensions"},{"excerpt":"GLTFLoadersParses a glTF file into a hierarchical scenegraph description that can be used to instantiate an actual Scenegraph in most WebGL…","rawMarkdownBody":"# GLTFLoaders\n\nParses a glTF file into a hierarchical scenegraph description that can be used to instantiate an actual Scenegraph in most WebGL libraries. Can load both binary `.glb` files and JSON `.gltf` files.\n\n| Loader                | Characteristic                                                             |\n| --------------------- | -------------------------------------------------------------------------- |\n| File Extensions       | `.glb`,`.gltf`                                                             |\n| File Types            | Binary/JSON/Linked Assets                                                  |\n| File Format           | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Format Category       | glTF Scenegraph                                                            |\n| Parser Type           | Asynchronous (Synchronous w/ limited functionality)                        |\n| Worker Thread Support | No                                                                         |\n| Streaming Support     | No                                                                         |\n\n## Usage\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLTFLoader);\n```\n\nTo decompress Draco compressed meshes:\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nimport {DracoLoader} from '@loaders.gl/draco';\nconst gltf = load(url, GLTFLoader, {DracoLoader, decompress: true});\n```\n\n## Options\n\n- `DracoWriter` - supply this to enable decoding of Draco compressed meshes. `import {DracoWriter} from '@loaders.gl/draco'`\n\n## Options\n\n| Option                 | Default     | Description                                                                                      |\n| ---------------------- | ----------- | ------------------------------------------------------------------------------------------------ |\n| `fetchLinkedResources` | `true`      | Fetch any linked .BIN files, decode base64 encoded URIS. Only supported in asynchronous parsing. |\n| `fetch`                | `fetchFile` | Function used to fetch linked resources                                                          |\n| `decompress`           | `true`      | Decompress Draco compressed meshes (if DracoLoader available)                                    |\n| `DracoLoader`          | `null`      | Supply to enable decoding of Draco compressed meshes.                                            |\n| `postProcess`          | `false`     | Perform additional post processing to simplify use in WebGL libraries                            |\n| `createImages`         | `false`     | Create image objects from loaded image data                                                      |\n\n## Structure of Loaded Data\n\nReturns a JSON object with \"embedded\" binary data in the form of typed javascript arrays.\n\nWhen parsed asynchronously (not using `loadSync` or `parseSync`):\n\n- linked binary resources will be loaded and resolved (if url is available).\n- base64 encoded binary data inside the JSON payload will be decoded\n\n## Attributions\n\nThe `GLTFLoader` was developed for loaders.gl.\n","slug":"docs/api-reference/gltf-loaders/gltf-loader","title":"GLTFLoaders"},{"excerpt":"GLTFParserThe  class parses GLB-encoded glTF files. glTF files can contain complex scenegraphs with extensions and application defined data…","rawMarkdownBody":"# GLTFParser\n\nThe `GLTFParser` class parses GLB-encoded glTF files. glTF files can contain complex scenegraphs with extensions and application defined data annotations.\n\nTo facilitiate working with the loaded data, the `GLTFParser` class provides:\n\n- A set of accessor methods to facilitate traversal the parsed glTF data.\n- A `resolveScenegraphs` method that resolves the index based linking between objects into a hierarchical javascript structure.\n\nCertain glTF extensions can be fully or partially processed by the `GLTFParser`. See [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions.md).\n\nReferences:\n\n- For more information, see [glTF and GLB support](docs/) in the Developer's Guide.\n\n## Usage\n\n```js\nimport {GLTFParser} from '@loaders.gl/gltf';\nimport {load} from '@loaders.gl/core';\n\n// Create a parser\nconst gltfParser = new GLTFParser();\n\n// Load and parse a file\nconst GLTF_BINARY = await load(...);\ngltfParser.parseSync(GLTF_BINARY);\n\n// Get the complete glTF JSON structure\nconst gltfJson = gltfParser.getJSON();\n\n// Get specific top-level fields from the glTF JSON chunk\nconst appData = gltfParser.getApplicationData('customData');\n\n// Get a top level extension from the glTF JSON chunk\nconst topLevelExtension = gltfParser.getExtension('ORGNAME_extensionName');\nif (topLevelExtension) {\n  ...\n}\n\n// Get images from the binary chunk (together with metadata)\nconst imageIndex = 0;\nconst image = gltfParser.getImage(imageIndex);\n\n// Get default glTF scenegraph\nconst scenegraph = gltfParser.getScenegraph();\n// Get specific glTF scenegraph\nconst scenegraph = gltfParser.getScenegraph(2);\n```\n\n## Structure of Loaded Data\n\nThe structure of the parsed data is as described in the glTF specification, with binary buffers and images resolved into typed arrays.\n\nCalling the `resolveScenegraphs` method adds a hierarchical structure that makes the scenegraph easier to traverse. After the call, each parent object in the scenegraph will contain an array of child objects, instead of a list of child indices.\n\n## Methods\n\n### constructor()\n\nCreates a new `GLTFParser` instance.\n\n### async parse(arrayBuffer : ArrayBuffer, options : Object) : Promise<Object>\n\nParses an in-memory, glTF/GLB formatted `ArrayBuffer` a JSON tree with binary typed arrays and image nodes. Once the `Promise` returned by the `parse()` method has successfully resolved, the accessors in this class can be used.\n\n- `options.url`= - Supplies a base URL that is used to resolve relative file names to linked resources. Only needed when loading glTF files that have linked resources (e.g. typically not the case for `.glb` encoded files).\n- `options.decompress`=`false` - Decompresses any Draco encoded meshes\n- `options.DracoDecoder` - To enable Draco encoding, the application also needs to import and supply the `DracoEncoder` class.\n\nNotes:\n\n- linked binary resources will be loaded and resolved (if url is available).\n- base64 encoded binary data inside the JSON payload will be decoded\n\n### parseSync(arrayBuffer : ArrayBuffer, options : Object) : Object\n\nParses an in-memory, glTF/GLB formatted `ArrayBuffer` a JSON tree with binary typed arrays and image nodes.\n\nOnce the `parseSync()` method has successfully completed the accessors in this class can be used.\n\nNotes:\n\n- **Synchronous parsing does not handle linked resources**\n\n### resolveScenegraphs() : Object\n\nThe `resolveScenegraphs` method resolves the index based linking between objects into a hierarchical javascript structure, making scenegraph traversal simpler.\n\n### getApplicationData(key : String) : Object\n\nReturns the given data field in the top-level glTF JSON object.\n\n### getExtraData(key : String) : Object?\n\nReturns a key in the top-level glTF `extras` JSON object.\n\n### getExtension(name : String) : Object?\n\nReturns the top-level extension by `name`, if present.\n\n### getUsedExtensionNames() : String[]\n\nReturns an array of extension names (covering all extensions used at any level of the glTF hierarchy).\n\n### getRequiredExtensionNames() : String[]\n\nReturns an array of extensions at any level of the glTF hierarchy that are required to properly display this file (covering all extensions used at any level of the glTF hierarchy).\n\n### getScenegraph([index : Number]) : Object?\n\nReturns the Scenegraph with the given index, or the default scenegraph if no index is specified.\n\n### getImage(index : Number) : Object\n\nReturns the image with specified index\n","slug":"docs/api-reference/gltf-loaders/gltf-parser","title":"GLTFParser"},{"excerpt":"GLTFWriterThe  is a writer for glTF scenegraphs.WriterCharacteristicFile Extensions,File TypesBinary/JSON/Linked AssetsFile FormatglTFFormat…","rawMarkdownBody":"# GLTFWriter\n\nThe `GLTFWriter` is a writer for glTF scenegraphs.\n\n| Writer                | Characteristic                                                             |\n| --------------------- | -------------------------------------------------------------------------- |\n| File Extensions       | `.glb`,`.gltf`                                                             |\n| File Types            | Binary/JSON/Linked Assets                                                  |\n| File Format           | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Format Category       | glTF Scenegraph                                                            |\n| Writer Type           | Asynchronous (Synchronous w/ limited functionality)                        |\n| Worker Thread Support | No                                                                         |\n| Streaming Support     | No                                                                         |\n\n## Usage\n\n```js\nimport {GLTFWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLTFWriter, options);\n```\n\n## Options\n\n- `packTypedArrays` - Packs typed arrays\n- `DracoWriter` - To enable DRACO encoding, the application needs to import and supply the `DracoWriter` class.\n- `DracoLoader` - To enable DRACO encoding, the application needs to import and supply the `DracoLoader` class.\n\n## Attributions\n\nThe `GLTFWriter` was developed for loaders.gl.\n","slug":"docs/api-reference/gltf-loaders/gltf-writer","title":"GLTFWriter"},{"excerpt":"Binary Utilitiesloaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal…","rawMarkdownBody":"# Binary Utilities\n\nloaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal with binary data in the JavaScript APIs for browser and Node.js, and some small but annoying \"gotchas\" that can trip up programmers when working with binary data.\n\n## Usage\n\n```js\nimport {toArrayBuffer} from '@loaders.gl/core';\n```\n\n## Functions\n\n### toArrayBuffer(binaryData : \\*) : ArrayBuffer\n\n\"Repackages\" a binary data in non-array-buffer form as an `ArrayBuffer`.\n\n- binaryData - ArrayBuffer, Buffer (Node.js), typed array, blob, ...\n\n## Remarks\n\n- Most functions in loaders.gl that accept binary data call `toArrayBuffer(...)` on input parameters before starting processing, thus ensuring that functions work on all types of input data.\n","slug":"docs/api-reference/core/binary-utilities","title":"Binary Utilities"},{"excerpt":"File UtilitiesSmall optional file reading utilities that work consistently across browser (both main thread and worker threads) as well as…","rawMarkdownBody":"# File Utilities\n\nSmall optional file reading utilities that work consistently across browser (both main thread and worker threads) as well as under Node. The main function is `fetchFile` which can be used as a slightly more featured/portable version of fetch.\n\n## Usage\n\nUse the `fetchFile` function as follows:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\ndata = await parse(fetchFile(url), OBJLoader);\n// Application code here\n...\n```\n\nNote that if you don't care about Node.js compatibility, you can just use the browser's built-in `fetch` directly.\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\ndata = await parse(fetch(url), OBJLoader);\n// Application code here\n...\n```\n\n## Functions\n\n### fetchFile(url : String [, options : Object]) : Promise<Response>\n\nA version of [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Response) that:\n\n- can be used in both the browser and Node.js.\n- Supports `setPathPrefix`: If path prefix has been set, it will be appended if `url` is relative (e.g. does not start with a `/`).\n\nReturns:\n\n- A promise that resolves into a fetch `Response` object, with the following methods/fields:\n  - `arrayBuffer() : Promise<ArrayBuffer>` - Loads the file as an `ArrayBuffer`.\n  - `text() : Promise<String>` - Loads the file and decodes it into text.\n  - `body : ReadableStream` - A stream that can be used to incrementally read the contents of the file.\n\nOptions:\n\nUnder Node.js, options include (see [fs.createReadStream](https://nodejs.org/api/fs.html#fs_fs_createreadstream_path_options)):\n\n- `options.highWaterMark` (Number) Default: 64K (64 \\* 1024) - Determines the \"chunk size\" of data read from the file.\n\nRemarks:\n\n- In the browser `fetchFile` will delegate to fetch after resolving the URL.\n- In node.js a mock `Response` object will be returned.\n\n### readFileSync(url : String [, options : Object]) : ArrayBuffer | String\n\n> This function only works on Node.js or using data URLs.\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- `fetchFile` is intended to be a small (in terms of bundle size) function to help applications work with files in a portable way. The `Response` object returned on Node.js does not implement all the functionality the browser does. If you run into the need\n- In fact, the use of any of the file utilities including `readFile` and `readFileAsync` functions with other loaders.gl functions is entirely optional. loader objects can be used with data loaded via any mechanism the application prefers, e.g. directly using `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n- The stream utilities are intended to be small optional helpers that facilitate writing platform independent code that works with streams. This can be valuable as JavaScript Stream APIs are still maturing and there are still significant differences between platforms. However, streams and iterators created directly using platform specific APIs can be used as parameters to loaders.gl functions whenever a stream is expected, allowing the application to take full control when desired.\n","slug":"docs/api-reference/core/fetch-file","title":"File Utilities"},{"excerpt":"encodeFunctionsencode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object , url : String]) : PromiseEncodes data…","rawMarkdownBody":"# encode\n\n## Functions\n\n### encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object [, url : String]]) : Promise<Any>\n\nEncodes data asynchronously using the provided writer.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n\n- `options.log`=`console` Any object with methods `log`, `info`, `warn` and `error`. By default set to `console`. Setting log to `null` will turn off logging.\n\n### encodeSync(fileData : ArrayBuffer | String, writer : Object | Array, [, options : Object [, url : String]]) : any\n\nEncodes data synchronously using the provided writer, if possible. If not, returns `null`, in which case asynchronous loading is required.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n","slug":"docs/api-reference/core/encode","title":"encode"},{"excerpt":"Iterator UtilsFunctionsgetStreamIterator(stream : Stream) : AsyncIteratorReturns an async iterator that can be used to read chunks of data…","rawMarkdownBody":"# Iterator Utils\n\n## Functions\n\n### getStreamIterator(stream : Stream) : AsyncIterator\n\nReturns an async iterator that can be used to read chunks of data from the stream (or write chunks of data to the stream, in case of writable streams).\n\nWorks on both Node.js 8+ and browser streams.\n","slug":"docs/api-reference/core/iterator-utilities","title":"Iterator Utils"},{"excerpt":"loadThe  function can be used with any loader object. They takes a  and one or more loader objects, checks what type of data that loader…","rawMarkdownBody":"# load\n\nThe `load` function can be used with any _loader object_. They takes a `url` and one or more _loader objects_, checks what type of data that loader prefers to work on (e.g. text, JSON, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n\n### load(url : String | File, loaders : Object | Object[][, options : object]) : Promise<Response>\n\n### load(url : String | File [, options : Object]) : Promise<Response>\n\nThe `load` function is used to load and parse data with a specific _loader object_. An array of loader objects can be provided, in which case `load` will attempt to autodetect which loader is appropriate for the file.\n\nThe `loaders` parameter can also be omitted, in which case any _loader objects_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `url` - Can be a string, either a data url or a request url, or in Node.js, a file name, or in the browser, a File object. Or any format could ba accepted by [`parse`](https://github.com/uber-web/loaders.gl/blob/master/docs/api-reference/core/parse.md#parsedata--arraybuffer--string--options--object--url--string--promise). If `url` is not a `string`, will call `parse` directly.\n- `data` - loaded data, either in binary or text format.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options` - optional, contains both options for the read process and options for the loader (see documentation of the specific loader).\n- `options.dataType`=`arraybuffer` - By default reads as binary. Set to 'text' to read as text.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n- `load` takes a `url` and a loader object, checks what type of data that loader prefers to work on (e.g. text, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n","slug":"docs/api-reference/core/load","title":"load"},{"excerpt":"parseThis function parses already loaded data. As a special case, it can also load (and then parse) data from a  or  response object…","rawMarkdownBody":"# parse\n\nThis function parses already loaded data. As a special case, it can also load (and then parse) data from a `fetch` or `fetchFile` response object).\n\n## Usage\n\nThe return value from `fetch` or `fetchFile` is a `Promise` that resolves to the fetch response object and can be passed directly to the non-sync parser functions:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\ndata = await parse(fetchFile(url), OBJLoader);\n// Application code here\n...\n```\n\nBatched (streaming) parsing is supported by some loaders\n\n```js\nimport {fetchFile, parseInBatches} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/obj';\n\nconst batchIterator = await parseInBatches(fetchFile(url), CSVLoader);\nfor await (const batch of batchIterator) {\n  console.log(batch.length);\n}\n```\n\n## Functions\n\n### parse(data : ArrayBuffer | String, loaders : Object | Object\\[] [, options : Object [, url : String]]) : Promise<Any>\n\n### parse(data : ArrayBuffer | String, [, options : Object [, url : String]]) : Promise<Any>\n\nParses data asynchronously using the provided loader.\nUsed to parse data with a selected _loader object_. An array of `loaders` can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\nThe `loaders` parameter can also be omitted, in which case any _loader objects_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n  - `Response` - `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\n- `options.log`=`console` Any object with methods `log`, `info`, `warn` and `error`. By default set to `console`. Setting log to `null` will turn off logging.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\n### parseSync(fileData : ArrayBuffer | String, loaders : Object | Object\\[], [, options : Object [, url : String]]) : any\n\n### parseSync(fileData : ArrayBuffer | String, [, options : Object [, url : String]]) : any\n\n> Synchronous parsing is not supported by all _loader objects_\n\nParses data synchronously using the provided loader, if possible. If not, returns `null`, in which case asynchronous parsing is required.\n\n- `data`: already loaded data, either in binary or text format. This parameter can be any of the following types:\n  - `Response`: `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer`: Parse from binary data in an array buffer\n  - `String`: Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator`: Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n    can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\n### parseInBatches(data : any, loaders : Object | Object\\[] [, options : Object [, url : String]]) : AsyncIterator\n\n### parseInBatches(data : any [, options : Object [, url : String]]) : AsyncIterator\n\n> Batched loading is not supported by all _loader objects_\n\nParses data in batches from a stream, releasing each batch to the application while the stream is still being read.\n\nParses data with the selected _loader object_. An array of `loaders` can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\nThe `loaders` parameter can also be ommitted, in which case any _loader objects_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n  - `Response` - `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Returns an async iterator that yields batches of data. The exact format for the batches depends on the _loader object_ category.\n","slug":"docs/api-reference/core/parse","title":"parse"},{"excerpt":"Loader RegistryThe loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing just…","rawMarkdownBody":"# Loader Registry\n\nThe loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing just the loaders they need and registering them during initialization.\n\nApplications can then make all those imported loaders available (via format autodetection) to all subsequent `parse` and `load` calls, without those calls having to specify which loaders to use.\n\n## Usage\n\nInitialization imports and registers loaders:\n\n```js\nimport {registerLoaders} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders(CSVLoader);\n```\n\nSome other file that needs to load CSV:\n\n```js\nimport {load} from '@loaders.gl/core';\n\n// The pre-registered CSVLoader gets auto selected based on file extension...\nconst data = await load('data.csv');\n```\n\n## Functions\n\n### registerLoaders(loaders : Object | Object[])\n\nRegisters one or more _loader objects_ to a global _loader object registry_, these loaders will be used if no loader object is supplied to `parse` and `load`.\n\n- `loaders` - can be a single loader or an array of loaders. The specified loaders will be added to any previously registered loaders.\n","slug":"docs/api-reference/core/register-loaders","title":"Loader Registry"},{"excerpt":"saveNeeds update and  function can be used with any writer.  takes a  and a writer object, checks what type of data that writer prefers to…","rawMarkdownBody":"# save\n\n> Needs update\n\n`save` and `saveSync` function can be used with any writer. `save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n## Functions\n\n### save(url : String | File, writer : Object [, options : Object]) : Promise<ArrayBuffer | String>\n\nThe `save` function can be used with any writer.\n\n`save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n- `url` - Can be a string, either a data url or a request url, or in Node.js, a file name, or in the browser, a File object.\n- `data` - saveed data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, contains both options for the read process and options for the writer (see documentation of the specific writer).\n- `options.dataType`=`arraybuffer` - By default reads as binary. Set to 'text' to read as text.\n\nReturns:\n\n- Return value depends on the category\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### saveSync(url : String [, options : Object]) : ArrayBuffer | String\n\nSimilar to `save` except saves and parses data synchronously.\n\nNote that for `saveSync` to work, the `url` needs to be saveable synchronously _and_ the writer used must support synchronous parsing. Synchronous saveing only works on data URLs or files in Node.js. In many cases, the asynchronous `save` is more appropriate.\n","slug":"docs/api-reference/core/save","title":"save"},{"excerpt":"setPathPrefixresolvePath(path : String) : StringApplies aliases and path prefix, in that order. Returns an updated path.addAliases(aliases…","rawMarkdownBody":"# setPathPrefix\n\n### resolvePath(path : String) : String\n\nApplies aliases and path prefix, in that order. Returns an updated path.\n\n### addAliases(aliases : Object)\n\nSets a map of aliases (file name substitutions).\n\n### setPathPrefix(prefix : String)\n\nThis sets a path prefix that is automatically prepended to relative path names provided to load functions.\n\n### getPathPrefix() : String\n\nReturns the current path prefix set by `setPathPrefix`.\n","slug":"docs/api-reference/core/set-path-prefix","title":"setPathPrefix"},{"excerpt":"Text Utilities and  polyfills are provided to ensure these APIs are always available. In modern browsers these will evaluate to the built-in…","rawMarkdownBody":"# Text Utilities\n\n`TextEncoder` and `TextDecoder` polyfills are provided to ensure these APIs are always available. In modern browsers these will evaluate to the built-in objects of the same name, however under Node.js polyfills are transparently installed.\n\n## Usage\n\n```js\nimport {TextEncoder, TextDecoder} from '@loaders.gl/core';\n```\n\n## Remarks\n\n- Refer to browser documentation for the usage of these classes, e.g. MDN.\n- In the browser, overhead of using these imports is very low, as they refer to built-in classes.\n- If working under older browsers, e.g. IE11, you may need to install your own TextEncoder/TextDecoder polyfills before loading this library (to be confirmed...)\n","slug":"docs/api-reference/core/text-utilities","title":"Text Utilities"},{"excerpt":"File UtilitiesNeeds updateA set of file load and save utilities that (attempt to) work consistently across browser and node…","rawMarkdownBody":"# File Utilities\n\n> Needs update\n\nA set of file load and save utilities that (attempt to) work consistently across browser and node.\n\n## Usage\n\n```js\nimport {writeFile} from '@loaders.gl/core';\nimport {DracoWriter} from '@loaders.gl/draco';\n\nawait writeFile(url, DracoWriter);\n```\n\n## Functions\n\n### writeFile(url : String [, options : Object]) : Promise<ArrayBuffer>\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### writeFileSync(url : String [, options : Object]) : ArrayBuffer\n\n> Only works on Node.js or using data URLs.\n\nReads the raw data from a \"file\" synchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- The use of the loaders.gl `writeFile` and `writeFileAsync` functions is optional, loaders.gl loaders can be used with any data loaded via any mechanism the application prefers, e.g. `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n","slug":"docs/api-reference/core/write-file","title":"File Utilities"},{"excerpt":"ImageLoaderAn image loader that works under both Node.js and the browser.RemarksWhile generic, the  is designed with WebGL applications in…","rawMarkdownBody":"# ImageLoader\n\nAn image loader that works under both Node.js and the browser.\n\n- `image`\n- `options.mimeType`\n\nRemarks\n\n- While generic, the `ImageLoader` is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a `WebGLTexture` both in the browser and in headless gl under Node.js\n","slug":"docs/api-reference/image-loaders/image-loader","title":"ImageLoader"},{"excerpt":"Image Data UtilsFunctionsisImage(imageData : ArrayBuffer)Returns  if the binary data represents a well-known binary image format…","rawMarkdownBody":"# Image Data Utils\n\n## Functions\n\n### isImage(imageData : ArrayBuffer)\n\nReturns `true` if the binary data represents a well-known binary image format.\n\n### getImageSize(imageData : ArrayBuffer [, mimeType : String])\n\nReturns an object with the size and mimeType of the image represented by the data.\n","slug":"docs/api-reference/image-loaders/image-utilities","title":"Image Data Utils"},{"excerpt":"loadImageFunctionsloadImage(url : String , options : Object) : Image / HTMLImageElementThis is a minimal basic image loading function that…","rawMarkdownBody":"# loadImage\n\n## Functions\n\n### loadImage(url : String [, options : Object]) : Image / HTMLImageElement\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/browser-only-red.svg?style=flat-square\" alt=\"browser only\" />\n</p>\n\nThis is a minimal basic image loading function that only works in the browser main threaqd. For image loading and writing that works across both browser and node, refer to the `@loaders.gl/images` module.\n\n`options.crossOrigin` - Provides control of the requests cross origin field.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n","slug":"docs/api-reference/image-loaders/load-image","title":"loadImage"},{"excerpt":"ImageWriterThe  class can encode an image into  both under browser and Node.jsencodeImage(image : any, options : Object) - This can be an…","rawMarkdownBody":"# ImageWriter\n\nThe `ImageWriter` class can encode an image into `ArrayBuffer` both under browser and Node.js\n\n# encodeImage(image : any, options : Object)\n\n- `image` - This can be an HTML image, a Canvas, or a ...\n- `options.mimeType`\n\n## MIME types\n\nSupported image types (MIME types) depends on the environment. Typically PNG and JPG are supported.\n","slug":"docs/api-reference/image-loaders/image-writer","title":"ImageWriter"},{"excerpt":"DracoLoaderDecodes a mesh or point cloud (maps of attributes) using DRACO compression compression.LoaderCharacteristicFile ExtensionFile…","rawMarkdownBody":"# DracoLoader\n\nDecodes a mesh or point cloud (maps of attributes) using [DRACO compression](https://google.github.io/draco/) compression.\n\n| Loader                | Characteristic                                                        |\n| --------------------- | --------------------------------------------------------------------- |\n| File Extension        | `.drc`                                                                |\n| File Type             | Binary                                                                |\n| File Format           | [Draco](https://google.github.io/draco/)                              |\n| Parser Category       | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md) |\n| Parser Type           | Synchronous                                                           |\n| Worker Thread Support | Yes                                                                   |\n| Streaming Support     | No                                                                    |\n\n## Usage\n\n```js\nimport {DracoLoader} from '@loaders.gl/draco';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, DracoLoader, options);\n```\n\n## Options\n\nN/A\n\n## Data Format\n\n`DracoLoader` loads a single primitive geometry for a point cloud or mesh and the return data follows the conventions for those categories.\n\n## Attribution/Credits\n\nBased on Draco examples\n","slug":"docs/api-reference/mesh-loaders/draco-loader","title":"DracoLoader"},{"excerpt":"DracoWriterEncodes a mesh or point cloud (maps of attributes) using Draco3D compression.LoaderCharacteristicFile ExtensionFile…","rawMarkdownBody":"# DracoWriter\n\nEncodes a mesh or point cloud (maps of attributes) using [Draco3D](https://google.github.io/draco/) compression.\n\n| Loader                | Characteristic                                                        |\n| --------------------- | --------------------------------------------------------------------- |\n| File Extension        | `.drc`                                                                |\n| File Typoe            | Binary                                                                |\n| File Format           | [Draco](https://google.github.io/draco/)                              |\n| Data Format           | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md) |\n| Encoder Type          | Synchronous                                                           |\n| Worker Thread Support | Yes                                                                   |\n| Streaming Support     | No                                                                    |\n\n## Usage\n\n```js\nimport {DracoWriter} from '@loaders.gl/draco';\nimport {encode} from '@loaders.gl/core';\n\nconst mesh = {\n  attributes: {\n    POSITION: {...}\n  }\n};\n\nconst data = await encode(mesh, DracoWriter, options);\n```\n\n## Options\n\n- `pointcloud`=`false` (Boolean): Set to `true` to compress pointclouds (mode=`0` and no `indices`)/\n- `method`=`MESH_EDGEBREAKER_ENCODING` (String) - set Draco encoding method (applies to meshes only)\n- `speed`=`[5, 5]` ([Number, Number] - set Draco speed options.\n- `quantization`=`{POSITION: 10}` (Object) - set Draco attribute quantization. Lower numbers means higher compression but more information loss.\n- `log`= (Function) - callback for debug info.\n\n## Input Data\n\nAccepts a standardized mesh.\n\n## Attribution/Credits\n\nBased on Draco examples\n","slug":"docs/api-reference/mesh-loaders/draco-writer","title":"DracoWriter"},{"excerpt":"LASLoaderThe LASER (LAS) file format is a public format for the interchange of 3-dimensional point cloud data data, developed for LIDAR…","rawMarkdownBody":"# LASLoader\n\nThe LASER (LAS) file format is a public format for the interchange of 3-dimensional point cloud data data, developed for LIDAR mapping purposes.\n\n| Loader                | Characteristic                                                                                                           |\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n| File Extension        | `.las`                                                                                                                   | `.laz` |\n| File Type             | Binary                                                                                                                   |\n| File Format           | [LASER FILE FORMAT](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) |\n| Data Format           | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md)                                                    |\n| Encoder Type          | Synchronous                                                                                                              |\n| Worker Thread Support | Yes                                                                                                                      |\n| Streaming Support     | No                                                                                                                       |\n\nNote: LAZ is the compressed version of LAS\n\n## Usage\n\n```js\nimport {LASLoader} from '@loaders.gl/las';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, LASLoader, options);\n```\n\n## Options\n\n- `skip`=`1` (Number) - Read one from every _n_ points. Default `1`.\n- `onProgress`= (Number) - Callback when a new chunk of data is read.\n\n## Attribution/Credits\n\nLASLoader is a fork of Uday Verma and Howard Butler's [plasio](https://github.com/verma/plasio/) under MIT License.\n","slug":"docs/api-reference/mesh-loaders/las-loader","title":"LASLoader"},{"excerpt":"OBJLoaderThis loader handles the OBJ half of the classic Wavefront OBJ/MTL format. The OBJ format is a simple ASCII format that lists…","rawMarkdownBody":"# OBJLoader\n\nThis loader handles the OBJ half of the classic Wavefront OBJ/MTL format. The OBJ format is a simple ASCII format that lists vertices, normals and faces on successive lines.\n\n| Loader                | Characteristic                                                          |\n| --------------------- | ----------------------------------------------------------------------- |\n| File Extension        | `.obj`                                                                  |\n| File Type             | Text                                                                    |\n| File Format           | [Wavefront OBJ file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) |\n| Data Format           | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md)   |\n| Encoder Type          | Synchronous                                                             |\n| Worker Thread Support | Yes                                                                     |\n| Streaming Support     | No                                                                      |\n\n## Usage\n\n```js\nimport {OBJLoader} from '@loaders.gl/obj';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, OBJLoader);\n```\n\n## Loader Options\n\nN/A\n\n## Data Loaded\n\n- `positions` -\n- `normals` -\n- `faces` -\n\n## Attribution/Credits\n\nOBJLoader is a wrapper around the [`webgl-obj-loader`](https://www.npmjs.com/package/webgl-obj-loader) module.\n","slug":"docs/api-reference/mesh-loaders/obj-loader","title":"OBJLoader"},{"excerpt":"PCDLoaderA point cloud format defined by the Point Cloud Library.LoaderCharacteristicFile ExtensionFile TypeText/BinaryFile FormatPoint…","rawMarkdownBody":"# PCDLoader\n\nA point cloud format defined by the [Point Cloud Library](https://en.wikipedia.org/wiki/Point_Cloud_Library).\n\n| Loader                | Characteristic                                                                            |\n| --------------------- | ----------------------------------------------------------------------------------------- |\n| File Extension        | `.pcd`                                                                                    |\n| File Type             | Text/Binary                                                                               |\n| File Format           | [Point Cloud Library](http://pointclouds.org/documentation/tutorials/pcd_file_format.php) |\n| Data Format           | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md)                     |\n| Encoder Type          | Synchronous                                                                               |\n| Worker Thread Support | Yes                                                                                       |\n| Streaming Support     | No                                                                                        |\n\nNote: Currently only `ascii` and `binary` subformats are supported. Compressed binary files are currently not supported.\n\n## Usage\n\n```js\nimport {PCDLoader} from '@loaders.gl/pcd';\nimport {load} from '@loaders.gl/core';\n\nconst {header, attributes} = await load(url, PCDLoader);\n// Application code here, e.g:\n// return new Geometry(attributes)\n```\n\nLoads `position`, `normal`, `color` attributes.\n\n## Attribution/Credits\n\nPCDLoader loader is a fork of the THREE.js PCDLoader under MIT License. The THREE.js source files contained the following attributions:\n\n- @author Filipe Caixeta / http://filipecaixeta.com.br\n- @author Mugen87 / https://github.com/Mugen87\n","slug":"docs/api-reference/mesh-loaders/pcd-loader","title":"PCDLoader"},{"excerpt":"PLYLoaderPLY is a computer file format known as the Polygon File Format or the Stanford Triangle Format. It was principally designed to…","rawMarkdownBody":"# PLYLoader\n\nPLY is a computer file format known as the Polygon File Format or the Stanford Triangle Format. It was principally designed to store three-dimensional data from 3D scanners.\n\n| Loader                | Characteristic                                                        |\n| --------------------- | --------------------------------------------------------------------- |\n| File Extension        | `.ply`                                                                |\n| File Type             | Binary/Text                                                           |\n| File Format           | [PLY format](<https://en.wikipedia.org/wiki/PLY_(file_format)>)       |\n| Data Format           | [Standardized Mesh](docs/api-reference/mesh-loaders/category-mesh.md) |\n| Encoder Type          | Synchronous                                                           |\n| Worker Thread Support | Yes                                                                   |\n| Streaming Support     | No                                                                    |\n\n## Usage\n\n```js\nimport {PLYLoader} from '@loaders.gl/ply';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, PLYLoader);\n```\n\n## Attribution/Credits\n\nPLYLoader is a fork of the THREE.js PLYLoader under MIT License. The THREE.js source files contained the following attributions:\n\n@author Wei Meng / http://about.me/menway\n","slug":"docs/api-reference/mesh-loaders/ply-loader","title":"PLYLoader"},{"excerpt":"ZipLoaderDecodes a Zip Archive into a file map.LoaderCharacteristicFile ExtensionFile FormatBinaryCategoryArchiveData\"File Map\"Parser…","rawMarkdownBody":"# ZipLoader\n\nDecodes a Zip Archive into a file map.\n\n| Loader         | Characteristic |\n| -------------- | -------------- |\n| File Extension | `.zip`         |\n| File Format    | Binary         |\n| Category       | Archive        |\n| Data           | \"File Map\"     |\n| Parser Type    | Asynchronous   |\n| Worker Thread  | No             |\n| Streaming      | No             |\n\n## Usage\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {ZipLoader} from '@loaders.gl/zip';\n\nconst fileMap = await parse(arrayBuffer, ZipLoader);\nfor (const fileName in FILE_MAP) {\n  const fileData = fileMap[key];\n  // Do something with the subfile\n}\n```\n\n## Input\n\n`ArrayBuffer` containing a valid Zip Archive\n\n## Output\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.loadAsync](https://stuk.github.io/jszip/documentation/api_jszip/load_async.html).\n\n## Attributions\n\nZipLoader is a wrapper around the [JSZip module](https://stuk.github.io/jszip/). JSZip has extensive documentation on options (and more functionality than this loader object can expose).\n","slug":"docs/api-reference/misc-loaders/zip-loader","title":"ZipLoader"},{"excerpt":"ZipWriterEncodes a filemap into a Zip Archive. Returns an  that is a valid Zip Archive and can be written to file.LoaderCharacteristicFile…","rawMarkdownBody":"# ZipWriter\n\nEncodes a filemap into a Zip Archive. Returns an `ArrayBuffer` that is a valid Zip Archive and can be written to file.\n\n| Loader         | Characteristic                                                  |\n| -------------- | --------------------------------------------------------------- |\n| File Extension | `.zip`                                                          |\n| File Type      | Binary                                                          |\n| File Format    | [Zip Format](<https://en.wikipedia.org/wiki/Zip_(file_format)>) |\n| Category       | Archive                                                         |\n| Data           | \"File Map\"                                                      |\n| Parser Type    | Asynchronous                                                    |\n| Worker Thread  | No                                                              |\n| Streaming      | No                                                              |\n\n## Usage\n\n```js\nimport {encode, writeFile} from '@loaders.gl/core';\nimport {ZipWriter} from '@loaders.gl/zip';\n\nconst FILEMAP = {\n  filename1: arrayBuffer1,\n  'directory/filename2': ...\n};\n\nconst arrayBuffer = await encode(FILE_MAP, ZipWriter)\nwriteFile(zipFileName, arrayBuffer);\n```\n\n## Input\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.generateAsync](https://stuk.github.io/jszip/documentation/api_jszip/generate_async.html), however type is always set to `arraybuffer` to ensure compatibility with writer driver functions in `@loaders.gl/core`.\n\n## Attributions\n\nZipWriter is a wrapper around the [JSZip module](https://stuk.github.io/jszip/). JSZip has extensive documentation on options (and more functionality than this writer object can expose).\n","slug":"docs/api-reference/misc-loaders/zip-writer","title":"ZipWriter"},{"excerpt":"ArrowLoader (Experimental)A simple non-streaming loader for the Apache Arrow columnar table format.LoaderCharacteristicFile ExtensionFile…","rawMarkdownBody":"# ArrowLoader (Experimental)\n\nA simple non-streaming loader for the Apache Arrow columnar table format.\n\n| Loader                | Characteristic                                                            |\n| --------------------- | ------------------------------------------------------------------------- |\n| File Extension        | `.arrow`                                                                  |\n| File Type             | Binary                                                                    |\n| File Format           | [IPC: Encapsulated Message Format](http://arrow.apache.org/docs/ipc.html) |\n| Category              | Columnar Table                                                            |\n| Parser Type           | Synchronous                                                               |\n| Worker Thread Support | Yes                                                                       |\n| Streaming Support     | Yes                                                                       |\n\n## Options\n\nN/A\n\n## Background\n\nApache Arrow is an emerging standard for large in-memory columnar data.\n\n## Attributions\n\nArrowLoader development benefitted from extensive technical advice from Paul Taylor @ Graphistry.\n","slug":"docs/api-reference/table-loaders/arrow-loader","title":"ArrowLoader (Experimental)"},{"excerpt":"Table Category (Experimental)Table Typesloaders.gl deals with (and offers utilities to convert between) three different types of tables…","rawMarkdownBody":"# Table Category (Experimental)\n\n## Table Types\n\nloaders.gl deals with (and offers utilities to convert between) three different types of tables:\n\n### Classic Tables (Row-Major)\n\nThis is the classic JavaScript table that consists of an `Array` of `Object` instances. It would be the natural output of e.g. a JSON loader or a YAML loader.\n\n### Columnar Tables (Column-Major)\n\nColumnar tables are stored as one array per column. Columns that are numeric can be loaded as typed arrays which are stored in contigous memory.\n\nContiguous memory has tremendous benefits:\n\n- Values are adjacent in memory, the resulting cache locality can result in big performance gains\n- Typed arrays can of course be efficiently transferred from worker threads to main thread\n- Can be directly uploaded to the GPU for further processing.\n\n### Chunked Columnar Tables\n\nA problem with columnar tables is that column arrays they can get very long, causing issues with streaming, memory allication etc. A powerful solution is to worked with chunked columnar tables, where columns is are broken into matching sequences of typed arrays.\n\nThe down-side is that complexity can increase quickly and it is best to use helper libraries (such as Apache Arrow) to manage the necessary data structures and book-keeping.\n\n### DataFrames (Arrow)\n\nData Frames are optimized to minimize the amount of copying/moving/reallocation of data during common operations such e.g. loading and transformations, and support zero-cost filtering through smart iterators etc.\n\nUsing the Arrow API it is possible to work extremely efficiently with very large (multi-gigabyte) datasets.\n\n## Table Schemas\n\nFor certain operations, it is useful to have a schema that describes the columns in the table. loaders.gl defines a simple schema object, as follows\n\n## Utilities\n\nA set of utilities is provided.\n\n- `deduceTableSchema(table)` - returns a schema object, autodeduced from columnar or other tables\n-\n","slug":"docs/api-reference/table-loaders/category-table","title":"Table Category (Experimental)"},{"excerpt":"CSVLoader (Experimental)Streaming loader for comma-separated value and delimiter-separated value encoded files.LoaderCharacteristicFile…","rawMarkdownBody":"# CSVLoader (Experimental)\n\nStreaming loader for comma-separated value and [delimiter-separated value](https://en.wikipedia.org/wiki/Delimiter-separated_values) encoded files.\n\n| Loader                | Characteristic                                 |\n| --------------------- | ---------------------------------------------- |\n| File Extension        | `.csv`, `.dsv`                                 |\n| File Type             | Text                                           |\n| File Format           | [RFC4180](https://tools.ietf.org/html/rfc4180) |\n| Category              | Table                                          |\n| Parser Type           | Asynchronous                                   |\n| Worker Thread Support | Yes                                            |\n| Streaming Support     | Yes                                            |\n\n## Options\n\nThe following options are passed on to [papaparse](https://www.papaparse.com/docs#config):\n\n| Option                   | Description                                                                                                                                                                                                                                                                                     |\n| ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `delimiter`=             | The delimiting character. By default auto-detects from a list of common delimiters (or `delimitersToGuess`).                                                                                                                                                                                    |\n| `newline`=               | The newline sequence. By default auto-detects. Must be `\\r`, `\\n`, or `\\r\\n`.                                                                                                                                                                                                                   |\n| `quoteChar`=`\"`          | The character used to quote fields. (Note: unquoted fields are parsed correctly).                                                                                                                                                                                                               |\n| `escapeChar`=`\"`         | The character used to escape the quote character within a field.                                                                                                                                                                                                                                |\n| `dynamicTyping`=`true`   | If true, numeric and boolean data values will be converted to their type (instead if strings).                                                                                                                                                                                                  |\n| `preview`=`0`            | If > 0, only that many rows will be parsed.                                                                                                                                                                                                                                                     |\n| `encoding`=              | The encoding to use when reading files. Defaults to UTF8.                                                                                                                                                                                                                                       |\n| `comments`=`false`       | A string that indicates a comment (for example, \"#\" or \"//\"). When Papa encounters a line starting with this string, it will skip the line.                                                                                                                                                     |\n| `skipEmptyLines`=`false` | If `true`, lines that are completely empty (those which evaluate to an empty string) will be skipped. If set to 'greedy', lines that don't have any content (those which have only whitespace after parsing) will also be skipped.                                                              |\n| `transform`              | A function to apply on each value. The function receives the value as its first argument and the column number or header name when enabled as its second argument. The return value of the function will replace the value it received. The transform function is applied before dynamicTyping. |\n| `delimitersToGuess`=     | An array of delimiters to guess from if the delimiter option is not set. Default is `[',', '\\t', '|', ';', Papa.RECORD_SEP, Papa.UNIT_SEP]`                                                                                                                                                     |\n| `fastMode`=              | Force set \"fast mode\". Fast mode speeds up parsing significantly for large inputs but only works when the input has no quoted fields. Fast mode will be auto enabled if no \" characters appear in the input.                                                                                    |\n\nNotes:\n\nNote that the following `papaparse` options are NOT supported by `CSVLoader` (they are either already used internally or they interfere with the more flexible data loading and parsing model used by `loaders.gl`):\n\n| Option             | Description                                                                  | Reason/Replacement                                                              |\n| ------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n| `header`=`false`   | If true, the first row of parsed data will be interpreted as field names. \\* | Header is detected and parsed by `CSVLoader`                                    |\n| `transformHeader`= | Function to apply on each header.                                            | (Only available in version 5.0)                                                 |\n| `worker`           | Whether to use a worker thread.                                              | Use `CSVWorkerLoader` instead.                                                  |\n| `step`             | Callback function for streaming.                                             | Use `loadInBatches` instead.                                                    |\n| `complete`         | Callback function for streaming.                                             | Use `loadInBatches` instead.                                                    |\n| `error`            | Callback function for error.                                                 | Errors will be handled by `CSVLoader`.                                          |\n| `download`         | First argument is URL from which to download a file.                         | Use external functions to load data (such as `fetch` or `fetchFile`).           |\n| `chunk`            | Callback executed after every chunk is loaded.                               | Use `loadInBatches` instead.                                                    |\n| `beforeFirstChunk` | Callback executed before parsing of first chunk.                             | Use `loadInBatches` instead.                                                    |\n| `withCredentials`  | `XMLHttpRequest.withCredentials` property.                                   | Control credentials using your loading functions (e.g. `fetch` or `fetchFile`). |\n\n## Attributions\n\nCSVLoader is based on a minimal fork of the [papaparse](https://github.com/mholt/PapaParse) module, under MIT license.\n","slug":"docs/api-reference/table-loaders/csv-loader","title":"CSVLoader (Experimental)"},{"excerpt":"PolyfillsOlder browsers (mainly Edge and IE11) as well as versions of Node.js prior to v11 do not provide certain classes that loaders.gl…","rawMarkdownBody":"# Polyfills\n\nOlder browsers (mainly Edge and IE11) as well as versions of Node.js prior to v11 do not provide certain classes that loaders.gl depends on.\n\nWhile there are many good polyfill modules available on `npm`, to make the search for a version that works perfectly with loaders.gl a little easier, a polyfill module is included.\n\n## Usage\n\nJust import `@loaders.gl/polyfills` before you start using other loaders.gl modules.\n\n```js\nimport '@loaders.gl/polyfills';\nimport '@loaders.gl/core';\n```\n\n## Included Polyfills\n\n| Polyfill  | Platforms | Comments |\n| ---       | ---       | ---      |\n| `TextEncoder`/`TextDecoder` | Node.js < 11 and older Browsers | Only UTF8 is guaranteed to be supported |\n| `fetch` | Node.js  | A subset of the fetch API is supported. `Response.body`, `.text()`, `.arrayBuffer()` are supported. |\n\n\n## Remarks\n\nNote: Applications should only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, importing this module will increase the application's bundle size.\n","slug":"docs/api-reference/polyfills","title":"Polyfills"},{"excerpt":"Writer Object SpecificationTo be compatible with  functions such as , writer objects need to conform to the following specification:Common…","rawMarkdownBody":"# Writer Object Specification\n\nTo be compatible with `@loaders.gl/core` functions such as `encode`, writer objects need to conform to the following specification:\n\n### Common Fields\n\n| Field       | Type     | Default  | Description                                                     |\n| ----------- | -------- | -------- | --------------------------------------------------------------- |\n| `name`      | `String` | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension` | `String` | Required | Three letter (typically) extension used by files of this format |\n| `category`  | `String` | Optional | Indicates the type/shape of data                                |\n\n### Encoder Function\n\n| Field                            | Type       | Default | Description                                            |\n| -------------------------------- | ---------- | ------- | ------------------------------------------------------ |\n| `encodeSync`                     | `Function` | `null`  | Encodes synchronously                                  |\n| `encode`                         | `Function` | `null`  | Encodes asynchronously                                 |\n| `encodeInBatches` (Experimental) | `Function` | `null`  | Encodes and releases batches through an async iterator |\n\nNote: The format of the input data to the encoders depends on the loader. Several loader categories are defined to provided standardized data formats for similar loaders.\n","slug":"docs/api-reference/specifications/writer-object-format","title":"Writer Object Specification"},{"excerpt":"Loader Object SpecificationTo be compatible with the parsing/loading functions in  such as  and , a parser needs to be described by a…","rawMarkdownBody":"# Loader Object Specification\n\nTo be compatible with the parsing/loading functions in `@loaders.gl/core` such as `parse` and `load`, a parser needs to be described by a \"loader object\" conforming to the following specification.\n\n## Loader Object Format v1.0\n\n### Common Fields\n\n| Field        | Type       | Default  | Description                                                     |\n| ------------ | ---------- | -------- | --------------------------------------------------------------- |\n| `name`       | `String`   | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension`  | `String`   | Required | Three letter (typically) extension used by files of this format |\n| `extensions` | `String[]` | Required | Array of file extension strings supported by this loader        |\n| `category`   | `String`   | Optional | Indicates the type/shape of data                                |\n\nNote: Only one of `extension` or `extensions` is required. If both are supplied, `extensions` will be used.\n\n### Test Function\n\n| Field      | Type       | Default | Description                                                                       |\n| ---------- | ---------- | ------- | --------------------------------------------------------------------------------- |\n| `testText` | `Function` | `null`  | Guesses if a file is of this format by examining the first characters in the file |\n\n### Parser Function\n\nWhen creating a new loader object, at least one of the parser functions needs to be defined.\n\n| Parser function field               | Type       | Default | Description                                                                            |\n| ----------------------------------- | ---------- | ------- | -------------------------------------------------------------------------------------- |\n| `parseInBatches` (Experimental)     | `Function` | `null`  | Parses binary data chunks (`ArrayBuffer`) to output data \"batches\"                     |\n| `parseInBatchesSync` (Experimental) | `Function` | `null`  | Synchronously parses binary data chunks (`ArrayBuffer`) to output data \"batches\"       |\n| `parseSync`                         | `Function` | `null`  | Atomically and synchronously parses binary data (e.g. file contents) (`ArrayBuffer`)   |\n| `parseTextSync`                     | `Function` | `null`  | Atomically and synchronously parses a text file (`String`)                             |\n| `parse`                             | `Function` | `null`  | Asynchronously parses binary data (e.g. file contents) asynchronously (`ArrayBuffer`). |\n| `loadAndParse`                      | `Function` | `null`  | Asynchronously reads a binary file and parses its contents.                            |\n\nNote: Only one parser function is required. Synchronous parsers are more flexible as they can support synchronous parsing in addition to asynchronous parsing, and iterator-based parsers are more flexible as they can support batched loading in addition to atomic loading. You are encouraged to provide the most capable parser function you can (e.g. `parseSync` or `parseToIterator` if possible). Unless you are writing a completely new loader, the appropriate choice usually depends on the loader you are encapsulating.\n","slug":"docs/api-reference/specifications/loader-object-format","title":"Loader Object Specification"},{"excerpt":"Mesh (PointCloud) Loader CategoryLoaders such as , , ,  etc. all load a \"single geometry primitive\" consisting of a set of \"attributes…","rawMarkdownBody":"## Mesh (PointCloud) Loader Category\n\nLoaders such as `PCD`, `LAZ`, `PLY`, `OBJ` etc. all load a \"single geometry primitive\" consisting of a set of \"attributes\", perhaps `positions`, `colors`, `normals` etc. These attributes are all typed arrays containing successive values for each \"vertex\".\n\nThe mesh loaders do the following to standardize the loaded mesh\n\n- Provide a primitive drawing `mode` (the numeric values matches the corresponding WebGL constants).\n- Unpacks attributes (and indices if present) into typed arrays.\n- Wrap all attributes (and indices if present) into common \"accessor objects\": `{size: 1-4, value: typedArray}`.\n- Maps known attribute names to glTF attribute names.\n- Add `indices` field to the result (only if indices are present in the loaded geometry).\n\n### PointCloud/Mesh Data Structure\n\n| Field        | Type                | Contents                                                                                                                 |\n| ------------ | ------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n| `loaderData` | `Object` (Optional) | Loader and format specific data                                                                                          |\n| `header`     | `Object`            | See below                                                                                                                |\n| `mode`       | `Number`            | Aligned with [OpenGL/glTF primitive types](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#primitive) |\n| `attributes` | `Object`            | Each key contains an \"accessor\" object representing the contents of one attribute.                                       |\n| `indices`    | `Object` (Optional) | If present, contains the indices (elements) typed array (`Uint32Array` or `Uint16Array`).                                |\n\nNote that glTF attributes (keys in the `glTFAttributeMap`) are named per [glTF 2.0 recommendations](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#geometry) with standardized, captilalized names.\n\n### Header Object\n\nThe `header` fields are only recommended at this point, applications can not assume they will be present:\n\n| `header` Field | Type     | Contents |\n| -------------- | -------- | -------- |\n| `vertexCount`  | `Number` |          |\n\n### Primitive Mode\n\nPrimitive modes are the usual standard WebGL/OpenGL constants:\n\n| Value | Primitive Mode   | Comment                                                                                              |\n| ----- | ---------------- | ---------------------------------------------------------------------------------------------------- |\n| `0`   | `POINTS`         | Used for point cloud category data                                                                   |\n| `1`   | `LINES`          | Lines are rarely used due to limitations in GPU-based rendering                                      |\n| `2`   | `LINE_LOOP`      | -                                                                                                    |\n| `3`   | `LINE_STRIP`     | -                                                                                                    |\n| `4`   | `TRIANGLES`      | Used for most meshes. Indices attributes are often used to reuse vertex data in remaining attributes |\n| `5`   | `TRIANGLE_STRIP` | -                                                                                                    |\n| `6`   | `TRIANGLE_FAN`   | -                                                                                                    |\n\n### Accessors\n\n`attributes` and `indices` are represented by glTF \"accessor objects\" with the binary data for that attribute resolved into a typed array of the proper type.\n\n| Accessors Fields | glTF? | Type                | Contents                                                                                                                                           |\n| ---------------- | ----- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `value`          | No    | `TypedArray`        | Contains the typed array (corresponds to `bufferView`). The type of the array will match the GL constant in `componentType`.                       |\n| `size`           | No    | `1`-`4`             | Decoded \"type\". i.e. number of components                                                                                                          |\n| `byteOffset`     | Yes   | `Number`            | Starting offset into the bufferView. Currently always `0`                                                                                          |\n| `count`          | Yes   | `Number`            | The number of elements/vertices in the attribute data                                                                                              |\n| `originalName`   | No    | `String` (Optional) | If this was a named attribute in the original file, the original name (before substitution with glTF attribute names) will be made available here. |\n\n### GLTF Attribute Name Mapping\n\nAlso, to help applications manage attribute name differences between various formats, mesh loaders map known attribute names to [glTF 2.0 standard attribute names](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#geometry) a best-effort basis.\n\nWhen a loader can map an attribute name, it will replace ir with the glTF equivalent. This allows applications to use common code to handle meshes and point clouds from different formats.\n\n| Name         | Accessor Type(s)     | Component Type(s)                                                                                                  | Description                                                                                                        |\n| ------------ | -------------------- | ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------ |\n| `POSITION`   | `\"VEC3\"`             | `5126`&nbsp;(FLOAT)                                                                                                | XYZ vertex positions                                                                                               |\n| `NORMAL`     | `\"VEC3\"`             | `5126`&nbsp;(FLOAT)                                                                                                | Normalized XYZ vertex normals                                                                                      |\n| `TANGENT`    | `\"VEC4\"`             | `5126`&nbsp;(FLOAT)                                                                                                | XYZW vertex tangents where the _w_ component is a sign value (-1 or +1) indicating handedness of the tangent basis |\n| `TEXCOORD_0` | `\"VEC2\"`             | `5126`&nbsp;(FLOAT)<br>`5121`&nbsp;(UNSIGNED_BYTE)&nbsp;normalized<br>`5123`&nbsp;(UNSIGNED_SHORT)&nbsp;normalized | UV texture coordinates for the first set                                                                           |\n| `TEXCOORD_1` | `\"VEC2\"`             | `5126`&nbsp;(FLOAT)<br>`5121`&nbsp;(UNSIGNED_BYTE)&nbsp;normalized<br>`5123`&nbsp;(UNSIGNED_SHORT)&nbsp;normalized | UV texture coordinates for the second set                                                                          |\n| `COLOR_0`    | `\"VEC3\"`<br>`\"VEC4\"` | `5126`&nbsp;(FLOAT)<br>`5121`&nbsp;(UNSIGNED_BYTE)&nbsp;normalized<br>`5123`&nbsp;(UNSIGNED_SHORT)&nbsp;normalized | RGB or RGBA vertex color                                                                                           |\n| `JOINTS_0`   | `\"VEC4\"`             | `5121`&nbsp;(UNSIGNED_BYTE)<br>`5123`&nbsp;(UNSIGNED_SHORT)                                                        | See [Skinned Mesh Attributes](#skinned-mesh-attributes)                                                            |\n| `WEIGHTS_0`  | `\"VEC4\"`             | `5126`&nbsp;(FLOAT)<br>`5121`&nbsp;(UNSIGNED_BYTE)&nbsp;normalized<br>`5123`&nbsp;(UNSIGNED_SHORT)&nbsp;normalized | See [Skinned Mesh Attributes](#skinned-mesh-attributes)                                                            |\n\n> Note that for efficiency reasons, mesh loaders are not required to convert the format of an attribute's binary data to match the glTF specifications (i.e. if normals were encoded using BYTES then that is what will be returned even though glTF calls out for FLOAT32). Any such alignment needs to be done by the application as a second step.\n\n## Scenegraph Format Support\n\nFor more complex, scenegraph-type formats (i.e. formats that don't just contain single geometric primitives), loaders.gl currently focuses on glTF 2.0 support.\n\nIt is assumed that other scenegraph-type format loaders (e.g. a hyptothetical COLLADA loader) could convert their loaded data to a similar structure, essentially converting to glTF 2.0 on-the-fly as they load.\n\nFor now it is best to convert such assets off-line to glTF before attempting to loade them with loaders.gl.\n\n### Material support\n\nMaterial support is provided by some mesh formats (e.g. OBJ/MTL) and is currently not implemented by loaders.gl, however the glTF loader has full support for PBR (Physically-Based Rendering) materials.\n","slug":"docs/api-reference/mesh-loaders/category-mesh","title":" Mesh (PointCloud) Loader Category"}]}}